# -*- org-confirm-babel-evaluate: nil; org-babel-use-quick-and-dirty-noweb-expansion: t -*-

#+TITLE: Language mechanisms and patterns for extending interpreters
#+AUTHOR: fmdkdd
#+LATEX_CLASS: thesis
#+OPTIONS: tags:nil toc:nil
#+BIBLIOGRAPHY: refs

#+MACRO: acr @@latex:\textsc{$1}@@

#+LATEX: \input{frontmatter}
#+LATEX: \mainmatter

* Contents                                                   :TOC@4:noexport:
 - [[#acknowledgements][Acknowledgements]]
 - [[#introduction][Introduction]]
   - [[#problème-étendre-un-interprèteur-par-de-multiple-analyses][Problème: étendre un interprèteur par de multiple analyses]]
   - [[#but-mécanismes-pour-étendre-simplement-un-interpréteur-en-préservant-la-séparation-des-préoccupations][But: mécanismes pour étendre simplement un interpréteur en préservant la séparation des préoccupations]]
   - [[#portée-interpréteurs-en-javascript][Portée: interpréteurs en JavaScript]]
   - [[#survol][Survol]]
 - [[#État-de-l’art][État de l’art]]
   - [[#de-l’exécution-du-programme-au-code-source][De l’exécution du programme au code source]]
   - [[#smalltalk][Smalltalk]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#self][Self]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#prototype-based-programming][Prototype-based programming]]
   - [[#building-from-modules][Building from modules]]
   - [[#building-with-monads][Building with monads]]
   - [[#le-problème-d’expression][Le problème d’expression]]
   - [[#notion-of-modularity][Notion of modularity]]
   - [[#parnas’-modularity][Parnas’ modularity]]
   - [[#structured-programming][Structured programming]]
   - [[#literate-programming][Literate programming]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#aspect-oriented-programming][Aspect-Oriented Programming]]
   - [[#context-oriented-programming][Context-Oriented Programming]]
   - [[#feature-oriented-programming][Feature-Oriented Programming]]
   - [[#model-driven-development][Model-driven development]]
   - [[#domain-specific-languages][Domain-specific languages]]
   - [[#bytecode-instrumentation][Bytecode instrumentation]]
   - [[#open-implementation][Open Implementation]]
     - [[#mechanisms-for-open-implementation][Mechanisms for open implementation]]
   - [[#aspect-oriented-programming][Aspect-Oriented Programming]]
     - [[#mechanisms-for-instrumentation][Mechanisms for instrumentation]]
   - [[#emacs][Emacs]]
      - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#dynamic-binding][Dynamic binding]]
   - [[#eclipse-and-other-ides][Eclipse and other IDEs]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#web-browsers][Web browsers]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#lua][Lua]]
      - [[#mechanisms-for-extension][Mechanisms for extension]]
   - [[#scripting-languages][Scripting languages]]
   - [[#reflection][Reflection]]
   - [[#software-product-lines][Software product lines]]
     - [[#mechanisms-for-instrumentation][Mechanisms for instrumentation]]
   - [[#caesar][Caesar]]
   - [[#hyperj][Hyper/J]]
     - [[#mechanisms-for-instrumentation][Mechanisms for instrumentation]]
   - [[#information-transparency][Information transparency]]
   - [[#semantic-patches][Semantic patches]]
     - [[#mechanisms-for-extension][Mechanisms for extension]]
 - [[#Étude-de-cas-extension-ad-hoc-de-narcissus][Étude de cas: extension ad-hoc de Narcissus]]
 - [[#variations-sur-un-interpréteur-de-lambda-calcul-extensible][Variations sur un interpréteur de lambda-calcul extensible]]
 - [[#construire-un-interpréteur-extensible][Construire un interpréteur extensible]]
     - [[#finding-a-core-example][Finding a core example]]
     - [[#the-expression-problem][The expression problem]]
     - [[#the-expression-problem-with-a-twist][The expression problem, with a twist]]
     - [[#the-modular-instrumentation-problem][The modular instrumentation problem]]
   - [[#variations][Variations]]
     - [[#javascript][JavaScript]]
       - [[#split-oo-style-instrumented-interpreter-into-modules][Split OO-style instrumented interpreter into modules]]
       - [[#split-pattern-matching-instrumented-interpreter-into-modules][Split pattern-matching instrumented interpreter into modules]]
       - [[#summary-of-javascript-variations][Summary of JavaScript variations]]
   - [[#lisp][Lisp]]
   - [[#haskell][Haskell]]
     - [[#building-scaffolding-with-language-features][Building scaffolding with language features]]
       - [[#monadic-interpreters][Monadic interpreters]]
       - [[#either-data-type][Either data type]]
       - [[#type-classes][Type classes]]
       - [[#data-types-à-la-carte][Data types à la carte]]
       - [[#implicit-arguments][Implicit arguments]]
       - [[#facets-as-a-monad][Facets as a monad]]
     - [[#extending-the-syntax][Extending the syntax]]
   - [[#modular-monadic-interpreters][Modular monadic interpreters]]
     - [[#the-giants][The giants]]
       - [[#data-types--à-la-carte-2015][Data types  à la carte (2015)]]
       - [[#wadler-—-the-essence-of-functional-programming][Wadler — The essence of functional programming]]
     - [[#the-bigger-picture][The bigger picture]]
 - [[#dynamic-scoping-to-build-interpreters][Dynamic scoping to build interpreters]]
   - [[#the-base-datatype][The base datatype]]
   - [[#adding-a-data-variant][Adding a data variant]]
   - [[#adding-an-operation][Adding an operation]]
   - [[#adding-an-operation-as-a-module][Adding an operation as a module]]
     - [[#a-use-case-for-with][A use-case for =with=]]
     - [[#selective-imports-with-an-iife][Selective imports with an IIFE]]
   - [[#modifying-an-operation][Modifying an operation]]
     - [[#non-destructive-modification][Non-destructive modification]]
   - [[#passing-state][Passing state]]
   - [[#all-in-one][All in one]]
 - [[#dynamic-scoping-to-modify-narcissus][Dynamic scoping to modify Narcissus]]
   - [[#the-idea-manipulating-scopes][The idea: manipulating scopes]]
   - [[#how-to-manipulate-scopes-in-javascript][How to manipulate scopes in JavaScript]]
   - [[#evaluation-on-narcissus][Evaluation on Narcissus]]
   - [[#discussion][Discussion]]
     - [[#insights][Insights]]
     - [[#broad-applications][Broad applications]]
     - [[#other-js-interpreters][Other JS interpreters]]
     - [[#related-work][Related work]]
       - [[#open-modules][Open modules]]
       - [[#isn’t-a-visitor-pattern-enough][Isn’t a visitor pattern enough?]]
       - [[#object-oriented-interpreter][Object-oriented interpreter]]
       - [[#aop][...AOP?]]
       - [[#scoping-strategies][Scoping strategies]]
     - [[#objections-and-downsides][Objections and downsides]]
       - [[#with-is-deprecated][=with= is deprecated]]
       - [[#open-scope-defeats-the-purpose-of-the-module-pattern][Open scope defeats the purpose of the module pattern]]
       - [[#it’s-not-modular-if-there-is-no-interface-for-instrumentation][It’s not modular if there is no interface for instrumentation]]
       - [[#it’s-still-ad-hoc][It’s still ad-hoc]]
       - [[#application-of-open-scope-if-not-module-pattern][Application of open scope if not module pattern?]]
       - [[#open-scope-only-captures-top-level-declaration-of-module][Open scope only captures top-level declaration of module]]
       - [[#aliasing-inside-the-module-can-be-problematic][Aliasing inside the module can be problematic]]
       - [[#what-about-interferences-between-instrumentations][What about interferences between instrumentations?]]
       - [[#performance-costs][Performance costs]]
 - [[#synthèse][Synthèse]]
   - [[#séparation-des-préoccupations-pourquoi][Séparation des préoccupations: pourquoi?]]

* Acknowledgements                                                 :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: tex/acks.tex
:END:
Raganwald for a Game of Life implementation showing off literate programming and
AOP, and getting me interested in finding better ways to structure programs.

Bret Victor for the realization of the dissonance between textual programming
languages and the dynamic processes they describe.  Also, for inspiration.

* Introduction
** Problème: étendre un interprèteur par de multiple analyses
- Contexte de sécurité web.
- Scripts de pages web passent par un interpréteur.
- Sécuriser un script = analyser ses fonctionnalités
  - runtime monitoring
  - access control
  - logging
- Une analyse dynamique = une modification de l’interpréteur
- Modification du code source en conflit avec la séparation des préoccupations
  - perte d’extensibilité, perte de lisibilité, difficulté de maintenance...
- Ajouter une analyse devrait être simple
  - sans requérir des modifications invasives de l’interpréteur
  - maximiser la flexibilité, minimiser le coût d’adoption
- Ajouter une analyse ne doit pas impacter la lisibilité du code de l’interpréteur
  - séparation des préoccupations
- Analyses peuvent se composer sans effort particulier (lorsqu’elles
  n’interfèrent pas entre elles)

** But: mécanismes pour étendre simplement un interpréteur en préservant la séparation des préoccupations
- Trouver des mécanismes, des constructions (patterns) pour étendre des interpréteurs
- Améliorer la situation

** Portée: interpréteurs en JavaScript
- Mécanismes et patterns génériques, pas nécessairement liés à un langage
  particulier.
- Software engineering
- Point de vue du programmeur
  - Travail sur le code source, l’éditeur de programmes, les outils du programmeur

** Survol

* État de l’art
** De l’exécution du programme au code source
Pour modifier l’exécution d’un programme, il est bon de savoir /comment/ le
programme est exécuté par la machine.  Du point de vue de la machine, un
programme est une simple liste d’instructions.  Une séquence d’additions, de
soustractions, de chargement et stockage de valeurs en mémoire, et de sauts
conditionnels.  Ces instructions sont présentées à la machine sous la seule
forme que son processeur est capable de manipuler: des nombres, codés par des
suites de zéros et de uns.

#+CAPTION: Un programme (extrait; point de vue d’une machine x86).
#+BEGIN_figure*
\begin{verbatim}
...00101011011100101011010101110101011111010100010110100000000110001011101010010101011...
\end{verbatim}
\vspace{-2em}
#+END_figure*

Le programme est alors exécuté instruction par instruction.  La machine lit une
instruction, puis effectue l’opération correspondante ; elle charge
l’instruction suivante, la lit, effectue l’opération, charge, lit, effectue,
etc.  Cette monomanie contribue à l’utilité que nous trouvons à la machine, car
son processeur agit certes simplement, mais il agit /vite/.  N’importe quel
processeur actuel est capable d’effectuer plusieurs milliards d’opérations par
seconde.  La machine ne différencie donc pas un programme d’un autre; tous se
mêlent en une immense suite binaire.

Le programmeur en revanche cherche à structurer cette suite de nombres.  Une
suite infinie de zéros et de uns est difficile à appréhender pour un cerveau
humain; il lui faut des repères, découper le programme en unités plus
digestibles par nos facultés organiques.  Un programmeur manipule donc rarement
des bits afin de déclarer ses intentions à la machine; il utilise plutôt un
langage de programmation.

#+CAPTION: Un programme (point de vue d’un programmeur connaissant le langage
#+CAPTION: JavaScript, vers 2015).
#+NAME: fib
#+BEGIN_SRC js
function fibonacci(n) {
  return n < 2 ? 1 : fibonacci(n-1) + fibonacci(n-2)
}

print(fibonacci(10))
#+END_SRC

Dans les langages de programmation exotériques[fn:eso-lang], un programme est
constitué principalement de lettres plutôt que de nombres.  Les lettres,
arrangées en permutations judicieuses, nous servent à /nommer/ les objets de la
computation tels que les variables et constantes, les fonctions, les classes et
autres structures.  Les mots sont plus facilement prononçables, davantage
mnémoniques, et peuvent surtout communiquer l’intention du programmeur par
homonymie.  On peut ainsi, simplement en suivant les mots, supposer que le code
[[fib]] calcule et affiche le onzième nombre de la suite de Fibonacci, sans être
vraiment certain de la sémantique de JavaScript.  Les mots /évoquent/
immédiatement du sens alors que les nombres doivent être décodés[fn:misleading].

Mais, si le programme ainsi décrit est plus confortable pour le programmeur, il
est incompréhensible par la machine en tant que tel.  Il faut au préalable
/traduire/ ce programme en code machine avant de pouvoir l’exécuter.  L’analogie
avec les langues naturelles est pertinente; si je souhaite communiquer le
contenu de ce manuscrit de thèse à des non-francophones, soit je rédige une
nouvelle version dans une langue qu’ils parlent (écrire le programme en langage
machine), soit j’emploie les services d’un traducteur[fn:third-option].
Heureusement, la traduction d’un programme est une tâche moins hasardeuse que la
traduction d’un document en langue naturelle.  Les langages de programmation
sont définis de façon à éliminer toute ambiguïté d’interprétation, ce qui permet
d’effectuer la traduction en langage machine automatiquement.

C’est le /compilateur/ qui traduit des programmes d’un langage de programmation
vers le langage machine.  Le compilateur lit le texte brut décrivant le
programme, vérifie que ce texte est conforme aux règles syntaxiques et
grammaticales du langage qu’il traduit, puis applique les règles de traduction
en langage machine.  Un compilateur permet donc au programmeur d’obtenir un
programme exécutable par la machine à partir de code écrit dans le langage de
son choix.

#+CAPTION: Le compilateur lit le programme, en extrait la structure, et produit
#+CAPTION: du code machine.
#+BEGIN_SRC dot :file img/compile-pipeline.png
digraph {
  rankdir=LR;
  node [shape=record];
  source [label = "print(1 + 1)"];
  lex [label = "_print_ ( _1_ _+_ _1_ )"];
  ast [label = "body { call { print, add { 1, 1 }}}"];
  asm [label = "lda 1 - add 1 - psh - call $a0 - ... "];
  bin [label = "0101011101..."];
  source -> lex -> ast -> asm -> bin;
}
#+END_SRC

Notons que la compilation n’attache aucun sens particulier aux noms choisis par
le programmeur.  Les noms servent juste à vite remplacés par des références.  On
pourrait écrire le programme de [[fib]], de manière totalement équivalente:

#+BEGIN_SRC js
function xx(x){return x<2?1:xx(x-1)+xx(x-2)}print(xx(10))
#+END_SRC

# Structures do not count (modules, classes, files).  Structure is for humans.

Le programme, n’a donc pas comme seule vocation à ordonner la machine, il doit
aussi être lisible par d’autres programmeurs
vocations parallèles:

Pour organiser un manuscrit, on recourt aux phrases, aux paragraphes
Lorsqu’un texte devient trop long, il devient plus pratique de le découper
en unités indépendantes.

# Other structures, classes, modules


[fn:eso-lang] Par opposition aux langages /ésotériques/ qui sont conçus
davantage pour démontrer la créativité de leur auteur que pour simplifier la
conception de programmes.  [[cite:Esolang][Esolang]] recense des spécimens tels que Brainfuck,
Piet ou Whitespace, qui ne manquent pas de nous rappeler qu’être Turing-complet
n’est pas un critère suffisant pour être un langage /utile/.

[fn:misleading] Les mots peuvent également évoquer un contresens; une définition
maladroite–ou intentionnelle–peut induire en erreur le lecteur.  Un ami m’a
raconté avoir passé tout un après-midi à pister une erreur dans du code écrit en
C, pour finalement découvrir dans un fichier inclus la ligne: ~#define true
false~.

[fn:third-option] Une troisième option serait que les non-francophones
apprennent le français pour lire ce manuscrit.  En suivant l’analogie, cela
correspond à une machine qui exécuterait directement un programme JavaScript.
Les deux situations sont également improbables.

[fn:obfuscation] Les archives du concours international d’obfuscation de code C
[[cite:IOCCC][IOCCC]] recèlent d’exemples à suivre pour mystifier tout collaborateur.


#+BEGIN_SRC dot :file img/bg-map.png
digraph {
  rankdir=LR;
  node [shape=record];
  source [label = "Code source"];
  box [label = "Magie"];
  effects [label = "Effets"];
  source -> box -> effects;
}
#+END_SRC

#+RESULTS:
[[file:img/bg-map.png]]

#+BEGIN_SRC dot :file img/bg-map2.png
digraph {
  rankdir=LR;
  node [shape=record];
  source [label = "Code source"];
  box [label = "Interpréteur"];
  effects [label = "Effets"];
  source -> box -> effects;
}
#+END_SRC

#+RESULTS:
[[file:img/bg-map2.png]]

#+BEGIN_SRC dot :file img/bg-map3.png
digraph {
  rankdir=LR;
  node [shape=record];
  source [label = "Code source"];
  box [label = "Compilateur"];
  bin [label = "Binaire"];
  effects [label = "Effets"];
  source -> box -> bin -> effects;
}
#+END_SRC

#+RESULTS:
[[file:img/bg-map3.png]]

** Smalltalk
Design and Implementation [[cite:Ing-78][Ing-78]].

Opens with a definition of modularity:
#+BEGIN_QUOTE
No part of a complex system should depend on the internal details of any other
part.

[...]

Objects are created and manipulated by sending messages.  The communication
metaphor supports the principle of modularity, since any attempt to examine or
alter the state of an object is sent as a message to that object, and the sender
need never know about internal representation.

[...]

The class is the natural unit of modularity, as it describes all the external
messages understood by its instances, as well as all the internal details about
methods for computing responses to messages and representation of data in the
instances.
#+END_QUOTE

Smalltalk is designed with modularity, as classes encapsulate object
descriptions and methods, and can only interact through messages.

An example of extending the system: adding new objects and a printer for them.
Similar to the expression problem.

#+BEGIN_QUOTE
Adding a new class of data to a programming system is soon followed by the need
to print objects of that class. In many extensible languages, this can be a
difficult task at a time when things should be easy.  One is faced with having
to edit the system print routine which (a) is difficult to understand because it
is full of details about the rest of the system, (b) was written by someone else
and may even be in another language, and (c) will blow the system to bits if you
make one false move.  Fear of this often leads to writing a separate print
routine with a different name which then must be remembered.

In our object-oriented system, on the other hand, printing is always effected by
sending the message =printon: s= (where s is a character stream) to the object
in question.  Therefore the only place where code is needed is right in the new
class description.  If the new code should fail, there is no problem; the
existing system is unmodified, and can continue to provide support.
#+END_QUOTE

Changing a field inside =Rectangle= does not need to change code external to the
object, and global recompilation is avoided.

Additional story on the vision of Smalltalk can be found in [[cite:Kay-93][Kay-93]]; a larger
perspective is given in [[cite:Mul-15][Mul-15]].

*** Mechanisms for extension
Subclassing, and reflection.  Everything as an object, so message dispatch is
just a method on the meta class, and can be altered.

** Self
The power of simplicity [[cite:US-91][US-91]].

Pure object-oriented language.  No variables, but slots containing objects that
return themselves.

No classes.  No control structure.

The absence of distinction may not be a good thing in practice:
#+BEGIN_QUOTE
The absence of class-instance distinction may make it too hard to understand
which objects exist solely to provide shared information for other objects.
Perhaps SELF programmers will create entirely new organizational structures.  In
any case, SELF’s flexibility poses a challenge to the programming environment;
it will have to include navigational and descriptive aids.

[later, in the conclusion]

Reducing the number of basic concepts in a language can make the language easier
to explain, understand, and use.  However, there is a tension between making the
language simpler and making the organization of a system manifest.  As the
variety of constructs decreases, so does the variety of linguistic clues to a
system’s structure.
#+END_QUOTE

They cite [[cite:UCC+91][UCC+91]] for pointers on structuring programs in SELF.

Classes are abstract description of objects, but prototypes are always
concrete.  Each object is an example, and can be easily cloned.  Class
hierarchies are hard, and impose a structure; prototypes less so.

Classes forces you to create a template, even when you deal with several objects
with unique behavior.

Activation records for methods inherit from the receiver object, so the receiver
is on the chain for binding lookup.

They note that they could build “class-like” objects that hold code to create
new clones, and also hold the shared behavior, though they “do not believe this
is the best way to construct a system”.

In [[cite:UCC+91][UCC+91]], the following organization is described:
- Traits object for methods (shared by all instances of an object).
- A prototype object with a default implementation.
- Instances are created from cloning the prototype.

Abstract objects dispense of the prototype, and singleton objects contain
methods and state without providing a copy method.

They note that OO supports “differential programming”, which is to define new
data types as differences from existing data types.  In Smalltalk, differential
programming is achieved through subclassing.  In SELF, they call it “refining
traits objects”, but the mechanism is delegation through the parent link.

An oddity: they state that parent links are constant, though the introduce a
=dataParent= setter in figure 3.  Later they say that parent slot are like other
data slots, assignable.

Prototypes allow for multiple behavior modes, through dynamic inheritance
switching.  Behavior modes enhance the clarity of the code, though they do not
comment on the potential performance costs.

*** Mechanisms for extension
Prototypes, and message passing.  Prototypes are more general and simpler than
inheritance.

However prototypes do not originate with SELF [[cite:Bor-86][Bor-86]] [[cite:Lie-86][Lie-86]].

[[cite:Lie-86][Lie-86]] makes a good case for prototypes as being a simpler model to learn, as
well as being more intuitive.  Humans derive general concepts from examples, not
the other way around.  Class-based languages require you to commit to the
concepts first.

Prototype-based and class-based languages provide different mechanisms for
realizing differential programming,

Is differential programming sufficient to solve the problem of modular
instrumentation?  In the case of Narcissus, it was not, since the interpreter
was not OO.  But the open scope pattern might be equivalent, dynamically, to
inheritance.

** Prototype-based programming
A collection of (at least) the following papers (or revisions of them):
- [[cite:Tai-97][Tai-97]]
- [[cite:DMB-98a][DMB-98a]] (mostly a translation of [[cite:DMB-98][DMB-98]])
- [[cite:GBO+98][GBO+98]]
- [[cite:Bor-86][Bor-86]]
- [[cite:SU-95][SU-95]]
- [[cite:Smi-95][Smi-95]]
- [[cite:MMM+98][MMM+98]]
- [[cite:Bla-91][Bla-91]] & [[cite:Bla-94][Bla-94]]
- [[cite:Wol-96][Wol-96]]
- [[cite:Moo-96][Moo-96]]
- [[cite:Nob-01][Nob-01]]
- [[cite:DeM-98][DeM-98]]

[[cite:Tai-97][Tai-97]] is a philosophical take on the basis for class-based and prototype-based
languages.  Ascribing to classes is following the school of Plato and Aristotle
[[cite:Pla-98][Pla-98]] [[cite:Ari-35][Ari-35]].  Plato distinguished between /forms/, the ideal description of
things, and /instances/ of these forms.  He regarded forms as being more real
than instances.  Aristotle believed in “a single correct taxonomy of all natural
things”, and classified things using the following rule:
: essence = genus + differential
which mirrors class creation in class-based languages.

Classification has been criticized, notably by Wittgenstein [[cite:Wit-53][Wit-53]], as being
subjective.  Some concepts are difficult to define by intension – through a list
of common properties that all instances must share.  Rather, Wittgenstein
proposes the notion of /family resemblance/.  Meaning is not determined by a
definition, but by similarity to representative prototypes.

This philosophical heritage has a few implications for programming:
- there are no optimal class hierarchies
- in a class hierarchy, the middle classes are often the best representatives.
  Higher classes are too abstract; lower classes too specific.
- prototypes may map better to the usual human process: iterate from examples.

Designers of class-based or prototype-based languages are seldom aware of the
philosophical issues of both models, but focus more on technical matters.  Kevo
[[cite:Tai-93][Tai-93]] is a prototype-based language with a notion of family
resemblance. [[cite:Tai-93b][Tai-93b]] offer similar insights on the notion of object.

[[cite:DMB-98][DMB-98]] tries to classify prototype-based languages (ironically).  Prototype-base
d languages are advantageous for describing exceptional instances, multiple
points of view of the same entity, and incomplete objects.

They identify the following mechanisms common to prototype-based languages:
- message passing
- 3 ways of creating objects (ex nihilo, cloning, and extension)
- delegation
- dynamic dispatch

Prototype-based languages also introduce new issues:
- Fragmented entities.  Since objects are described differentially, no single
  object in the system reify the complete entity.  To clone it completely, we
  would need to clone all its parts, but they are not reified (e.g., traits
  objects are only conventions, not language primitives).
- Sharing between clones of the same object.
- Sharing between clones of different objects.

** Building from modules
Findler & Flatt, Newspeak

** Building with monads
Wadler, Steele, Spinoza, Swierstra, Rúnar, ...

Free algebras, free monads.  Basically reify data in a way that is accepted by
the type system of the underlying language to allow unanticipated extension.

[[cite:OC-12][OC-12]] gives Java code with generics for solving the expression problem using
/object algebras/.  Object algebras are akin to a free algebra.  Instead of
locking down the actual objects used as expressions too early, they leave them
open using abstract factories.  Providing a factory when evaluating the
expression gives you either integer evaluation, or pretty-printing.

Their solution is applicable to Java with generics, without significant
syntactic overhead (less than related work).  And, they leverage the type system
to capture erroneous composition.

** Le problème d’expression
Wadler, Odersky, Krishnamurthi, Oliveira (expression families) ...

** Notion of modularity
Notion of modularity [[cite:OGK+11][OGK+11]].  Modularity is rooted in classical logic thinking.
Classical logic is inflexible, incompatible with the realities of software.
Especially, information hiding is not the silver bullet.  Approaches to software
development that seem to break information hiding, and even oppose modular
reasoning, have their virtues.  Those can be thought of using nonclassical
logics.

** Parnas’ modularity
Parnas is usually credited with the notion of modularity, as well as notions of
/separation of concerns/ and /information hiding/ [[cite:Par-72][Par-72]].  Parnas advocates
improving the methodology of programming through up-front planning and critical
analysis of designs.  He does not believe in language solutions to software
modularity [[cite:DBB+03][DBB+03]] [[cite:Par-96][Par-96]], although he is often quoted by proponents of
modularity through languages.

#+BEGIN_QUOTE
My engineering teacher laid down some basic rules:

1. Design before implementing.
2. Document your design.
3. Review and analyze the documented design.
4. Review implementation for consistency with the design.

There rules apply to software as least as much as they do to circuits or
machines.
#+END_QUOTE

** Structured programming
Argues for a single entry point into procedures, and single exit point.  Not
jumping directly in the middle, or exiting prematurely.

Exemplified by ALGOL, and Pascal [[cite:Wir-74][Wir-74]] [[cite:Wir-74a][Wir-74a]].

Dijkstra notoriously argued against the GOTO statement, as a superfluous control
structure [[cite:Dij-68][Dij-68]].  On grounds of obscuring the “independent coordinates”
implicitly used by programmers to understand the dynamic flow of a program.
“Unbridled use” of GOTO statements makes finding such coordinates “terribly
hard”.  In short, peppering GOTO statements leads to spaghetti code.

The article has a strong prescriptive tone, as usual from Dijkstra, yet it opens
with a reasonable appeal: “to shorten the conceptual gap between the static
program and the dynamic process, to make the correspondence between the program
(spread out in text space) and the process (spread out in time) as trivial as
possible”.

On the legacy front, most programmers are cargo-culting the fear of GOTO (though
Knuth argues that it has its uses [[cite:Knu-74][Knu-74]]).  Few languages in use today propose
it.  However, the discipline of single-exit is more controversial, as most
modern languages offer constructs for early exits from procedures (return
statement) or from loops (break and continue statements, sometimes with
labels).

The fear of GOTO is an example of focusing on the wrong issue: structured
programming is a proposal for clearer programs.  Blindly removing all GOTOs and
labels from an unstructured program does not make it structured.  The focus is
on writing programs that clearly reflect their dynamic process.  As Parnas noted
[[cite:DBB+03][DBB+03]], modularity is solved by improving the design and documentation
processes, not by adding a “module” statement to the language.  The same
situation arises here.

** Literate programming
Programs are constructed as they are explained.  Knuth, LiterateCoffee, Org
mode.

[[cite:Knu-84][Knu-84]] for the original notion:

#+BEGIN_QUOTE
Instead of imagining that our main task is to instruct a /computer/ what to do,
let us concentrate rather on explaining to /human beings/ what we want to do.
#+END_QUOTE

As usual, Knuth writing is delightfully witty:

#+BEGIN_QUOTE
I must confess that there may also be a bit of malice in my choice of a title.
During the 1970s I was coerced like everybody else into adopting the ideas of
structured programming, because I couldn’t bear to be found guilty of writing
/unstructured/ programs.  Now I have a chance to get even.  By coining the
phrase “literate programming,” I am imposing a moral commitment on everyone who
hears the term; surely nobody wants to admit writing an /illiterate/ program.
#+END_QUOTE

The WEB system allows one to write a TeX + source code document, and then
produce documentation (using the WEAVE program) or complete program (using
TANGLE).  The focus is on documenting first what the program does, then
producing a machine version as a second concern.  The source code can be
presented out-of-order in the document, for expository purposes, using links and
macros.

The WEB way of writing programs is “psychologically correct”, as it reflects the
way in which the program was conceived and elaborated.

#+BEGIN_QUOTE
When I first began to work with the ideas that eventually became the WEB system,
I thought that I would be designing a language for “top-down” programming, where
a top-level description is given first and successively refined.  On the other
hand I knew that I often created major parts of programs in a “bottom-up”
fashion, starting with the definitions of basic procedures and data structures
and gradually building more and more powerful routines.  I had the feeling that
top-down and bottom-up were opposing methodologies: one more suitable for
program exposition and the other more suitable for program creation.

[...] I have come to realize that there is no need to choose once and for all
between top-down and bottom-up, because a program is best thought of as a web
instead of a tree.  [...] A complex piece of software consists of simple parts
and simple relations between those parts; the programmer’s task is to state
those parts and those relationships, in whatever order is best for human
comprehension – not in some rigidly determined order like top-down or
bottom-up.

[...]

Thus the  WEB language allows a person to express programs in a “stream of
consciousness” order.
#+END_QUOTE

An unexpected benefit of WEB is a better separation of concerns.  Although Knuth
does not use the term, each part of a program can be described in its own
section, thus each section can focus on one concern.  He gives the example of
separating error recovery from a simple data structure update routine.

#+BEGIN_QUOTE
While writing the program for [error recovery], a programmer subconsciously
tries to get by with the fewest possible lines of code, since the program for
[updating the structure] is quite short.  If an extensive error recovery is
actually programmed, the subroutine will appear to have error-messages printing
as its main purpose.  But the programmer knows that the error is really an
exceptional case that arises only rarely; therefore a lengthy error recovery
doesn’t look right, and most programmers will minimize it [...] in order to make
the subroutine’s appearance match its intended behavior.  [Programming] with
WEB, the purpose of =update= can be be shown quite clearly, and the possibility
of error recovery can be reduce to a mere mention when =update= is defined.
When another section [related to error recovery] is subsequently written, the
whole point of that section is to do the best error recovery, and it becomes
quite natural to write a better program.
#+END_QUOTE

Knuth notes that the target programming language can impact the writing of WEB
programs.  Having to declare variables at the start of a program leads to
appending to the same “Local variables” program section.

Taking the time to document the code as you write it is not free, but is
beneficial in the long run.

#+BEGIN_QUOTE
I had known for a long time that the programs I construct for publication in a
book, or the programs that I construct in front of a class, have tended to be
comparatively free of errors, because I am forced to clarify my thoughts as I do
the programming.  By contrast, when writing for myself alone, I have often taken
shortcuts that proved later to be dreadful mistakes.  It’s harder for me to fool
myself in such ways when I’m writing a WEB program, because I’m in “expository
mode” (analogous to classroom lecturing) whenever a WEB is being spun.  Ergo,
less debugging time.
#+END_QUOTE

#+BEGIN_QUOTE
WEB may be only for the subset of computer scientists who like to write and to
explain what they are doing.
#+END_QUOTE

Noweb is a language-agnostic syntax and implementation of WEB, which is used in
Org-mode.

*** Mechanisms for extension
The idea of documenting as you program is important, as is the focus on writing
“what the human meant to do”.

The mechanisms of including and referencing code snippets allows one to
structure the program as they see fit.  Especially, it allows to separate
concerns through quantification.

** Aspect-Oriented Programming
Manipulation of static and runtime code.  Joinpoints reifie extension points.
Pointcuts give powerful quantification over joinpoints.  Aspects promote
separation of concerns.

Treats the code as an implicit interface.  Runtime code is data.  Obliviousness
both a blessing and a curse.  COMEFROM destroys local reasoning or referential
transparency.

** Context-Oriented Programming
Expressive separation of concerns when behavior can change depending on the
context in which the program is executed.  Composition of programs by layers.

** Feature-Oriented Programming
Promise of high-level programming, where features are built standalone, and
interaction between them are dealt with separately.

** Model-driven development
You build meta-models that encompass all variations of the solution space.

[[cite:HT-06][HT-06]] makes some good points about the promises and reality of MDD (in 2006).
The distinction between the three categories of sketchers, blueprinters and
model programmers in the modeling community is relevant in order to not
amalgamate different intentions.

** Domain-specific languages
Greater control for language designer.  Gives a constrained playground for
programmers.

Downsides include tooling, development time, unfamiliarity and competition with
general-purposes languages.

Monads can be seen as DSLs (but this is an insight better saved for later).

** Bytecode instrumentation
Ansaloni.  Targets bytecode, which is low-level code.

Jinliner [[cite:TSN+02][TSN+02]] can insert code into the bytecode of a Java program.  Allows to
alter the behavior of a program with no access to its source code.  Inserts code
after/before point of interest.

[[cite:BRG+14][BRG+14]] instruments the bytecode interpreter of WebKit to enable information flow
tracking.  Bytecode instrumentation is difficult, because you lose high-level
details of the source code like “when does an if block ends”.  They have to
build a control-flow graph to know when to discard program counters used by the
information flow analysis.  Also, instrumenting the bytecode is specific to the
bytecode compiler of WebKit (there is no standard, unlike Java).

** Open Implementation
Before AOP, there was the concept of Open Implementation [[cite:Rao-91][Rao-91]] [[cite:Kic-96][Kic-96]] [[cite:MLM+97][MLM+97]]
[[cite:KLL+97][KLL+97]].

[[cite:Rao-91][Rao-91]] introduces the concept of a system with /open implementation/, which has
two interfaces: the base level interface and the metalevel interface that
reveals parts of the implementation of the base level.  They use reflection to
customize the behavior of a window system for writing a spreadsheet.  They find
that OO languages have advantages:
1. Object-centered specification closely maps the domain (here, a window
   system).
2. Polymorphism allows multiple implementation to coexist.
3. Inheritance allows reuse and differential programming.

Reflection is only one mechanism, that may not be optimal for clients of the
meta level interface (can be complex).  They believe in a more declarative
approach to meta level interfaces.

#+BEGIN_QUOTE
An Open Implementation of a software module exposes facets of its internal
operation to client control in a principled way.  They key assumption behind
Open Implementation is that software modules can be more reusable if they can be
designed to accommodate a range of implementation strategies.  Since no
implementation strategy is adequate for all clients, the module should support
several implementation strategies and allow clients to help select the strategy
actually used.
#+END_QUOTE
[[cite:MLM+97][MLM+97]]

The first sentence does not give the full picture.  Open Implementation is not
just about exposing an alternate interface.  The primary concern is to allow
client code to select different implementation strategies (to answer different
performance needs, for instance).

Metaobject protocols [[cite:KRB-91][KRB-91]] are given as an example of open implementation, for
object-oriented systems.

#+BEGIN_QUOTE
The goals of any Open Implementation are to ensure that suitable implementation
strategies are available for a range of clients, to ensure that the appropriate
strategy may be selected for or by a client, and to ensure that the benefits
associated with black-box abstraction are not unreasonably compromised.
#+END_QUOTE
[[cite:MLM+97][MLM+97]]

One key tenet of OI is “give control to the client in a disciplined way”.  That
means, some structure should be in place, otherwise the client is free to mess
with the implementation in any way.

#+BEGIN_QUOTE
Whereas black-box modules hide all aspects of their implementation, open
implementation modules allow clients some control over selection of their
implementation strategy, while still hiding many true details of their
implementation.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

The paper is broad: it considers what solution should a module implementer chose
for open implementation, depending on the client requirements.  It provides a
methodology for designing an open module.

They define 4 styles of open interface:
1. Client has no control: the module adapts its implementation by observing the
   client.
2. Client declares its usage pattern, module selects a strategy.
3. Client specifies the strategy among the predefined ones.
4. Client provides the strategy.

Style 4 is the one we want for modular instrumentation.  It is also recommended
in half the cases they consider, though it “might be difficult to engineer”.
They note that style 4 cannot be used when the integrity of the module must not
be compromised.

[[cite:KLL+97][KLL+97]] describes the four styles further.  Style 4 subsumes styles 1 and 3 (and
could be adapted to style 2), and is said to be /layered/, in the sense that
clients can choose the style better suited for their needs.

#+BEGIN_QUOTE
When there is a simple interface that can describe strategies that will satisfy
a significant fraction of clients, but it is impractical to accommodate all
important strategies in that interface, then the interfaces should be layered.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

From the set of client requirements, the module implementer should refine the
open interface in stages, until all requirements can be expressed.

*** Mechanisms for open implementation
Sadly, the language mechanisms for open implementation are not covered.

#+BEGIN_QUOTE
While the implementation techniques that support theses interfaces are crucial,
they are beyond the scope of this paper.  [footnote:] Many of the implementation
techniques are straightforward, and will be apparent simply from looking at the
interface design.  Others are more subtle, and involve recently developed
techniques in language and system implementation [[cite:KRB-91][KRB-91]] [[cite:CU-91][CU-91]] [[cite:Chi-95][Chi-95]].  There is,
as yet, no unified presentation of these techniques; a separate paper describing
this is in preparation.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

Could not find a trace of this paper in preparation.

The Strategy pattern comes to mind [[cite:GHJ+94][GHJ+94]] (though they actually cite [[cite:HO-87][HO-87]] for
the specific case of selecting algorithms with different space/time trade-offs).

Open Module [[cite:Ald-05][Ald-05]] does not mention Open Implementation, although they certainly
fit the description of style 4.

[[Reflection]] is another mechanism.

** Aspect-Oriented Programming
Did the initial vision of AOP covered the problem of extensibility?

[[cite:KLM+97][KLM+97]]
Motivation of AOP is a better match between design processes and programming
language mechanisms.

#+BEGIN_QUOTE
A design process and a programming language work well together when the
programming language provides abstraction and composition mechanisms that
cleanly support the kinds of units the design process breaks the system into.
#+END_QUOTE

OO languages, procedural languages, functional languages all provide a
/generalized procedure/ as key abstraction mechanism.  Design processes for a GP
language decompose systems into units of behavior.

First example of tangling: an efficient image filter system.  A filter loops on
all the pixels of the input image, and produces a new image.  Higher-level
filters (‘horizontal-edge’) are defined by composing lower-level ones (‘or’,
‘and’).  This is elegant, but inefficient as temporary images are created and
deleted, and many loops are made where only one sufficed.

The alternate solution is to code the higher-level filters explicitly with only
one loop.  Then the code is tangled.  Their actual system is 768 lines when
implemented “cleanly”, but the efficient version is 35213 lines.

The language only supports one kind of composition, the functional one, while
there is a need to also specify the fusion of loops, which is a composition of
data flow.

They distinguish /components/ from /aspects/:
- Components :: encapsulate cleanly a feature through a “generalized procedure”
                (object, method, procedure, API).  Components tend to be the
                unit of functional decomposition of the system.
- Aspects :: for features that cannot be cleanly encapsulated through a
             generalized procedure.  Aspects tend to be features orthogonal to
             the functionality of the system: data representation,
             synchronization constraints ...

The goal of AOP is to provide mechanisms to cleanly separate components from
aspects, components from components, and aspects from aspects.  GP languages
only provide mechanisms to separate components from each other.

They feel that dynamic scoping and catch/throw already help implementing
/aspects/, since they provide a complementary composition mechanism.

Error handling and performance issues are often aspects because they cross-cut
the components decomposition.

An AOP implementation has: a component language, an aspect language, and an
aspect weaver.  For example, in AspectJ the component language is Java, the
aspect language is the pointcuts/advice language provided by AspectJ.  But the
component language does not have to be a vanilla language — it can be a specific
one.

In the image filter example, the component language is procedural and allows
high-level filters to be defined cleanly, using a DSL for describing loops.  The
aspect language is also procedural, but allows to specify loop fusion.  The
weaver then creates a data-flow graph from the components, runs aspects on them,
and produces efficient C code.

They significantly improved the performance of the clean version by adding 352
lines of aspects (not counting the size of the weaver).  Though the manually
optimized version is still more efficient.

#+BEGIN_QUOTE
[...] the aspect languages must address different issues than the component
languages.
#+END_QUOTE

The second example is a book repository.  The component language is (a subset
of) Java, and the aspect language is a meta-program which captures method
invocation using compile-time reflective techniques.

[[Reflection]] can be used to write aspects, but may be too powerful a tool (hence,
a costly one).  A reflective system provides a component language and a
low-level aspect language, as well as the weaving mechanism.  The reflected
structures provide join points.  Reflective systems are general-purpose, and in
the paper they aim for more declarative aspects.

AspectJ is more limited than reflection, but still general.  Domain-specific
aspect languages are recommended to write aspects while retaining static
control.

#+BEGIN_QUOTE
AOP is a goal, for which reflection is a powerful tool.
#+END_QUOTE

[[cite:KHH+01][KHH+01]] presents the AspectJ AOP system.  It contains an intuitive footnote about
the separation of concerns.

#+BEGIN_QUOTE
When we say “separation of concerns” we mean the idea that it should be possible
to work with the design or implementation of a system in the natural units of
concern – concept, goal, team structure etc. – rather than in units imposed on
us by the tools we are using.  We would like the modularity of a system to
reflect the way “we want to think about it” rather than the way the language or
other tools force us to think about it.  In software, Parnas is generally
credited with this idea [[cite:Par-72][Par-72]] [[cite:Par-74][Par-74]].
#+END_QUOTE

The shift from domain-specific to general-purpose AOP is motivated by a desire
for adoption: providing an alternative paradigm for all Java programmers.

#+BEGIN_QUOTE
AspectJ is intended to be a practical AOP language that provides, in a Java
compatible package, a solid and well-worked-out set of AOP features.
#+END_QUOTE

They describe the joinpoints, pointcuts, and advice of AspectJ, as well as
the rules of advice precedence, and sketch the compilation strategy.

Advice declarations in AspectJ, through CLOS [[cite:KRB-91][KRB-91]], owe much to Flavors
[[cite:Can-03][Can-03]].

[[cite:MK-03][MK-03]] provides models and scheme implementations of four AOP systems; the
Pointcuts-Advice model for AspectJ in particular.

[[cite:FF-04][FF-04]] wants to answer the question “when are we looking at an AOP system?”.
They find two essential traits of AOP systems: quantification and obliviousness.

They describe AOP as the desire to make statements of the form

: In programs P, whenever condition C arises, perform action A.

suggesting three axes of choices for AOP systems:
1. What kinds of conditions can we specify? (Quantification)
2. How do actions interact with programs and with each other? (Interface)
3. How will the system mix the execution of programs and actions? (Weaving)

For quantification, they distinguish between static (conditions on the source
code structure) and dynamic (conditions on the runtime behavior).  Furthermore,
black-box systems quantify over the public interface of components (e.g.,
functions or object methods), and clear-box systems quantify over the internal
structure of the code (AST).

They note that rule-based systems (Prolog, OPS-5 [[cite:BFK+85][BFK+85]]) would not need AOP.
However

#+BEGIN_QUOTE
But by and large, people don’t program with rule-based systems.  This is because
rule-based systems are notoriously difficult to code.  They’ve destroyed the
fundamental sequentiality of almost everything.  The sequential, local, unitary
style is really very good for expressing most things.  The cleverness of
classical AOP is augmenting conventional sequentiality with quantification,
rather than supplanting it wholesale.
#+END_QUOTE

The paper has an interesting stance on the evolution of programming languages
with respect to /local/ and /unitary/ statements (\sect2.2).

#+BEGIN_QUOTE
The earliest computer machine-language programs had a strict correspondence
between the program text and the execution pattern.  Generally, each programming
language statement was both /unitary/ and /local/ — unitary in that it ended up
having effect in precisely /one/ place in the elaborated program, and local in
that it was almost always proximate to the statements executing around it.
#+END_QUOTE

They point out that adding code to a base class that has multiple subclasses is
a form of quantification.

[[cite:Ste-06][Ste-06]] questions the success of AOP by opposing the AOP vision to the actual
mechanisms provided.  Quoting [[cite:FF-04][FF-04]]:

#+BEGIN_QUOTE
Understanding something involves both understanding how it works (mechanism) and
what it’s good for (methodology).  In computer science, we’re rarely shy about
grandiose methodological claims (see, for example, the literature of AI or the
Internet).  But mechanism is important – appreciating mechanisms leads to
improved mechanisms, recognition of commonalities and isomorphisms, and plain
old clarity about what’s actually happening.
#+END_QUOTE

AOP has the issue of /fragile pointcuts/: sensitive to changes in the target
program.

AOP is detrimental to Parnas’s notion of modularity because of the strong
coupling between an aspect and the target program.  Independent development
cannot continue.

Interestingly, Parnas considers modularity as a design issue, not a language
one.  Confusing the two is harmful: using the module functionality of a language
does not mean the system is modular in the sense meant by Parnas.  Each task is
a single module with a clear interface, and implementation-specific information
is not shared across modules.

They suggest that AOP use should be restricted to applications where programmers
do not have to see it; e.g., generated code.  They do not regard AOP as
a “new paradigm”, especially they do not find convincing applications for it.

AOP promotes the localization of concerns (bringing tangled code in one place),
but this actually breaks the locality of code (executed statements are not
together in the source code).

I find strange that a critique of AOP does not even mention the original AOP
paper [[cite:KLM+97][KLM+97]].  This critique is focused on the AOP mechanism as realized by
AspectJ, mostly.  But the original paper focused on domain-specific aspect
languages, which /hid/ the weaver, joinpoints and pointcuts.  The original
contribution was also in formulating the goal of separating components from
aspects.  AspectJ is just one way to achieve this goal, but it might not be best
one, depending on the domain.

Overall, it is a critique of one mechanism for AOP, rather than a critique of
the methodology (separating aspects from components).

Aspects and monads are sometimes both viewed as mechanisms to achieve modularity
in software [[cite:DBB+03][DBB+03]] [[cite:HO-07][HO-07]] [[cite:Meu-97][Meu-97]].

AOP is [[https://encrypted.google.com/patents/US6467086][patented]] since 2002 by XEROX (US6467086 B1).

*** Mechanisms for instrumentation
The distinction between aspects and components is the most important
contribution of AOP.  Though it is unclear whether ‘aspects’ are inevitable
because of the complexity of the problem domain, or if they are accidental
artifacts created by the chosen programming model (like most design patterns are
motivated by the lack of first-class functions).

Java + AspectJ is only one aspect system: useful for tracing, logging, but
cumbersome for more specific needs.  The pointcuts/advice model is the
underlying formalism of AspectJ, but not necessarily of the AOP methodology.

Like Parnas’s modules, aspects are a design-time issue.  Solving the module
issues with language mechanisms was, according to Parnas [[cite:DBB+03][DBB+03]], a mistake.
Maybe the same can be said of aspects.

Is AOP useful for the instrumentation problem?  First, the initial use case of
AOP, like open implementation, is tangential concerns: algorithmic complexity,
choice of data representation, optimizations, etc.  Post-hoc extension is not
exactly a tangential concern: changing the behavior of the interpreter is a
primary concern.

Second, we have to consider separately the usefulness of the AOP methodology,
and of the AOP mechanisms.

The methodology of separating components from aspects is applicable if our
analyses are tangential.  They are not.  The problem we deal with is that
extensibility was not considered when designing the interpreter, and solutions
must be built on the implementation.

Preserving locality is a guiding tenet of the AOP methodology (avoiding
tangling).  It is also a motivation for writing modular analyses: we want the
analysis code to be in one place.  However, by regrouping the analysis code, we
are sacrificing locality of code execution: statements executed at runtime are
not next to each other in the source code.  Satisfying both notions of locality
would lead to duplication in the code, which is a worse state of affairs.
Solutions to this duplication must come from the tools used to write and browse
code, since the textual format we use offer none.  An editor can maintain two
views of the same unit of code: changes in one view will affect both places.
That way, both notions of locality can coexist.

The second notion of locality, the one from [[cite:FF-04][FF-04]], is one manifestation of the
more general need of a match between runtime behavior and static program
description.  The program source should tell readers what it does, and
navigating through dynamically-bound method calls and oblivious advices hinders
the reading.

Organization of the code should reflect the design decisions: what is primary is
explained first, then exceptions or tangential concerns are relegated to
appendices.  Literate programming [[cite:Knu-84][Knu-84]] can help organize the code in a such
way.

The mechanisms of AOP may serve to extend the interpreter with analyses, without
necessarily obeying the component/aspect decomposition.  Though without editor
support, using AOP mechanisms will only satisfy one notion of locality.

** Emacs
See Emacs Manual, [[cite:Sta-81][Sta-81]], [[cite:Hal-88][Hal-88]].  Emacs is an example of an extensible system.
The mechanisms: global namespace, dynamic scoping, and a simple aspect system.

In [[cite:Sta-81][Sta-81]], it is said that the TECO language was instrumental for the
extensibility of the EMACS system.  An interpreter should be available all the
time, and compiled languages often lack this functionality.

#+BEGIN_QUOTE
A system written in PL/I or PASCAL can be modified and recompiled, but such an
extension becomes a separate version of the entire program. The user must
choose, before invoking the program, which version he wants. Combining two
independent extensions requires comparing and merging the source files.  These
obstacles usually suffice to discourage all extension.
#+END_QUOTE

Especially they list “Language features for extensibility”:
1. Global variables.  They can be queried, referred to, and redefined.
2. [[Dynamic binding]].  Useful for redefining binding on the fly.
3. File-local variables.  Good for customization, but really they give a
   file-local value for a global variable.
4. Hooks.  They give points in the control flow to insert extension code.
   Especially when redefining assembly or C functions, which cannot be
   reinterpreted.
5. Error handling.  Throwing the debugger helps discover and recover from
   unexpected situations.
6. Non-local transfers.  Gives an example to exit an infinite loop.


In the related work, Multics EMACS [[cite:Gre-80][Gre-80]] is mentioned as being more flexible,
as it is written in MacLisp directly.  [[Smalltalk]] [[cite:Ing-78][Ing-78]] is also said to be
“oriented toward writing extensible programs”.

(The Augment editor demoed by Engelbart [[cite:EE-68][EE-68]] is also mentioned, though nothing
is said of its extensibility.)

[[cite:NS-01][NS-01]] proposes a dynamic scope analysis, to translate Emacs Lisp code using
dynamic binding to lexical binding.

***  Mechanisms for extension
Global variables, dynamic binding, hooks.

Though hooks are more a convention than a first-class mechanism.

** Dynamic binding
Introduced by McCarthy’s LISP [[cite:McC-60][McC-60]] as a bug.  Can be emulated by passing a
dynamic environment in lexical binding [[cite:Que-03][Que-03]].

Implicit parameters [[cite:LLM+00][LLM+00]] provide dynamic scoping for Haskell (though they lose
their first-class privileges).

[[cite:Mor-98][Mor-98]] gives a syntactic theory of dynamic binding, and prove that dynamic
binding adds expressiveness to a purely functional language.  They give examples
in Perl, TeX, Common Lisp and Bash.

[[cite:Tan-09a][Tan-09a]] generalizes dynamic and static binding by making explicit the two
dimensions of propagation of bindings (call stack and delayed lambdas), and
offering a filter function to toggle the activation of a propagated binding.

Some use-cases are mentioned, but none are demonstrated in the paper.  The
proposal is not motivated enough by concrete applications that would be
difficult to solve using existing mechanisms.  Also, the work is really focused
on the binding semantics of Scheme, which reduce its applicability.

** Eclipse and other IDEs
As noted by [[cite:Ler-11][Ler-11]], the Eclipse platform is extensible, and built using
plugins.  Each plugin states its dependencies (the hooks needed to function),
and its extension points (for other plugins).

Eclipse plugins are compiled, though they can be loaded dynamically (if they are
written properly).  Symptomatically of Java, writing plugins needs lot of
boilerplate code and XML (which Eclipse can generate for you, I understand).

*** Mechanisms for extension
The mechanisms for extension seems to revolve around the observer pattern: a
host plugin raises events which can be intercepted by extensions [[cite:Bol-03][Bol-03]].

So, a lot of convention.

** Web browsers
Many extensions are written for web browsers.  The mechanisms are heavy,
comparable to the effort of writing an Eclipse plug-in.

In fact, ZaphodFacets was an extension to change the JavaScript interpreter used
by the browser.

*** Mechanisms for extension
Convention.  Write manifest, and define the agreed-upon functions (install,
startup).

** Lua
An extensible extension language [[cite:IFF-96][IFF-96]].

Extensible systems comprise of a kernel and a configuration.  The kernel is the
core of the system, the parts that cannot change, and is usually compiled for
speed and efficiency.  The configuration part is written in an interpreted,
flexible language, which can interact with the kernel.

Another take, in the conclusion, is that the kernel is a virtual machine for
programs written in the configuration language.

Note that if performance can suffer, writing the whole system as a configuration
gives even greater flexibility.

Configuration languages can be simple: .ini files, X11 resource files, but they
can have more features (scripting languages).  Also called /extension
languages/.

Five requirements for extension languages:
1. good data structures (key-value maps for configuration)
2. simple syntax for amateur programmers
3. lightweight
4. not static type checking or exception handling, as only small programs are
   written in them
5. should be extensible

Requirement 4 is actually an absence of requirement.  Unfortunately, people
/will/ write large systems in it, especially if the language is easy to pick up.
Arguably, the cost of such features may conflict with requirement 3.  Otherwise,
this list looks more like a checklist for Lua.

On a related note, [[cite:Bla-82][Bla-82]] devotes a whole thesis against exceptions.

Extension programs have no =main=.

Associative arrays are a powerful data structure which make plenty of algorithms
trivial (free hashtables), and more efficient to implement than lists.

Amusingly, the associative array syntax was inspired by BibTeX.

Associative arrays + first-class functions = classes.

No error handling, but errors can be raised.  To catch them, we can define
/fallback/ functions.

Setting a fallback on the “index” event allows to define a custom delegation
mechanism between tables.

Compared to Lisp, Lua is portable and has easier syntax.  Tcl is slow and has
strange syntax.  Python is not embeddable, and is already too complex (modules
and exception handling).

At the time of writing, Lua is 20 times slower than C (this factor is said to be
“typical for interpreted languages”, and cites “Java, The Language” for this
assertion).

The latest numbers on the [[http://benchmarksgame.alioth.debian.org][benchmarks game]] show Lua being 5 to 79 times slower,
while consuming more memory.

In the conclusion, they allude at extending web browsers with Lua.  A follow-up
seems to be [[cite:HBI-98][HBI-98]], which proposes Lua as a target for CGI on web servers.

[[cite:IFF-07][IFF-07]] goes over the history of Lua, up to version 5.1 released in 2006.

One tenet of Lua is “Mechanisms, not policy”: provide language mechanisms and
let programmers code the way they want to with them.  An example is message
dispatch: rather than using a class construct, Lua programmers can use fallbacks.

Though they regret not stating a policy when it comes to modules, since everyone
is doing its thing, without agreeing on a common protocol.

***  Mechanisms for extension
The kernel+configuration, as seen in EMACS.  Mechanisms over policy shares our
philosophy and provides programmers with tools to solve their problems in their
own way.

To extend Lua, bindings from C can be added, and custom data structure as well.
Changing the interpreter does not seem possible, even from C.

** Scripting languages
Tcl 1988, Python 1991, Lua 1993, VBA 1993, JS 1995.  Scripting languages are an
early ‘90s phenomenon.  Dealing with low-level languages was deemed too heavy,
but writing your whole system in a high-level language was too costly.  The
compromise was to write the kernel in C, and the rest in a scripting language.

With sufficiently efficient high-level languages, the kernel+configuration
approach might be unneeded.

JavaScript being a scripting language for the browser, as well as an object used
in the Core, it might be adequate to have a dedicated background section to it.

** Reflection
[[cite:Tan-09][Tan-09]] gives a nice survey of reflection and its uses.  Useful distinctions are
made between /introspection/, /introcession/, /structural reflection/, and
/behavioral reflection/; also between a program (a textual description) and a
/computational system/ (a running process described by a program).

A interesting observation on binding is quoted from [[cite:MJD-96][MJD-96]]:
#+BEGIN_QUOTE
The general trend in the evolution of programming languages has been to postpone
formal binding times towards the running of programs, but to use more and more
sophisticated analysis and implementation techniques to bring actual times back
to the earlier stages.
#+END_QUOTE
Later binding = more runtime flexibility, but also less guarantees and less
performance.  The DLS submission is a perfect example.

[[cite:DS-01][DS-01]] give a general method to reify selected parts of a meta-circular
interpreter.

[[cite:Ste-94a][Ste-94a]] studies object-oriented languages which support open implementation.
The open implementation of a language (the interpreter) is itself written in one
language called the /implementation language/, and its meta-level interface
allows the system to interpret a range of /engendered languages/.

[[cite:SW-96][SW-96]] describe three approaches to code non-functional requirements while
preserving the separation of concerns: systems-based, language-based, and
MOP-based.  They find that MOP-based solutions are more flexible, especially as
they can be applied to other domains without modifying the code.  However, they
consider non-functional requirements like persistence and atomicity.

Reflection for dynamic adaptation [[cite:DSC+99][DSC+99]].  Dynamic adaptation echoes the
motivation of open implementation: an application should adapt dynamically to
the need of the users, thereby enhancing performance.  This is mostly a concern
in systems software, operating systems and middlewares.  They use a memory
allocator example and compare using design patterns, DLLs and reflection.
Essentially, reflection is more flexible, but also less efficient.

[[cite:RC-02][RC-02]] illustrates how unanticipated dynamic adaptation can be achieved using
MOPs in Java.

Unifying AOP and OOP [[cite:RS-09a][RS-09a]].

[[cite:ADF-11][ADF-11]] proposes a proxy protocol for values.  A /virtual value/ is wrapped by a
proxy which has a handful of traps that are useful to override: when the value
is called as a function, when the value is used as a record, when the value is
used as an index in an array, when the value is used in a binary operation ...

They exhibit several scenarios where virtual values are useful: lazy evaluation,
revocable membranes, and tainting.  They modified Narcissus (again!) to add
their virtual values extension, but the implementation seems incomplete
regarding all operations available in JavaScript.

They motivate virtual values as a nice way to extend languages without having to
touch the interpreter.  Though they do not talk at all of the limitations of
this approach: can you write any extension that you would write by modifying the
interpreter with virtual values?  The only downsides they acknowledge are
performance hits and potential breakage of JS invariants (‘x*x’ returning a
negative number, or ‘x === x’ returning false).

It seems evident that virtual values are only hooks for values.  So you cannot
override any other part of the module which is not explicitly given by a trap.
Getting a trace of the interpreter execution is out.  Also, you need to specify
your analysis from the point of view of handler on values, not by altering the
interpreter semantics.

[[cite:KT-13][KT-13]] implements access control on JS objects through ES6 proxies.  Improves a
previous implementation which used code transformation; better performance, less
maintenance.

** Software product lines
[[cite:ABK+13][ABK+13]] provides a well-rounded survey of the field.

An engineering methodology to create and maintain variants of a software
product, with optional features (analogy with car assembly lines, which allow
for adding optional features while reusing the same assembly process).

Inspired by the similar evolution in the mass production of consumer goods.
From handcrafting to mass production, to mass customization: product lines that
cover a spectrum of variations.  Examples abound: cars, multi-flavored
detergent, phones, Subway sandwiches ...  Software product lines are the
realization of mass customization for software products (yeah!).

A product line engineering platform combines all the artifacts, documentation
and methodologies of a family of products.  The goal of PLE is to manage the
/commonality/ and /variability/ of a product family.  PLE is not specific to
software.

Properties of a SPL:
- binding time (composition can happen at compile-time, load-time or run-time)
- language solution vs. tool based
- annotation (think C preprocessor) vs. composition (features in their own unit)
- preplanning effort (can you add features without designing for it?)
- feature traceability (mapping between feature model to solution space)
- separation of concerns
- information hiding
- uniformity

Software product lines mechanisms include:
- global parameters
- design patterns (observer, strategy, decorator)
- frameworks
- components

Using version control branches to manage variability is also discussed.  Each
branch correspond to a product, and code sharing is provided by the version
control tool.  However, version control manages /products/ rather than
/features/.  Features are not apparent independently of the base code, except
when looking at diffs.

Feature-oriented programming allows the decomposition of a program into features
first.  Jak is a Java extension that supports FOP [[cite:BSR-04][BSR-04]].  A feature corresponds
to a layer, and each layer can contain multiple classes that implement the
feature.  Further layers can /refine/ the classes of previous layer, and refer
to their implementation via the =original= keyword.

FeatureHouse [[cite:AKL-13][AKL-13]] is akin to [[Semantic patches]], in that it uses a reduced
syntax tree in order to transform code.  One writes a base program, then another
program can be superimposed on it by matching their reduced syntax trees.  The
base program code can be called using the =original= keyword.  Three-way merges
are also possible, and resolved like in version control systems.  The model of
reduced syntax trees of FeatureHouse is language independent, as are the
composition mechanisms.  Language plugins can be written to tell
FeatureHouse how to generate, compose, and pretty-print reduced syntax trees.

#+BEGIN_EXAMPLE
public class A {
  private int foo() { return 0; }
}

public class A {
  private int foo() { original(); return 1; }
}
#+END_EXAMPLE

FeatureHouse also supports quantification.  Mixins and traits mechanisms are
essentially instances of superimposition.

FOP is well-suited to implementing /heterogeneous concerns/ (one variation per
join point), while AOP is better for /homogeneous concerns/ (one variation,
multiple join points). [[cite:MO-04][MO-04]] illustrates the compromises of each approaches (and
presents [[Caesar]] as the superior solution).

If you cannot maintain a separation of concerns in the code itself, you can
emulate it through views.  /Virtual separation of concerns/ is using tools to
provide coherent views of features that are scattered in the code [[cite:AK-09][AK-09]].

Virtual separation of concerns has few downsides and many benefits: simplicity
and flexibility being the chief advantages.

Handling feature interactions is an open problem.  Detecting them also.

*** Mechanisms for instrumentation
FOP implementations presented here are static organization of code into
features.  Much like design patterns or frameworks, they require the programmer
to design for extensibility beforehand.  AspectJ allows extending an existing
code base (unlike the original AOP vision, which emphasized the design decision
of separating components from aspects).

The notion of superimposition is nice.  Recognizing that inheritance, mixins and
traits are all instances of superimposition is a powerful insight.

Virtual separation of concerns makes some good points.  If the primary
decomposition is tyranny, then we have no hope of organizing the physical code
into features.  However, we can leverage editing tools to re-arrange and view
the code in any way we like.  One physical representation, many views.  Each
view can provide different information about the system.

The motivation behind all such mechanisms is a desire to organize snippets of
code, to structure modules, and avoid repetitions.  The ultimate conclusion of
that trend is a language-agnostic manipulation syntax based on hypertext.  Each
snippet has a name, and tags (for marking membership of a feature, but mostly
for non-hierarchical organization).  Any snippets can be referenced by another
(for documentation), and can be included for execution.  Snippets can be
referenced to by name, or by tags.  Tags and wildcards allow quantification.

Tags also allow to view the program through different lenses.  Snippets can have
parameters, hence are a form a macros.

Links are two way, and kept in sync by the programming system (editor): this
prevents obliviousness.

** Caesar
[[cite:AGM+06][AGM+06]]

CaesarJ regroups virtual classes, mixins, pointcut-advice and binding classes.
All these mechanisms are brought together to allow composition along many axis.

But overall, I failed to see the problems that it solved.  Both papers [[cite:AGM+06][AGM+06]]
[[cite:MO-02][MO-02]] are dense and opaque; the examples are too complicated to make sense of
the benefit brought by the new mechanisms.

** Hyper/J
[[cite:TOH+99][TOH+99]] argues for a multi-dimensional separation of concerns.  First, they note
that modern software technologies provide mechanisms for the /decomposition/ and
/composition/ of source code, in order to cut the code into manageable pieces,
and put the pieces back together to produce the running program.

Existing software formalisms provide decomposition and composition mechanisms,
but typically support a single dominant dimension of decomposition.  They dub
this phenomenon the “tyranny of the dominant decomposition”.

A class hierarchy is insufficient for anticipating all the evolutions of an
expression language (see Expression problem).  Subclassing and design patterns
require pre-planning.

There are many concerns we need to manage simultaneously, and the dominant
decomposition typically sacrifices some of those concerns for the benefit of
others.  Thus, we are in presence of a multi-dimensional artifact, and each
decomposition gives only a lower-dimensional view of said artifact.

They propose /hyperslices/ as way to organize artifacts along all desired
dimensions of concern.  An hyperslice contains all the units of change related
to one concern.  Units of change can appear in multiple hyperslices, and thus
hyperslices can overlap.  In the expression example, one slice for the kernel
language, one slice for the pretty-printing, one slice for syntax checking, etc.

Composition of hyperslices must be specified manually, though a default strategy
can be installed.  They suggest one strategy based on name matching for merging
classes together (akin to superimposition).

Throughout the paper, they only use hyperslices on UML diagrams, not source
code.  Hyperslices can be applied to specification, design documents and code.
Though they do not highlight a way to link the related parts from those
different artifacts together, other than putting them in the same hyperslice.
There does not seem to be a way to deal with duplicates.

Compared to AOP, where components are the primary decomposition and aspects
gravitate around them, hyperslices do not impose a dominant structure (though it
may often appear in practice, e.g., the kernel slice of the expression
language).

There are no descriptive papers of Hyper/J, but there is a manual [[cite:TO-00][TO-00]].  The
manual gives details on how to implement the example of expression language.
Using Hyper/J requires to write three files describing: the hyperspace (all
classes that Hyper/J will care about), the concern mappings (which
package/class/method/field maps to which concern), and the hypermodules (which
features are part of a module, and how composition happens).  Running the
Hyper/J tool will compose all the hypermodules using the specified rules (merge
by name) to produce the final program.

Hyper/J simplifies the multi-dimensional concept by mapping units of change to
exactly one feature.  No overlap between hyperslices.

*** Mechanisms for instrumentation
Realization that the tyranny of the dominant decomposition is a manifestation of
looking at a multi-dimensional object through low-dimensional projections.  All
projections are unsatisfactory as they sacrifice one or more dimensions.

The Hyper/J solution is basically superimposition.

** Information transparency
[[cite:Gri-01][Gri-01]]

Tools for capturing the similarity of code across modules.  Tangled code should
be similar, according to the principle of consistency.  Hence, capturing similar
code should help gather and organize concerns.  E.g., changing the behavior of
the parsing of a =while= statement by grepping for ‘while’ in the source.

Principle of consistency:

#+BEGIN_QUOTE
Things that look similar should /be/ similar; things that /are/ different should
look different.
#+END_QUOTE
[[cite:Mac-87][Mac-87]]

First principle of information transparency:

#+BEGIN_QUOTE
Code elements likely to be changed together as part of a complete, consistent
change should look similar, and code elements unlikely to be changed together
should look different.
#+END_QUOTE

If a code base obeys this principle, it can be easily refactored using standard
tools like grep.

A second principle promotes using variable names to indicate implementation
choices.  Hungarian notation is given as an example.

#+BEGIN_QUOTE
The unmodularized code elements relating to a changeable design decision should
contain recognizable tags uniquely identifying the design decision.
#+END_QUOTE

Locality can be managed by tools.  They exhibit tools a bit more powerful than
grep, with knowledge of the target language AST, or matching on typos.

#+BEGIN_QUOTE
Both tools [Aspect Browser and Seesoft] embody the concept that, by leveraging
the human visual system, identifiable /symbols/ are a viable alternative to
/locality/ as a way of managing changes to software.
#+END_QUOTE

** Semantic patches
[[cite:PLM-07][PLM-07]].  A solution to /collateral evolution/.  When a library function changes
name, or gains an argument, client code must makes the necessary changes.  The
changes in client code are collateral.

In a semantic patch, one describes the pattern of collateral changes needed to
adapt client code.

#+BEGIN_EXAMPLE
@ rule2 @
identifier proc_info_func;
identifier hostptr;
@@
proc_info_func (
+ struct Scsi_Host *hostptr,
- int hostno
) {
  ...
- struct Scsi_Host *hostptr;
  ...
- hostptr = scri_host_hn_get(hostno);
  ...
- if (!hostptr) { ... return ...; }
  ...
- scsi_host_put(hostptr);
  ...
}
#+END_EXAMPLE

Identifiers are declared in the header with a syntactic class.  They are matched
in the target code according to the context where they appear in the body of the
semantic path.

The dots =...= are an operator to match any sequence of code between two lines.
There is a mention of the dots matching the /control-flow/ of the code, though
nothing indicates that =spatch= interprets the target code in any way.

[[cite:JH-07][JH-07]] demystifies the tool by giving a denotational semantics.  Indeed, the dots
only match the syntax.

The related work section of [[cite:JH-07][JH-07]] has a few surveys on software evolution, and
in particular the Journal of Software Maintenance and Evolution.

All around a nice idea, though you still have to write the semantic patches from
scratch for every change.

The (unintended) idea of source transformation based on dynamic control flow is
interesting.  See [[file:notes.org::*Shapes%20of%20computation][Shapes of computation]].

*** Mechanisms for extension
It’s another approach, transforming code to alleviate the maintenance cost.

However, it’s a crutch.  We would prefer not having to have to make those
changes in the first place, even if the kernel libraries are updated.

The concept of /collateral evolution/ is certainly related.  When interpreters
evolve, collateral changes are needed on the analyses.  Previous work [[cite:PLM-06][PLM-06]] was
more focused on introducing the collateral evolution problem, with plenty of
examples from the Linux kernel.

* Étude de cas: extension ad-hoc de Narcissus
Narcissus est un interpréteur JavaScript écrit et maintenu par Mozilla
[[cite:Narcissus][Narcissus]].  Narcissus est écrit en JavaScript, et est meta-circulaire: il
utilise l’environnement hôte pour son implémentation (p.ex., l’objet ~String~
exposé au code client n’est pas réimplémenté par Narcissus, mais est une simple
façade de l’objet ~String~ hôte).  Narcissus est une implémentation relativement
légère (environ 6000 lignes de code) du standard ECMAScript [[cite:ECM-99][ECM-99]], qui permet
de rapidement prototyper des fonctionnalités expérimentales pour le langage.

En 2012, Austin et Flanagan se sont servi de Narcissus pour implémenter leur
analyse d’évaluation multi-facettes [[cite:AF-12][AF-12]], une analyse dynamique de flot
d’information qui permet à une valeur d’être étiquetée par une autorité qui a
des droits d’écriture et de lecture pour cette valeur.  Lorsqu’une valeur
étiquetée est utilisée dans une expression, son étiquette est propagée au
résultat de l’expression, ce qui préserve les permissions de l’autorité sur le
résultat.  Dans l’analyse multi-facettes, chaque valeur étiquetée a deux
facettes: une facette contient la valeur “privée” à destination de l’autorité,
une autre facettes contient la valeur “publique” destinée à des observateurs
tiers non autorisés.  Dans une expression, les facettes sont toutes deux
évaluées en même temps afin de produire les deux facettes du résultat.  Afin de
suivre les étiquettes même lors de branchements (des flots /indirects/),
l’évaluation multi-facettes maintient une liste des embranchements suivis lors
de l’exécution; cette liste est appelée /program counter/ (PC).

#+ATTR_LATEX: :float margin
#+CAPTION: Une valeur à deux facettes.
[[file:img/a-facet.pdf]]

Par exemple, dans le code suivant, si le paramètre ~x~ est ~true~, alors la
fonction ~f~ retourne ~true~.  En revanche, si on fait de ~x~ une valeur à
facettes avec une valeur privée ~true~ et une valeur publique ~false~ (qu’on
écrit ~true:false~), alors le premier ~if~ sera exécuté deux fois: une fois pour
chaque facette de la condition.  Après le second ~if~, la fonction retourne la
valeur ~true:false~.  Un observateur non autorisé n’a accès qu’à la valeur
publique du résultat, et n’est donc pas capable d’inférer la valeur privée de
~x~, même à travers un flot indirect d’information.

#+ATTR_LATEX: :height 3.5cm :float t :no-center t
[[file:img/fenton-example.pdf]]

Pour donner une idée de l’échelle du projet, Narcissus fait 6000 lignes de
code[fn:lines-of-code], et les deux plus gros fichiers sont le parseur (1600
lignes) et le fichier principal de interpréteur, “jsexec” (1300 lignes).  Ce
fichier principal contient la logique pour interpréter des arbres de syntaxe
abstraits, et pour mettre en place l’environnement d’exécution des programmes
clients.  Les changements effectués pour l’implémentation de l’évaluation
multi-facettes sont restreints à ce fichier principal; 640 lignes sont
affectées, soit la moitié.

Pour réaliser l’instrumentation de Narcissus pour l’évaluation multi-facettes,
les auteurs ont modifié directement le code source de l’interpréteur Narcissus.
On peut obtenir l’ensemble des changements effectués en extrayant un /diff/ des
deux versions[fn:narcissus-diff].  La figure [[visual-narcissus-diff]] donne une
vue d’ensemble des changements.

#+NAME: visual-narcissus-diff
#+CAPTION: Visualisation des modifications apportées par l’instrumentation de
#+CAPTION: Narcissus pour l’évaluation multi-facettes.  Le diff de “jsexec” y
#+CAPTION: est représenté intégralement, coupés en colonnes de même taille.
#+CAPTION: Chaque ligne est colorée suivant le type de changement dont elle fait
#+CAPTION: partie.  Les lignes grises ne sont pas affectées par
#+CAPTION: l’instrumentation.  [[file:img/narcissus-diff-legend.pdf]]
[[file:img/narcissus-diff.pdf]]

On constate immédiatement que les changements effectués par l’instrumentation
touchent de nombreuses parties du code de l’interpréteur, sans être restreints à
une ou deux régions particulières.  Les changements sont *éparpillés* dans le
code.  De plus, les changements de même nature—appartenant à la même
catégorie—ne sont pas regroupés.  Résultat, il devient difficile de comprendre
les effets de l’instrumentation à l’œil nu, ou de s’assurer de sa justesse par
rapport à une spécification formelle.  Il devient difficile également, sans
connaissances avancées de Narcissus et de l’évaluation multi-facettes, de savoir
si une ligne de code de l’interpréteur instrumenté concerne l’interprétation
décrite par le standard ECMAScript, ou si elle concerne l’évaluation
multi-facettes.  Le code de l’interpréteur instrumenté ne comporte aucune
information qui permet de les distinguer.

Autre point important: l’instrumentation *duplique tout le code* de
l’interpréteur.  C’est une solution simple pour créer un interpréteur qui
supporte l’évaluation multi-facettes.  En revanche, la duplication de code a un
impact important sur la maintenance à long terme: plus de double du code doit
être maintenu.  Les changements requis dans le code source pour corriger un bug
dans Narcissus, ou pour ajouter une fonctionnalité doivent désormais être
répétés dans l’instrumentation.  Le coût de maintenance devient prohibitif
lorsque /plusieurs/ instrumentations sont envisagées.

En observant le diff de plus près, on peut distinguer quatre catégories de
changements: les imports/exports, l’ajout du paramètre /program counter/,
séparer l’évaluation des valeurs à facettes, et les ajouts à l’objet ~global~.

De nouvelles définitions ont besoin d’être importées dans le module de
l’interpréteur, et une nouvelle fonction est exportée.  Ce sont de simples
ajouts qui sont localisés en début et en fin de fichier respectivement.  Voici
comment ils se présentent dans le code:

#+BEGIN_SRC diff
+ var FacetedValue = Zaphod.facets.FacetedValue;
+ var ProgramCounter = Zaphod.facets.ProgramCounter;
...

-      test: test
+      test: test,
+      getPC: getGC
#+END_SRC

Les changements effectués pour accommoder le /program counter/ utilisé par
l’analyse.  D’abord, le constructeur de l’objet ~ExecutionContext~ est étendu
pour accepter un argument supplémentaire: la valeur courante du /program
counter/, ~pc~.  Voici un extrait du diff qui illustre ce
changement[fn:diff-syntax]:

#+BEGIN_SRC diff
- function ExecutionContext(type, version) {
+ function ExecutionContext(type, pc, version {
+   this.pc = pc;
#+END_SRC

Dans Narcissus, une instance de l’objet ~ExecutionContext~ est créée lorsque le
contrôle est transféré à du code client exécutable: lors de l’entrée dans une
fonction, lors d’un appel à ~eval~, ou lors de l’exécution d’un programme
entier.  L’objet ~ExecutionContext~ contient les variables importantes pour
l’exécution du code; en particulier l’environnement lexical utilisé pour
résoudre les noms de variables du code exécuté par ce contexte.  L’objet
~ExecutionContext~ est une réification du concept éponyme de la spécification
ECMAScript[fn:ecma-execution-contexts].

Puisque la signature du constructeur d'~ExecutionContext~ est étendue, tous ses
appels doivent être modifiés en conséquence pour fournir une valeur correcte
pour le paramètre /program counter/.  Il y a plus de 80 instances de ce simple
changement dans l’instrumentation.  En voici deux exemples:

#+BEGIN_SRC diff
- x2 = new ExecutionContext(MODULE_CODE);
+ x2 = new ExecutionContext(MODULE_CODE, x.pc);

- getValue(execute(n.children[0], x));
+ getValue(execute(n.children[0], x), pc);
#+END_SRC

Les changements effectués dans l’exécution de l’arbre de syntaxe abstrait (AST)
pour propager les étiquettes sur les valeurs à facettes.  Par exemple,
additionner deux valeurs à facettes devrait produire une nouvelle valeur à
facettes.  Dans l’implémentation, plutôt que de simplement additionner les deux
opérandes, l’interpréteur doit maintenant d’abord inspecter l’opérande gauche,
et si c’est une valeur à facettes, il faudra ajouter la valeur de l’opérande
droite à chaque facette.  Bien sûr, l’opérande droite peut également être une
valeur à facettes, et il faut alors séparer l’évaluation à nouveau.
L’interpréteur Narcissus ne contient aucun code pour gérer l’addition de deux
valeurs à facettes, donc l’instrumentation doit ajouter la logique nécessaire.
Pour ce faire, chaque évaluation d’une opération est enrobée dans un appel à la
fonction ~evaluateEach~ qui teste si une valeur est à facettes, et appelle
récursivement la fonction d’évaluation sur chaque facette si c’est le cas.  25
appels à ~evaluateEach~ ont été ainsi ajoutés dans l’instrumentation.  Le code
suivant donne la forme générale de ces changements:

#+BEGIN_SRC diff
- var v = getValue(node.a)
+ evaluteEach(getValue(node.a), function(v,x) {
    ... do something with v ...
+ }
#+END_SRC

À la première ligne on récupère une valeur d’un nœud de l’AST (p.ex., l’opérande
gauche d’une assignation, ou la condition d’un ~if~) puis on fait quelque chose
avec cette valeur.  Sur la seconde ligne, on récupère la même valeur, mais cette
fois on /sépare/ l’évaluation en appelant ~evaluateEach~ avec cette valeur comme
premier argument, et comme second une fonction qui opère sur une valeur simple.

Les changements effectués sur l’environnement d’exécution de code client.  Dans
un programme JavaScript, l’environnement d’exécution fournit un objet ~global~
qui contient les définitions de base comme ~Array~, ~Math~, ~String~ et
~Object~.  Puisque Narcissus est meta-circulaire, il réutilise l’objet global de
son environnement hôte pour construire l’objet global de l’environnement client.
Ceci est fait en trois étapes.  Premièrement, Narcissus crée un objet
~globalBase~ avec les propriétés qui surchargeront celle de l’environnement
hôte.  Deuxièmement, il crée un objet ~global~ client à partir de l’objet
~global~ de son environnement hôte, et met toutes les propriétés de ~globalBase~
dans cet objet ~global~ client.  Troisièmement, il ajoute à cet objet ~global~
client les versions réfléchies de certains objets de base (~Array~, ~String~,
~Function~).

L’instrumentation de l’évaluation multi-facettes enrichit l’objet ~global~
client en ajoutant 50 propriétés à ~globalBase~, comme la suivante:

#+BEGIN_SRC diff
   var globalBase = {
   ...
+  isFacetedValue: function(v) {
+    return (v instanceof FacetedValue);
+  },
#+END_SRC

L’instrumentation change également la propriété ~String~ de ~globalBase~ pour
suivre les valeurs étiquetées passées en argument du constructeur de chaînes de
caractères.

Le fait que la plupart des changements appartiennent à une de ces quatre
catégories indique qu’il y a un potentiel de factorisation.  Si l’on souhaite
rendre Narcissus extensibles, afin de pouvoir définir l’évaluation
multi-facettes sans duplication de code, il faut trouver des façons d’exprimer
les changements de ces quatre catégories.  Mais si nous souhaites exprimer
d’autres analyses, et étendre d’autres interpréteurs, il faut tenter des trouver
des mécanismes génériques qui pourront être réemployés pour ces autres cas.

Pour prendre un peu de recul par rapport à l’instrumentation de Narcissus, on
peut s’intéresser à la définition formelle de l’évaluation multi-facettes.
Celle-ci est donnée sous forme d’une sémantique opérationnelle d’un langage
proche du lambda-calcul: \lambda^{facet}.  La sémantique de ce langage est d’abord donnée
sans considérer l’évaluation multi-facettes, et suit une définition usuelle d’un
lambda-calcul en /call-by-value/; \lambda^{facet} contient en plus des constantes, des
références mutables, et une valeur absorbante pour faire écho au ~undefined~ de
JavaScript.

Dans un second temps, une sémantique alternative est présentée qui introduit les
changements nécessaires pour l’évaluation multi-facettes.  Il s’agit d’une copie
de la première sémantique, avec quelques changements et ajouts.  On y retrouve
les deux des quatre catégories de changements dégagées du diff.  Le /program
counter/ accompagne chaque règle d’évaluation, et de nouvelles règles sont
ajoutées pour séparer l’évaluation de valeurs à facette en deux parties.

Les règles de la sémantique instrumentées tiennent sur une page; un
interpréteur pour cette sémantique est donc considérablement plus petit qu’un
interpréteur JavaScript complet, ce qui en fait un excellent choix pour tester
des mécanismes d’extensibilité.


[fn:narcissus-diff] Le /diff/ est extrait des HEAD de [[https://github.com/taustin/narcissus][github/taustin/narcissus]]
et [[https://github.com/taustin/ZaphodFacets][github/taustin/ZaphodFacets]].

[fn:lines-of-code] Lignes physiques, commentaires inclus mais sans compter les
lignes vides.

[fn:diff-syntax] Le symbole ~-~ marque une ligne supprimée de l’interpréteur;
le symbole ~+~ marque une ligne ajoutée par l’instrumentation.  L’absence de
marque indique une ligne commune aux deux versions.

[fn:ecma-execution-contexts] Voir la section 10.3 de la spécification.

* Variations sur un interpréteur de lambda-calcul extensible
Essayons d’abord d’écrire un interpréteur pour chaque sémantique de \lambda^{facet}, sans
anticiper l’instrumentation; c’est à dire sans se préoccuper de pouvoir étendre
l’interpréteur facilement.

Nous appellerons \lambda^{standard} le langage décrit par la première sémantique d’Austin
et Flanagan, qui correspond à un lambda-calcul avec références, et \lambda^{facet} le
langage étendu pour l’évaluation multi-facettes.

Puisque Narcissus est en JavaScript, et suit le patron /module/, écrivons un
interpréteur de \lambda^{standard} dans le même style.

# From [[file:~/Archim%C3%A8de/Th%C3%A8se/lab/lamfa/js/lab/lamfa-narcissus-style.org][lamfa/js/lamfa-narcissus-style.org]]
#+BEGIN_SRC js
var interpreter = (function(){
  var bottom = {...}
  function Address() {...}
  function FunctionObject() {...}

  function ExecutionContext(parent) {
    ...
    this.scope = {}
    this.store = new Store()
  }

  function execute(node, context) {
    switch (node.type) {
    case 'CONST':  ...
    case 'VAR':    ...
    case 'FUN':    ...
    case 'APP':    ...
    ...
    }
  }

  return {
    run: run,
  }
}())
#+END_SRC

Ce simple interpréteur reflète la structure de Narcissus, mais est bien plus
succinct.  Comme dans Narcissus, le gros de la logique de l’interpréteur réside
dans la fonction ~execute~.  Les définitions qui précèdent réifient des objets
de la spécification: ~Address~, ~bottom~, ~FunctionObject~.  On réutilise le nom
~ExecutionContext~ pour indiquer l’objet qui contient l’environnement de
variable et qui est passé au fil des appels récursifs à ~execute~.  En bas ce
sont les fonctions exportées par le module.

On peut maintenant modifier cet interpréteur pour implémenter la seconde
sémantique de \lambda^{facet}.  Comme pour l’instrumentation de Narcissus, on part de
l’interpréteur du langage standard, qu’on modifie par endroits suivant les
besoins de la nouvelle sémantique.  L’idée ici n’est pas de construire un
interpréteur extensible, mais d’identifier les points qui vont varier en
définissant cette variation.  Pour cela, on s’intéresse surtout aux différences
entre les deux versions.

#+CAPTION: Diff simplifié qui illustre les modifications nécessaires pour
#+CAPTION: l’interpréteur \lambda^{facet}.
#+BEGIN_SRC diff
+ function Facet() {...}

+ function evaluateEach() {...}

  function ExecutionContext() {
+   this.pc = []
  }

  function execute(...) {
    ...
    case CALL:
-     v = f._call(a, context)
+     v = evaluateEach(f, context, (f, x) => f._call(a, x))

    case REF:
+     v = constructFacet(context.pc, v, bottom)

    case DEREF:
-     v = a.deref(context)
+     v = evaluateEach(a, context, (a, x) => a.deref(x))
  ...
  }

  return {
    ...
+   runWithPC: runWithPC,
  }
#+END_SRC

Il y a moins de différences que pour l’instrumentation de Narcissus, mais on
retrouve trois des mêmes catégories.  D’abord, il y a de nouvelles définitions
pour les valeurs à facettes et leur évaluation (~Facet~ et ~evaluateEach~).  La
fonction ~evaluateEach~ est utilisée, comme dans l’instrumentation de Narcissus,
pour évaluer les deux parties d’une valeur à facettes récursivement.  Le program
counter est présent, et rajouté à l’objet ~ExecutionContext~ pour pouvoir être
utilisé dans ~execute~.  Enfin, une nouvelle fonction est exportée par le
module.  La seule catégorie non représentée est l’extension de l’objet client
global, car il n’y a pas d’équivalent à l’objet global dans \lambda^{facet}.

Puisqu’on retrouve les mêmes catégories de changement sur cet exemple plus
restreint, on peut supposer que si l’on trouve des moyens d’instrumenter
l’interpréteur de \lambda^{standard} pour l’évaluation à facettes sans duplication de
code, ces moyens seront applicables à Narcissus également.

Il est clair que les choix d’implémentation de l’interpréteur standard sont la
cause de la duplication de code dans l’instrumentation.  Si l’interpréteur
possédait une interface pour être étendu, nous n’aurions pas besoin de dupliquer
le code pour changer ces quelques lignes.  Comment aurions nous dû /construire/
l’interpréteur pour que l’extension requise par \lambda^{facet} puisse être écrite en
minimisant le code dupliqué ?  Ou encore, est-il possible de /modifier/
l’interpréteur standard pour le rendre extensible ?  Ce sont les deux questions
que nous explorons par la suite.

* Construire un interpréteur extensible
Le style de l’interpréteur du chapitre précédent n’est pas extensible.
L’interpréteur suit le patron module en JavaScript, qui a pour intention de
verrouiller les définitions du module contre toute extension future; qu’elles
soient intentionnelles ou accidentelles (le chapitre ?? décrit en détail le
fonctionnement de ce patron).

Mais ce n’est pas la seule façon d’écrire l’interpréteur en JavaScript.  On peut
très bien suivre un style inspiré de la programmation objet.  Le patron
/interpréteur/ [[cite:GHJ+94][GHJ+94]] est d’ailleurs un bon candidat: chaque nœud de l’arbre de
syntaxe est un objet, et contient une méthode ~interpret~.  Le ~switch~ de la
fonction ~execute~ de Narcissus est donc séparé en morceaux indépendants: chaque
nœud possède le code qui permet d’évaluer sa valeurs et ses effets.

Écrivons un interpréteur pour \lambda^{standard} dans ce style.  D’abord, les objets qui
réifient les nœuds de l’AST:

#+BEGIN_SRC js
var stdInterp = {
  CONST: {
    new(e) {...},
    execute(context) {...}},

  VAR: {
    new(e) {...},
    execute(context) {...}},

  FUN: {
    new(argName, body) {...},
    execute(context) {...}},

  APP: {
    new(fun, arg) {...},
    execute(context) {...}},
#+END_SRC

Chaque objet a deux méthodes: une pour l’instancier, et une pour évaluer le nœud
de l’AST.  Les méthodes ~execute~ contiennent la même logique que les ~case~ du
~switch~ de l’interpréteur dans le style de Narcissus.

Les objets de l’exécution subissent la même transformation:

#+BEGIN_SRC js
  Address: {
    new(a) {...},
    deref(context) {...},
  },

  ExecutionContext: {
    new(scope, store) {...},
  },

  Store: {
    new() {...},
    add(value) {...},
    retrieve(addr) {...},
  },

  FunctionObject: {
    new(node, scope) {...},
    _call(arg, x) {...},
  },
#+END_SRC

Et enfin, la fonction d’entrée de l’interpréteur, ~run~, peut simplement appeler
la méthode ~execute~ sur la racine de l’AST.

#+BEGIN_SRC js
  run(node) {
    return node.execute(this.ExecutionContext.new())
  },
})
#+END_SRC

L’intérêt de cette décomposition c’est qu’on peut maintenant facilement créer
une variante de l’interpréteur grâce à la délégation par prototype de
JavaScript:

#+BEGIN_SRC js
var fctInterp = derive(stdInterp, {
  APP: derive(stdInterp.APP, {
    execute(context) {
      ...
      return evaluateEach(...) }})

  REF: derive(stdInterp.REF, {
    execute(context) {
      ...
      return constructFacet(...) }}),

  DEREF: derive(stdInterp.DEREF, {
    execute(context) {
      return evaluateEach(...) }}),

  ExecutionContext: derive(stdInterp.ExecutionContext, {
    new(scope, store, pc) {...}
  }),

  runWithPC(node, pc) {...},
})
#+END_SRC

Ici, ~fctInterp~ a pour prototype ~stdInterp~, donc toute propriété non présente
sur ~fctInterp~ sera prise de ~stdInterp~.  Inversement, la propriété ~APP~ est
définie sur ~fctInterp~, donc elle “surcharge” la propriété ~APP~ de
~stdInterp~.  La délégation par prototype est un mécanisme bien adapté à notre
problème, car on souhaite que le code de ~fctInterp~ exprime la /différence/ de
l’évaluation de \lambda^{facet}, et seulement cette différence.  Dans un langage à
prototype, un objet n’est pas instancié à partir d’une classe, mais /dérivé/
d’un prototype.  Un objet dérivé n’a besoin que de définir en quoi il diffère de
son prototype.  C’est donc un mécanisme de langage idéal pour notre scénario
avec deux interpréteurs.

#+NAME: lamfa-proto
#+CAPTION: Définition d’un interpréteur en utilisant la délégation par
#+CAPTION: prototype.  L’objet ~fctInterp~ possède une unique propriété propre:
#+CAPTION: ~APP~.  Les autres propriétés sont déléguées à son prototype,
#+CAPTION: ~stdInterp~.
[[file:img/lamfa-proto.pdf]]

L’interpréteur de \lambda^{facet} peut donc être défini uniquement en codant les
différences entre ces deux modes d’interprétation.  Les deux objets ~stdInterp~
et ~fctInterp~ coexistent à l’exécution, ce qui permet de pouvoir choisir entre
l’une ou l’autre interprétation pour un même programme.

# Pas si simple, subtilités de références d’objets imbriqués


# Créer de nouveaux interpréteurs à l’exécution.  Pas modifier la même instance
# d’interpréteur.

*** Finding a core example
Looking at the operational semantics for faceted evaluation, we can
see the patterns mentioned previously (=pc= parameter, new cases for
=FacetedValues=).  They are based on a lambda calculus variant, with
references and a “Bottom” value.  Let’s try to write an interpreter
for this lambda calculus without anticipating the later
instrumentation.

We’ll drop the read/write rules since they only add noise to this
example.  We’ll also leave out error handling.

[[file:js/lab/lamfa-es6-standard.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, node) {
    return rules[node.type](σ, θ, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, v1, v2) {
    return application_rules[v1.type](σ, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, e);
    },
  };

  function eval_deref(σ, v) {
    return deref_rules[v.type](σ, v);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}) {
      return σ[a];
    },
  };

  function eval_assign(σ, v1, v2) {
    return assign_rules[v1.type](σ, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = v;
      return σ2;
    },
  };

  let rules = {
    c(σ, θ, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return eval_apply(σ2, v1, v2);
    },

    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = v;
      return [ σ2, address(a) ];
    },

    deref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      return [ σ1, eval_deref(σ1, v) ];
    },

    assign(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return [ eval_assign(σ2, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}) {
    return interpretNode(env, store, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

We used destructuring from ES6 and Unicode identifiers to approximate
the appearance of the big-step semantics.  To effect the operational
rules, we use an ad-hoc pattern matching.  Each AST node is an object
with a =type= field, and the =interpNode= function dispatches to the
function in the =rules= object corresponding to the value of this type
field.  The same pattern matching mechanism is used to distinguish
between an address, a closure or a bottom value.

We can easily instrument this base interpreter by following the
operational semantics from Austin and Flanagan.

[[file:js/lab/lamfa-es6-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, pc, node) {
    return rules[node.type](σ, θ, pc, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, pc, v1, v2) {
    return application_rules[v1.type](σ, pc, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, pc, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, pc, e);
    },

    facet(σ, pc, {k, vh, vl}, v2) {
      if (pc.has(k)) {
        return eval_apply(σ, pc, vh, v2);
      }

      else if (pc.has(-k)) {
        return eval_apply(σ, pc, vl, v2);
      }

      else {
        let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
        let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
        return [ σ2, mk_facet(k, vh1, vl1) ];
      }
    },
  };

  function eval_deref(σ, v, pc) {
    return deref_rules[v.type](σ, v, pc);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}, pc) {
      return σ[a];
    },

    facet(σ, {k, vh, vl}, pc) {
      if (pc.has(k))
        return eval_deref(σ, vh, pc);
      else if (pc.has(-k))
        return eval_deref(σ, vl, pc);
      else
        return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
    },
  };

  function eval_assign(σ, pc, v1, v2) {
    return assign_rules[v1.type](σ, pc, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, pc, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = mk_facet(pc, v, σ[a]);
      return σ2;
    },

    facet(σ, pc, {k, vh, vl}, v) {
      let σ1 = eval_assign(σ, pc.union(k), vh, v);
      return eval_assign(σ1, pc.union(-k), vl, v);
    },
  };

  let rules = {
    c(σ, θ, pc, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, pc, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, pc, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return eval_apply(σ2, pc, v1, v2);
    },

    ref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    },

    deref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      return [ σ1, eval_deref(σ1, v, pc) ];
    },

    assign(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return [ eval_assign(σ2, pc, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return interpretNode(env, store, pc, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );
#+END_SRC

The interesting story is told by looking at the differences between
these two versions.  We see patterns 1 and 2 reappear from our
analysis of the Narcissus instrumentation.  The =pc= parameter must be
passed around in nearly every function, and new cases must be added to
handle facet values.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-es6-standard.js js/lab/lamfa-es6-facets.js; exit 0
#+END_SRC

#+begin_src diff
7,8c7,8
< function interpretNode(σ, θ, node) {
<   return rules[node.type](σ, θ, node);
---
> function interpretNode(σ, θ, pc, node) {
>   return rules[node.type](σ, θ, pc, node);
14a15,28
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
18,19c32,33
< function eval_apply(σ, v1, v2) {
<   return application_rules[v1.type](σ, v1, v2);
---
> function eval_apply(σ, pc, v1, v2) {
>   return application_rules[v1.type](σ, pc, v1, v2);
27c41
<   closure(σ, {x, e, θ}, v) {
---
>   closure(σ, pc, {x, e, θ}, v) {
30c44,60
<     return ↆ(σ, θ1, e);
---
>     return ↆ(σ, θ1, pc, e);
>   },
>
>   facet(σ, pc, {k, vh, vl}, v2) {
>     if (pc.has(k)) {
>       return eval_apply(σ, pc, vh, v2);
>     }
>
>     else if (pc.has(-k)) {
>       return eval_apply(σ, pc, vl, v2);
>     }
>
>     else {
>       let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
>       let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
>       return [ σ2, mk_facet(k, vh1, vl1) ];
>     }
34,35c64,65
< function eval_deref(σ, v) {
<   return deref_rules[v.type](σ, v);
---
> function eval_deref(σ, v, pc) {
>   return deref_rules[v.type](σ, v, pc);
43c73
<   address(σ, {a}) {
---
>   address(σ, {a}, pc) {
45a76,84
>
>   facet(σ, {k, vh, vl}, pc) {
>     if (pc.has(k))
>       return eval_deref(σ, vh, pc);
>     else if (pc.has(-k))
>       return eval_deref(σ, vl, pc);
>     else
>       return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
>   },
48,49c87,88
< function eval_assign(σ, v1, v2) {
<   return assign_rules[v1.type](σ, v1, v2);
---
> function eval_assign(σ, pc, v1, v2) {
>   return assign_rules[v1.type](σ, pc, v1, v2);
57c96
<   address(σ, {a}, v) {
---
>   address(σ, pc, {a}, v) {
59c98
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, σ[a]);
61a101,105
>
>   facet(σ, pc, {k, vh, vl}, v) {
>     let σ1 = eval_assign(σ, pc.union(k), vh, v);
>     return eval_assign(σ1, pc.union(-k), vl, v);
>   },
65c109
<   c(σ, θ, {e}) {
---
>   c(σ, θ, pc, {e}) {
69c113
<   v(σ, θ, {e}) {
---
>   v(σ, θ, pc, {e}) {
73c117
<   fun(σ, θ, {x, e}) {
---
>   fun(σ, θ, pc, {x, e}) {
77,80c121,124
<   app(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return eval_apply(σ2, v1, v2);
---
>   app(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return eval_apply(σ2, pc, v1, v2);
83,84c127,128
<   ref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
---
>   ref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
87c131
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, bottom);
91,93c135,137
<   deref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
<     return [ σ1, eval_deref(σ1, v) ];
---
>   deref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
>     return [ σ1, eval_deref(σ1, v, pc) ];
96,99c140,143
<   assign(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return [ eval_assign(σ2, v1, v2), v2 ];
---
>   assign(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return [ eval_assign(σ2, pc, v1, v2), v2 ];
103,104c147,149
< function interpretProgram(AST, env = {}, store = {}) {
<   return interpretNode(env, store, AST);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return interpretNode(env, store, pc, AST);
117c162,163
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Our ad-hoc pattern matching is perhaps not the most straightforward
way to write such an interpreter in JavaScript.  Another, maybe more
familiar way is to use object-oriented dispatching [fn:: And indeed,
this form is quite similar to the Interpreter pattern from Gamma et
al.].

[[file:js/lab/lamfa-oo-standard.js]]
#+BEGIN_SRC js
  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return v1.eval_apply(σ2, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        return [ σ1, v.eval_deref(σ1, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return [ v1.eval_assign(σ2, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}) {
    return AST.eval(env, store);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

[[file:js/lab/lamfa-oo-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) {
    return {
      eval_apply(σ, pc, v2) {
        if (pc.has(k)) {
          return vh.eval_apply(σ, pc, v2);
        }

        else if (pc.has(-k)) {
          return vl.eval_apply(σ, pc, v2);
        }

        else {
          let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
          let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
          return [ σ2, mk_facet(k, vh1, vl1) ];
        }
      },

      eval_deref(σ, pc) {
        if (pc.has(k))
          return vh.eval_deref(σ, pc);
        else if (pc.has(-k))
          return vl.eval_deref(σ, pc);
        else
          return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
      },

      eval_assign(σ, pc, v) {
        let σ1 = vh.eval_assign(σ, pc.union(k), v);
        return vl.eval_assign(σ1, pc.union(-k), v);
      }
    };
  }

  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, pc, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1, pc);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return v1.eval_apply(σ2, pc, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom);
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        return [ σ1, v.eval_deref(σ1, pc, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return [ v1.eval_assign(σ2, pc, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return AST.eval(env, store, pc);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );

#+END_SRC

But here again, the pattern appears when looking at the diff.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-oo-standard.js js/lab/lamfa-oo-facets.js; exit 0
#+END_SRC

#+begin_src diff
0a1,53
> Set.prototype.union = function(elem) {
>   let n = new Set(this);
>   n.add(elem);
>   return n;
> }
>
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) {
>   return {
>     eval_apply(σ, pc, v2) {
>       if (pc.has(k)) {
>         return vh.eval_apply(σ, pc, v2);
>       }
>
>       else if (pc.has(-k)) {
>         return vl.eval_apply(σ, pc, v2);
>       }
>
>       else {
>         let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
>         let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
>         return [ σ2, mk_facet(k, vh1, vl1) ];
>       }
>     },
>
>     eval_deref(σ, pc) {
>       if (pc.has(k))
>         return vh.eval_deref(σ, pc);
>       else if (pc.has(-k))
>         return vl.eval_deref(σ, pc);
>       else
>         return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
>     },
>
>     eval_assign(σ, pc, v) {
>       let σ1 = vh.eval_assign(σ, pc.union(k), v);
>       return vl.eval_assign(σ1, pc.union(-k), v);
>     }
>   };
> }
>
31c84
<     eval_apply(σ, v) {
---
>     eval_apply(σ, pc, v) {
34c87
<       return e.eval(σ, θ1);
---
>       return e.eval(σ, θ1, pc);
41c94
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
49c102
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
57c110
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
65,68c118,121
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return v1.eval_apply(σ2, v2);
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return v1.eval_apply(σ2, pc, v2);
75,76c128,129
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
79c132
<       σ2[a] = v;
---
>       σ2[a] = mk_facet(pc, v, bottom);
87,89c140,142
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
<       return [ σ1, v.eval_deref(σ1, v) ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
>       return [ σ1, v.eval_deref(σ1, pc, v) ];
96,99c149,152
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return [ v1.eval_assign(σ2, v2), v2 ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return [ v1.eval_assign(σ2, pc, v2), v2 ];
104,105c157,159
< function interpretProgram(AST, env = {}, store = {}) {
<   return AST.eval(env, store);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return AST.eval(env, store, pc);
111c165,166
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Hence, these patterns appear regardless of the instrumented language
(lambda calculus or JavaScript), and regardless of the language
features used by the implementation (pattern matching or dynamic
dispatch).  Therefore, the problem of finding a way to describe the
instrumentation as a module and minimizing coupling is not specific to
Narcissus or JavaScript interpreters.

We will now exhibit variations in writing the instrumentation as
a separate module with minimal code duplication.

*** The expression problem
First, let’s list the additions brought by our extension to the
interpreter:

- we add a new constructor =Couple= to the data type =Term=
- we add a new constructor =Pair= to the data type =Value=
- we add a new case =Couple= to the function =interp=
- we add two new (symmetric) cases =Pair= to the function =plus=

Now, what would a differential description of the extension to our
interpreter look like?

#+BEGIN_SRC haskell
  extend data Term = Couple Int Int
  extend data Value = Pair Int Int

  extend interp (Couple i1 i2) = (Pair i1 i2)
  extend plus (Number a) (Pair i1 i2) = (Pair (a + i1) (a + i2))
  extend plus (Pair i1 i2) (Number a) = (Pair (a + i1) (a + i2))
#+END_SRC

These are all the additions brought by the extension to the original
interpreter.  The new =extend= keyword allows us to:

1. Extend data types with new constructors.
2. Extend function definitions with new cases.

How to extend both data types and functions in a program, without
sacrificing modularity, is a problem known as the /expression problem/
[Wadler].  This (imaginary) =extend= keyword is a solution; there are
real solutions [[[http://www.andres-loeh.de/OpenDatatypes.pdf][Open Data Types and Open Functions]]] (Instead of an
=extend= keyword, they provide an =open= keyword to prefix to initial
declarations of data types and functions.  One could also require both
extended and original codes to include keywords.)

*** The expression problem, with a twist
The expression problem is only concerned with /adding/ data types and
functions, but when we instrument an interpreter, we will often want
to /modify/ its behavior rather than just extend it.

When we take modification into account, what does a differential
description look like?

First, let’s go back to the original interpreter, and modify its
behavior.

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Number i)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

Say we need to change the value returned by the interpretation of the
term ‘Constant’.  We want to return a ‘Pair’ value instead of a single
number, the second value being a default zero.  We would write the
full version, with replication as:

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int | Pair Int Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Pair i 0)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

What would be the differential description of this change?

#+BEGIN_SRC haskell
  extend data Value = Pair Int Int

  modify interp (Constant i) = (Pair i 0)
#+END_SRC

The keyword =modify= replaces the definition for the targeted case of
=interp=.

[What if we want to combine multiple modifications to the same case
function? A: You’d have to have a clue about the precedence order to
make sense of the result, but these problems are shared by other AOP
applications.]

*** The modular instrumentation problem
We have an interpreter I for a language L, and the source code for I.
We want to instrument the interpreter I, by extending and modifying
its behavior.  Namely, the instrumentation can:
- add terms to the base language
- add values to the base language
- add new operations
- alter the behavior of existing operations, or even suppress them
  entirely.

We constrain the instrumentation by imposing the following
restrictions:

- The instrumented interpreter I’ must still be able to execute
  programs written in the language L.  The instrumentation cannot
  remove or modify existing terms of the language.
- The instrumentation must modify only a part of the original
  interpreter operations.  Otherwise, the instrumented interpreter
  may end up with semantics so different from the original interpreter
  that it does not qualify as “instrumentation” anymore; it might as
  well be another interpreter in its own right.

The implementation of this instrumentation will give a new interpreter
I’.  Ideally, this implementation should be as modular as possible; it
should:
- promote isolated reasoning,
- minimize code replication and accidental complexity.  We should be
  able to map the differential description of the instrumentation and
  the code for its implementation.

The /modular instrumentation problem/ is then: how to implement the
instrumentation with the above constraints of modularity?

Note that the changes may bring only additional side effects, and
leave the original behavior unaltered.  How to recognize or enforce
“side-effects only” instrumentation is an interesting
question. [“Recognize” I don’t know how.  “Enforce” you can do with
monads, if you have a monadic interpreter.]

** Variations
*** JavaScript
See [[file:js/aoping.org]].

See also [[file:lassy15.org]] for a way to build interpreters incrementally.

**** Split OO-style instrumented interpreter into modules
Let’s try to separate the object-oriented instrumented interpreter in
two modules: one for the base interpreter, ‘base.js’, and one for the
instrumentation, ‘facets.js’.  The ‘base.js’ file should not contain
any instrumentation-specific code, and be as close as possible to the
standard interpreter.

Looking at the diff, we can put =mk_facet=, =facet= and the extension
to =Set.prototype= in a separate file right away.

***** Handling the extra PC parameter
The second, more pervasive change is the addition of the program
counter context in every call.  There are at least two solutions to
this problem:
1. Using a global variable.
2. Using a context object.

****** Using a global variable
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

We can use a global variable for the ‘pc’.  This eliminates the need
to pass the ‘pc’ as a formal parameter to most functions, and to pass
it down to tail calls.  However, we then need to save the previous
value of this global pc when temporarily changing the current pc to
evaluate branches in =facet.eval_assign= and =facet.eval_apply=.

With the ‘pc’ argument:
#+BEGIN_SRC js
  let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
  let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
#+END_SRC

With a global variable ‘pc’:
#+BEGIN_SRC js
  let pc_old = pc;
  pc = pc.union(k);
  let [σ1, vh1] = vh.eval_apply(σ, v2);
  pc = pc.union(-k);
  let [σ2, vl1] = vl.eval_apply(σ1, v2);
  pc = pc_old;
#+END_SRC

This temporary rewriting of a variable is essentially emulating a
dynamic binding of the ‘pc’ variable.  There are no built-ins
constructs or syntactic sugar for dynamic binding in JavaScript, but
we can add one for this particular variable.

#+BEGIN_SRC js
  function with_pc(new_pc, thunk) {
    let old_pc = pc;
    pc = new_pc;
    let ret = thunk();
    pc = old_pc;
    return ret;
  }
#+END_SRC

Now the =facet.eval_apply= function is cleaner:

#+BEGIN_SRC js
  let [σ1, vh1] = with_pc(pc.union(k), () => vh.eval_apply(σ, v2));
  let [σ2, vl1] = with_pc(pc.union(-k), () => vl.eval_apply(σ1, v2));
  return [ σ2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The ‘pc’ parameter is thus part of the closure of all facet-related
functions.  The =with_pc= construct require mutability in order to
change the current program counter referenced in the closure.
Mutability can be waived as a requirement if the language supports
dynamic scoping [Art of Interp].

****** Using a context object
[[file:js/lab/oo-split-context/base.js]]
[[file:js/lab/oo-split-context/facets.js]]

Using a global variable for just the ‘pc’ and not the store or
environment seems heterogeneous.  We can adopt the position that the
‘pc’ is an extension to the state of the interpreter, and bundle all
this state in a ‘context’ objet passed around in function calls.  Then
we profit from the dynamic nature of JavaScript objects: we can add
any property at runtime.  Rather than hiding the ‘pc’ away, it makes
the state passing explicit, and removes the need for =with_pc=.

This solution has the benefit of homogenizing the order of formal
parameters: the context object will always be the first one, and the
=eval= functions will always return a context object rather than just
the store.

Here is what the third branch of =facet.eval_apply= looks like:

#+BEGIN_SRC js
  let [C1, vh1] = vh.eval_apply(with_pc(C, C.pc.union(k)), v2);
  let [C2, vl1] = vl.eval_apply(with_pc(C1, C.pc.union(-k)), v2);
  return [ C2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The evaluation of functions now explicitly deals with contexts:

#+BEGIN_SRC js
  function app(e1, e2) {
    return {
      eval(C) {
        let [C1, v1] = e1.eval(C);
        let [C2, v2] = e2.eval(C1);
        return v1.eval_apply(C2, v2);
      }
    };
  }
#+END_SRC

The signatures are simpler, though we lose a bit in legibility, as =C=
is opaque: we do not see what the context made is of by looking at the
formal parameters.  Only looking at the entry point
(=interpretProgram=) reveals its contents.

The context object bundles all the state needed by the interpreter,
and its extensions.  Adding another piece of state is modular since
the base interpreter already passes down this context object.

This solution does not require mutability, but it benefits from
dynamic typing.  In the case of the base interpreter, the context
object has type (Store * Environment), while in the faceted
interpreter, it has type (Store * Environment * PC).

Note that we could combine both methods, and avoid passing the context
object explicitly in all functions by using a global variable.  In
that case, both mutability and dynamic typing are leveraged.

***** Handling the change in the reference rule
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

With the extra ‘pc’ out of the way, the only change remaining is the
=mk_facet= call in the reference rule.  This is how runtime faceted
values are effectively created.

#+BEGIN_SRC diff
79c79
<       σ2[a] = mk_facet(pc, v, bottom);
---
>       σ2[a] = v;
#+END_SRC

We can solve this by creating two different versions of the =ref=
function, one with the standard behavior, and one suited for the
instrumentation.

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom); // this line changes
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

But how do we differentiate which version of =ref= to use when calling
=interpretProgram=?  We need namespaces.  Using objects as namespaces,
both versions of =ref= can coexist.

#+BEGIN_SRC js
  this.base = {
    interpretProgram,
    c, v, fun, app, ref, deref, assign,
    _innards: { bottom, address, closure}
  };
#+END_SRC

The base interpreter exports both public interface and a “restricted”
one intended for extension purposes.

#+BEGIN_SRC js
  this.facets = {
    __proto__: this.base,
    interpretProgram,
    ref
  };
#+END_SRC

The instrumented interpreter exports the same interface as the base,
only overriding definitions for =ref= and =interpretProgram=.

Both interpreters can then be tested independently.

#+BEGIN_SRC js
  with (base) {
    console.log('std', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}
    ));
  }

  with (facets) {
    console.log('facets', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}, [1]
    ));
  }
#+END_SRC

Finally, both base interpreters (with a global ‘pc’, and with a
context object) are free of instrumentation-specific concerns.  In
fact, the base interpreter with the global ‘pc’ is /identical/ to the
base object-oriented interpreter.

***** Summary
We split the object-oriented interpreter into two modules:
1. The base interpreter which contains no concerns pertaining to
   faceted evaluation.
2. The facet interpreter which extends the context of the base
   interpreter, either by using a (delimited) global, or by extending
   the context object.  The facet interpreter then selects and
   overrides the entry point (=interpretProgram=) and the reference
   rule (=ref=).

We used two different mechanisms to handle the extra ‘pc’ parameter:
1. A ‘pc’ variable in the closure of all facet-related functions.
   This solution works if the language provides mutable variables or
   dynamic scoping.
2. Modifying the base interpreter to use a context object for the
   store and environment.  We then leveraged the ability to add
   properties to existing objects and included the program counter in
   this context.  Mutability is not required, but dynamic typing
   allows us to use a context object with two incompatibles types.

Overriding the =ref= function required only delegation using the
prototype chain.

**** Split pattern-matching instrumented interpreter into modules
[[file:js/lab/es6-split-global/base.js]]
[[file:js/lab/es6-split-global/facets.js]]

We can handle the extra ‘pc’ parameter in the same ways we did with
the object-oriented interpreter.

However, when we try to extend the =ref= rule, two things are
different.

***** The facet-specific rules are scattered
The =eval_apply=, =eval_deref= and =eval_assign= code for facet values
is split across the three =application_rules=, =deref_rules= and
=assign_rules= objects.  In the object-oriented approach, they were
regrouped under the same =facet= object.

#+BEGIN_SRC js
  function facet(k, vh, vl) {
    return {
      eval_apply(σ, v2) {...},
      eval_deref(σ) {...},
      eval_assign(σ, v) {...}
    };
  }
#+END_SRC

#+BEGIN_SRC js
  base.application_rules.facet = (σ, {k, vh, vl}, v2) => {...};
  base.deref_rules.facet = (σ, {k, vh, vl}) => {...};
  base.assign_rules.facet = (σ, {k, vh, vl}, v) => {...};
#+END_SRC

This is a manifestation of the expression problem, or the “tyranny of
the primary decomposition”: in the object-oriented approach, adding a
term is just adding an object with all its evaluation functions; while
adding evaluation functions requires modifying all the objects.  Note
that since in JavaScript objects can be extended at runtime, the
primary decomposition has a lower impact.

However, here we have to add the =facet= rules to the base objects
directly.  A more modular approach would be to create new rules
objects that extends (by prototype links) the base rules objects.  But
this is not an option here because the base rules objects are enclosed
by the evaluation functions.  If we create a =facet_application_rules=
object like so:

#+BEGIN_SRC js
  facet_application_rules = {
    __proto__: base.application_rules,
    facet(σ, {k, vh, vl}, v2) => {...}
  };
#+END_SRC

Then we have to redefine the =eval_apply= function to call this new
object instead of the =base.application_rules=.

#+BEGIN_SRC js
  function eval_apply(σ, v1, v2) {
    return facet_application_rules[v1.type](σ, v1, v2);
  }
#+END_SRC

But =eval_apply= is in the closure of the =rules.app= function, so we
have to redefine it as well just to be able to update its closure.

#+BEGIN_SRC js
  app(σ, θ, {e1, e2}) {
    ...
    return eval_apply(σ2, v1, v2);
  },
#+END_SRC

And we would have to do the same for every rules object, and for every
function that refer to these rules object.  In the end, we are back to
copy-pasting the base interpreter just to update the closures of its
functions.

It is clear that the lexical scoping of the rules object is the issue.
Being able to refer to these object from a dynamic scope would resolve
it.

***** Extending the ref function
In the object-oriented approach, we redefined the =ref= function in
the facets module and delegated the other rules to the base
interpreter.  Here we cannot do so.

In the object-oriented interpreter, the =ref= function returns an
object which contains its own evaluation method.  Thus, by overriding
this =ref= function we can change the way =ref= nodes are evaluated.
In the client code, a call to =ref= is dynamically dispatched to
either the base or facet version.

In the pattern matching interpreter, the =ref= function returns an
object /without/ an evaluation function -- only bearing a type used by
=interpretNode= to dispatch to the correct evaluation function.  The
evaluation function is in the =rules= object, separated from the
object created by =ref=.  To change the evaluation of =ref= nodes, we
need to change the =ref= function inside the =rules= object, but we
also need to update the dispatching function.

The =rules= object is in the closure of =base.interpretNode=.
Extending the =rules= object with a new =ref= function would not
change the behavior of =base.interpretNode=.

#+BEGIN_SRC js
  let rules = {
    __proto__: base.rules,
    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    }
  };
#+END_SRC

Trying to redefine a new =interpretNode= function in the facets module
to close over this new =rules= object is not a solution.  Rules from
the base interpreter will still call the =base.interpretNode= function
which refer to the =base.rules= object.  Redefining every function
that calls =base.interpretNode= to call =facets.interpretNode= would
work, but that’s basically duplicating the base interpreter.

What does work is to ‘fluid-let’ the =rules= object inside
=facets.interpretProgram=.

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    let old_ref = base._innards.rules.ref;
    base._innards.rules.ref = (σ, θ, {e}) => {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    };
    let r = with_pc(new Set(default_pc), () =>
                    base.interpretProgram(AST, env, store));
    base._innards.rules.ref = old_ref;
    return r;
  }
#+END_SRC

Note that, again, this construct is emulating a dynamic scoping of the
=rules.ref= function.  We could define a function similar to =with_pc=
and write the following instead:

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    return with_ref((σ, θ, {e}) => { ... }, () =>
                    with_pc(new Set(default_pc), () =>
                            base.interpretProgram(AST, env, store)));
  }
#+END_SRC

But at this point, having to write several ‘with’ functions becomes a
pattern we would like to abstract away once and for all.

Furthermore, this emulation is a brittle way of extending the
functionality.  First we add coupling by using the =base._innards=
interface; secondly we disallow any opportunity for combining
extensions (since we do not /extend/ but /replace/ the base
functionality).  A proper dynamic scoping of the =rules= object has
none of these downsides.

As a compromise, we can recognize that the pattern matching done for
AST nodes is trivial: the objects returned by =app=, =ref=, etc. only
have one method, =eval=.  Instead of returning objects, we can return
closures directly and eliminate the need for the =rules= object.

[[file:js/lab/closures/base.js]]
[[file:js/lab/closures/facets.js]]

#+BEGIN_SRC js
  function ref(e) {
    return (σ, θ) => { ... };
  }
#+END_SRC

This allows us to override the functionality of =ref= using a
namespace, as we did in the object-oriented approach.

***** Summary
The two solutions (global variable and context object) for dealing
with the extra ‘pc’ parameter can be applied here as well.  The same
remarks apply.

However, extending the =ref= function requires the ability to modify
values inside the closures of the evaluation function =interpretNode=.
This ability is not provided by the language, but shadowing the =ref=
function (or the rules object, or the =interpretNode= dispatcher) by
using dynamic scoping achieves the same effects.

**** Summary of JavaScript variations
It appears that, the more the interpreter rely on dynamic features,
the easier it is to instrument.  The /dynamic dispatching/ of the
object-oriented interpreter allows the effortless addition of the
=facet= value.  In all the other cases, /dynamic scoping/ was
prescribed.  We also saw that /dynamic typing/ was required at least
in the ‘context object’ solution to the extra ‘pc’ argument.

Intuitively, it makes sense.  We want different instances of the same
names (=interpretProgram=, =ref=, =rules=) to have different
behaviors, depending on context.  This is exactly what dynamic
dispatching is for: the same method slot can refer to different
implementations, depending on the actual instance of the receiver.
This is also the difference between lexical and dynamic scoping: the
former binds free names to their static surrounding context at
definition time, while the latter binds free names to their caller’s
context at runtime.

In a language with dynamic scoping the interpreter should be a breeze
to instrument.  That is the focus of the [[Lisp][Lisp variations]].

** Lisp
[[file:lisp/pm.lisp]]

Common Lisp offers both lexical and dynamic scoping of variables.  We
implement the pattern-matching standard interpreter by defining the
=*rules*= object to be dynamically scoped (by using =defparameter=).
We follow the Common Lisp convention of using stars to surround a
dynamically-scoped name.

#+BEGIN_SRC lisp
  (defparameter *rules*
    `((c . ,(lambda (s env node) ...))

      (ref . ,(lambda (s env node) ...))))

  (defun eval-node (store env node)
    (let ((f (lookup (car node) *rules*)))
      (funcall f store env node)))

  (defun eval-program (AST env store)
    (eval-node store env AST))

  (eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '())
#+END_SRC

Instrumentation is then effortless.  First we define facet-specific
rules by extending the basic rules objects (=append ... *rules*=).

#+BEGIN_SRC lisp
  (defparameter *facets/rules*
    (append
     `((ref . ,(lambda (s env node)
                 ...
                 (mk-facet *pc* v1 bottom)
                 ...)))
     ,*rules*
     ))

  (defparameter *facets/application-rules* ...)
  (defparameter *facets/deref-rules* ...)
  (defparameter *facets/assign-rules* ...)
#+END_SRC

Note the reference to the free dynamic variable =*pc*=.  In the entry
point to facet evaluation, we override the standard rules with the new
ones.  We also declare the =*pc*= argument to be dynamically scoped
inside this call, using =(declare (special *pc*))=.

#+BEGIN_SRC lisp
  (defun facets/eval-program (AST env store *pc*)
    (declare (special *pc*))
    (let ((*rules* *facets/rules*)
          (*application-rules* *facets/application-rules*)
          (*deref-rules* *facets/deref-rules*)
          (*assign-rules* *facets/assign-rules*))
      (eval-program AST env store)))

  (facets/eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '() '(1))
#+END_SRC

So as anticipated, dynamic scoping is an adequate solution to the
issues we ran into in the JavaScript variations.  But dynamic scoping
is not available in all languages.  As a fallback, we saw that a
language with mutable global variables could emulate dynamic scoping.
This begs the question: in an immutable language without dynamic
scoping, how do we deal with the problem of modular instrumentation?

Furthermore, we also saw that dynamic typing was useful in the
‘context’ object case.  What if we want to benefit from the guarantees
provided by static types?  Is the modular instrumentation still
feasible?  Is it cumbersome to write?  That is the focus of the
[[Haskell][Haskell variations]].

** Haskell
Building scaffolding with languages features has the following
advantages:
+ No extra syntax or rewriting program required
+ In statically-typed Haskell, the scaffolding is type-checked

Downsides:
- The scaffolding might is seldom straightforward
- Extension + overriding of existing definitions leads to very complex
  code

Extending the syntax is the same, with pros and cons inversed:
- Extra syntax and rewriting program required
- Rewritten program is type-checked, but transformation must be proven
  correct

Advantages:
+ Lightweight syntax is straightforward to use
- Overriding it still awkward to read

*** Building scaffolding with language features
**** Monadic interpreters
The monadic interpreter is mostly taken from [[http://homepages.inf.ed.ac.uk/wadler/papers/essence/essence.ps][Wadler]].

- [[http://web.cecs.pdx.edu/~mpj/pubs/modinterp.html][Monads Transformers and Modular Interpreters]]
  + [[http://www.cas.mcmaster.ca/~kahl/FP/2003/Interpreter.pdf][Haskell implementation]]

**** Either data type
See [[file:hs/extend-types/Extension.fail.1.hs]].  Types are extended like
so:

: data FacetTerm  = Facet Principal FacetTerm FacetTerm | BaseTerm Term

***** What does work
- Maximum reuse from the file ‘Base.hs’
- Able to execute `term0` and `term1`

***** What fails
- `term2` gives a type error:
    “Couldn't match expected type `Term' with actual type `FacetTerm'”

: term2 = (Lam "y" (Facet 0 (BaseTerm (Lam "x" Bot)) (BaseTerm Bot)))

- Also, have to wrap Base.Term values with the BaseTerm constructor

***** What I wanted
- `eval term2` gives the same result as when using Extended.eval.

- The raw term1 and term2 should type without wrapping values.

***** Why it doesn’t work
A FacetTerm is either a Facet or a BaseTerm.  A Facet can contain
FacetTerms (and hence BaseTerms), but since BaseTerms are just Terms,
they cannot contain Facets.

***** Conclusion
What we really want is to insert the constructor `Facet` into the
existing data type `Term`.

**** Type classes
Another suggestion by Rémi.

#+BEGIN_SRC haskell
interp :: Dom d => Term -> d
interp (Add l r) = myAdd (interp l) (interp r)

class Dom d where
  myAdd :: d -> d -> d

instance Dom Int where
  myAdd = +

instance Dom OddOrEven where
  myAdd = xor
#+END_SRC

Here you must generalize the interpreter, to accomodate multiple
domains.  But at least the generalization is done using types: the
overhead is minimal.  Though you still need to have indirect calls.

> Ismael: Some disadvantages of this approach are discussed in the
Open Data Types paper, in Section 6.4.

***** Multi-param types classes
#+BEGIN_SRC haskell
class Eval term value where
  eval :: term -> value

#+END_SRC

Becomes quite complicated rapidly.  First you need an extension, then
you quickly run into typing issues that are not worth the flexibility
offered by this strategy.

**** Data types à la carte
From Swierstra, 2008.

[[file:hs/extend-types/Extension.swierstra.hs][A first attempt]]

[[file:hs/InterpreterALC.hs][Another one, with Ismael]]

[[file:hs/ALC-Lambda.hs][Lambda calculus with references and bottom]]

[[file:hs/ALC-Lambda-Facets.hs][Lambda calculus with faceted evaluation]]

[[file:hs/ALC-Facets-Flow.hs][Lambda calculus with faceted evaluation and FlowR tainting]]

Overview of this scaffolding:

Pros:
+ Allows type-checked extension of terms

Cons:
- Quite hairy
- Cannot change the resulting value with using the same approach for
  the value type, which would be even /hairier/.

See also my [[Data types  à la carte (2015)][my second attempt]] at Swierstra, in 2015.

**** Implicit arguments
A simple idea: implicit arguments can replace global variables.

Unfortunately, describing an override of the base eval function is still an
issue.  Maybe use type classes?

See [[file:hs/implicit][implicit]].

**** Facets as a monad
The faceted evaluation strategy requires a new state global: the program
counter.  It makes sense to add this state in a state monad to a monadic
interpreter.  This can be done using monad transformers, or monad views, or
whatever is more convenient.

But maybe we can also view facet values themselves as a specific case of the
list monad.

See [[file:hs/list-monad/][list-monad]].

*** Extending the syntax
See [[file:hs/transform/notes.org]]

[[file:hs/transform/tests/2/LC.hs][Lambda calculus with FlowR instrumentation]]

- Cannot override existing definitions (like one would do with aspects)
- Extending the monadic stack is best done with scaffolding, though
  obliviousness is lost

** Modular monadic interpreters
*** The giants
There’s a body of work on monadic interpreters in functional programming
languages built around monads, dating from the ‘90s.

Starting with Eugenio Moggi lectures at Edinburgh in 1990, then followed by
Wadler, Steele, Hudak and Jones.

- Eugenio Moggi.  An abstract view of programming languages. 1990
- Philip Wadler.  The essence of functional programming. 1992
- Guy Steele.  Building interpreters by composing monads. 1994
- Sheng Liang, Paul Hudak, Mark Jones.  Monad Transformers and Modular
  Interpreters. 1995
- Luc Duponcheel.  Using catamorphisms, subtypes and monad transformers for
  writing modular functional interpreters.  1995
- David Spinoza.  Semantic Lego (thesis).  1995

Then there is the, more recent, use of the free monad to build DSLs and
interpreters in functional languages.  Swierstra reports that free monads were
well-known in category theory, but unfamiliar to functional programmers.

- Wouter Swierstra.  Data types à la carte.  2008
- Tom Schrijvers, Bruno C.d.S. Oliveira.  Functional Pearl: The Monad Zipper.
  2010
- Oleg Kiselyov, Amr Sabry, Cameron Swords.  Extensible Effects -- An
  Alternative to Monad Transformers.  2013
- Rúnar Óli.  Compositional application architecture with reasonably-priced
  monads (Scala Days talk).  2014

On the other side of the fence, in OO-land, the go-to solution is a mix of
modules and design patterns.

- Robert Bruce Findler and Matthew Flatt.  Modular Object-Oriented Programming
  with Units and Mixins.  1998
- Bruno C.d.S. Oliveira.  Modular Visitor Components -- A practical Solution to
  the Expression Families Problem.  2009
- Brudo C.d.S. Oliveira, William R. Cook.  Extensibility for the Masses --
  Practical Extensibility with Object Algebras.  2012

**** Data types  à la carte (2015)
Re-reading Swiestra in 2015, my brain parses it much better.

- Coproduct is just disjoint-union.
- Construction of values of the coproduct type are unwieldy, hence the recourse
  to type classes that will automatically call the right injection into the
  coproduct based on the type of the expression.  Expression types are required,
  otherwise the compiler cannot find the right injection.

  The use class constraints and instances to automatically figure out the right
  injection is quite clever.
- Can’t we derive functors, for the coproduct, since they seem rather
  straightforward?  We can.  Probably `deriving Functor` was not available in
  Haskell in 2008, when Swierstra wrote it.
- We can give a larger type signature than necessary, since the compiler cannot
  infer it for us.  =Add :+: Val :+: Add :+: Val= is perfectly acceptable.  In
  practice, this is harmless.
- While we can easily extend the language and compose languages using this
  approach, modifying behavior requires defining a new algebra.  Can we re-use
  parts of a previous algebra?  There is no clue as to how to achieve that in
  this paper.
- Having terms appear in the signature to programs gives you guarantees for
  free.  As illustrated in the paper, a program of type =Term Recall Int= (\sect6)
  will never modify the value of the memory cell.  Also the topic of \sect7, and
  related work on type systems for effects.

**** Wadler — The essence of functional programming
How to build a monadic interpreter.  Also serves as a tutorial on monads.

An interpreter just creates computations inside a monad M.  By changing the
definition of M (its type, unit and bind) /and other small changes/, one is able
to change the evaluation order of arguments (from call-by-value to
call-by-name), or give a non-deterministic interpreter.

Plugging the identity monad reduces the monadic interpreter to a straightforward
lambda-calculus interpreter, showing that the monadic interpreter is a
generalization.

Variation one already requires changes in addition to redefining the monad.
This is inevitable to be able to distinguish between calls of =unitM= that
signal errors from calls that produce ground values.

Variation two requires to extend the =interp= function.  The current position is
saved in the =P= monad, which is akin to a “forgetful” state monad.

One important observation on monadic interpreters is that if the interpreter
code needs to interact with the monad (e.g., for resetting the current
position), then the =interp= code needs to make a call to =resetP=.  Monads are
not oblivious at all in this case.  But if the interpreter code does not need to
interact with the monad, then changes to the monad can affect the meaning of the
interpreter in an oblivious fashion.

*** The bigger picture
In the solutions from the functional programming world, the idea seems to
revolve around reifying the computation (the target program of an interpreter),
and write different interpreters.  In other words, to achieve modularity, one
must turn behavior into data: data has no effect unless it is executed by some
interpreter and turned into an actual computation.

So, essentially we are writing programs in a DSL of which we know one
interpreter (hopefully).  This is high-level, programming the specification.
Then we are free to write other interpreters for the same program, to change its
meaning slightly.

The connection to DSLs of course brings works from other communities to the
table.

Viewing the interpreter as data is something that is free in Lisps.  The code
can always be seen as a data structure that is easily parsed.  If turning your
code to data gives you the greatest modularity, then homoiconic languages have
a clear advantage of not requiring a parser.

* Dynamic scoping to build interpreters
From the LASSY’15 paper.

An arithmetic expression language, the example taken by Wadler and
Odersky as a base to propose solutions to the expression problem.

#+NAME: svg
#+BEGIN_SRC js :exports none
  function toMarkup(obj, open, close, attr) {
    if (typeof obj !== 'object') return obj.toString();

    let tagName = obj.t;
    let attrs = Object.getOwnPropertyNames(obj.attrs).map(n => attr(n, obj.attrs[n]));
    let content = obj.children.map(o => toMarkup(o, open, close, attr)).join('');
    return open(tagName, attrs) + content + close(tagName);
  }

  function toSVG(obj) {
    function openSVG(tagName, attrs) { return '<' + (tagName + ' ' + attrs.join(' ')).trim() + '>' }
    function closeSVG(tagName) { return '</' + tagName + '>' }
    function attrSVG(k, v) { return k  + '="' + v + '"' }
    return toMarkup(obj, openSVG, closeSVG, attrSVG);
  }

  function t(tagName, ...args) {
    let o = {
      t: tagName,
      attrs: Object.create(null),
      a(k, v) { o.attrs[k] = v; return o },
      children: args,
    };
    let shortcuts = ['width', 'height', 'fill', 'stroke', 'cx', 'cy', 'r',
                     'x', 'y', 'x1', 'y1', 'x2', 'y2', 'rx', 'ry',
                     'id', 'viewBox', 'refX', 'refY', 'markerUnits',
                     'markerWidth', 'markerHeight', 'orient',
                     'transform'];
    shortcuts.forEach(s => { o[s] = o.a.bind(o, s) });
    return o;
  }

  function box(names) {
    let marginTop = 0;
    let marginBottom = 10;
    let marginLeft = 7;
    let marginRight = 10
    let fontSize = 15;
    let lineHeight = 18;
    let charWidth = 9;

    let longest = names.map(p => p.length).reduce((a,b) => Math.max(a,b), 0);
    let width = marginLeft + longest * charWidth + marginRight;
    let height = marginTop + marginBottom + lineHeight * names.length;

    let propText = names.map((n, i) => t('text', n).x(marginLeft)
                                                   .y((i+1) * lineHeight)
                                                   .fill('#657b83'));

    let g = t('g',
              t('rect').width(width).height(height).stroke('#657b83').fill('#fdf6e3'),
              ...propText)
      .a('font-size', fontSize)
      .a('font-family', 'Monospace')
      .height(height)
      .width(width);

    return g;
  }

  function link(a, b, options) {
    let {vertical, dashed, reversed, offset} = options || {};
    offset = offset || 15;

    let arrowLength = 50;
    let arrowTip = 15;
    let space = arrowLength + arrowTip;

    if (vertical) {
      var g = above(a, b, space);
      [a, b] = g.children;
      var y1 = a.attrs.height;
      var y2 = y1 + arrowLength;
      var x1 = offset, x2 = offset;
      var center = {x: x1, y: y1 + space / 2};
    } else {
      var g = beside(a, b, space);
      [a, b] = g.children;
      var x1 = a.attrs.width;
      var x2 = x1 + arrowLength;
      var y1 = offset, y2 = offset;
      var center = {x: x1 + space / 2, y: y1};
    }

    let arrow = t('line').x1(x1).x2(x2).y1(y1).y2(y2).stroke('#fdf6e3')
      .a('stroke-width', 5).a('marker-end', 'url(#triangle)');

    if (dashed) arrow.a('stroke-dasharray', '3 1');
    if (reversed) arrow.transform(`rotate(180 ${center.x} ${center.y})`);

    return t('g', a, arrow, b).width(g.attrs.width).height(g.attrs.height);
  }

  function beside(a, b, hspace) {
    hspace = hspace || 20;

    return t('g', a, b.transform(`translate(${a.attrs.width + hspace}, 0)`))
      .width(a.attrs.width + hspace + b.attrs.width)
      .height(Math.max(a.attrs.height, b.attrs.height));
  }

  function above(a, b, vspace) {
    vspace = vspace || 20;
    return t('g', a, b.transform(`translate(0, ${a.attrs.height + vspace})`))
      .width(Math.max(a.attrs.width, b.attrs.width))
      .height(a.attrs.height + vspace + b.attrs.height);
  }

  function name(n) {
    let g = box([n]);
    g.children[0].rx(10).ry(10);
    return g;
  }

  function ref(a, b, options) {
    options = options || {};
    options.dashed = true;
    return link(a, b, options);
  }

  let proto = link;
  let obj = function(o) { return box(Object.getOwnPropertyNames(o)) };

  function svg(...gs) {
    let triangle = t('marker', t('path').a('d', 'M 0 0 L 10 5 L 0 10 z'))
      .id('triangle').viewBox('0 0 10 10').refX(0).refY(5)
      .markerWidth(4).markerHeight(3)
      .orient('auto').fill('#fdf6e3');

    print(toSVG(t('svg', triangle, ...gs)));
  }
#+END_SRC

** The base datatype
#+NAME: num
#+BEGIN_SRC js
  var num = {
    new: function(n) { return {__proto__: this, n} },
    eval: function() { return this.n }};
#+END_SRC

#+BEGIN_SRC js
  <<num>>

  var e1 = num.new(3);
  print(e1.eval());
#+END_SRC

#+RESULTS:
: 3

#+BEGIN_SRC js :results silent
  <<num>>

  <<svg>>
  redirect('img/num.svg')
  svg(ref(name('e1'),
          proto(obj(num.new(3)),
                ref(obj(num), name('num'),
                    {reversed:true}))))
#+END_SRC

# [[file:img/num.svg]]

** Adding a data variant
#+NAME: plus
#+BEGIN_SRC js
  var plus = {
    new: function(l, r) { return {__proto__: this, l, r,} },
    eval: function() { return this.l.eval() + this.r.eval() }};
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>

  var e2 = plus.new(num.new(1), num.new(2));
  print(e2.eval());
#+END_SRC

#+RESULTS:
: 3

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>

  <<svg>>
  redirect('img/num-plus.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(3)),
                      ref(obj(num), name('num'),
                          {reversed:true})))

  var $e2 = ref(name('e2'),
                proto(obj(plus.new(num.new(1), num.new(2))),
                      ref(obj(plus), name('plus'),
                          {reversed:true})))

  svg(above($e1, $e2))
#+END_SRC

# [[file:img/num-plus.svg]]

** Adding an operation
#+NAME: show-invasive
#+BEGIN_SRC js
  num.show = function() { return this.n.toString() }
  plus.show = function() { return this.l.show() + '+' + this.r.show() }
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>

  var e2 = plus.new(num.new(1), num.new(2));

  <<show-invasive>>

  print(e2.show()); // Dynamic extension, without recreating the expression
  print(plus.new(num.new(1), num.new(2)).show());
#+END_SRC

#+RESULTS[dcac8561b9f21e4bb1985c63e9e70ff10ee557dc]:

This extension is invasive: it modifies the prototypes of =num= and
=plus=.  If we want, we can extend safely both objects.

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show-invasive>>

  <<svg>>
  redirect('img/show-invasive.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(3)),
                      ref(obj(num), name('num'),
                          {reversed:true})))

  var $e2 = ref(name('e2'),
                proto(obj(plus.new(num.new(1), num.new(2))),
                      ref(obj(plus), name('plus'),
                          {reversed:true})))

  svg(above($e1, $e2))

#+END_SRC

# [[file:img/show-invasive.svg]]

** Adding an operation as a module
#+NAME: show
#+BEGIN_SRC js
  var show = function(base) {
    var num = {__proto__: base.num,
      show() { return this.n.toString() }};

    var plus = {__proto__: base.plus,
      show() { return this.l.show() + '+' + this.r.show() }};

    return {num, plus};
  };
#+END_SRC

#+NAME: show-ex1
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  var s = show({num, plus});
  print(s.plus.new(s.num.new(1), s.num.new(2)).show());
#+END_SRC

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  let s = show({num, plus});
  let e3 = s.plus.new(s.num.new(1), s.num.new(2));

  <<svg>>
  redirect('img/show-module.svg')
  let $s = ref(name('s'), obj(s));
  let $e2 = ref(name('e2'), proto(obj(plus.new(num.new(1), num.new(2))),
                                  ref(obj(plus), name('plus'), {reversed:true})));

  let $e3 =  ref(name('e3'),
                 proto(obj(e3),
                       ref(obj(e3.__proto__),
                           name('s.plus'), {reversed:true})));

  let g = link($e2, $e3, {vertical:true, reversed:true, offset: 220});
  svg(above($s, g))
#+END_SRC

# [[file:img/show-module.svg]]

Works, but can mix languages in unsafe ways:

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  var s = show({num, plus});

  try { print(s.plus.new(num.new(1), s.num.new(2)).show()); }
  catch (e) { print(e) }
#+END_SRC

#+RESULTS:
: TypeError: this.l.show is not a function

*** A use-case for =with=
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  with(show({num, plus})) {
    print(plus.new(num.new(1), num.new(2)).show())
  }
#+END_SRC

#+RESULTS:
: 1+2

Cannot mix languages anymore because of name shadowing: only one =num=
and one =plus= is known in the body of =with=, and they are both from
the same language.

Inside =with=, here is what we see:

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  let s = show({num, plus});

  <<svg>>
  redirect('img/show-module-with.svg')
  let $num = ref(name('num'), proto(obj(s.num), obj(s.num.__proto__)));
  let $plus = ref(name('plus'), proto(obj(s.plus), obj(s.num.__proto__)));

  svg(above($num, $plus))
#+END_SRC

# [[file:img/show-module-with.svg]]

Outside =with=, the =show= module is out of scope:

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  <<svg>>
  redirect('img/show-module-with-outside.svg')
  let $num = ref(name('num'), obj(num));
  let $plus = ref(name('plus'), obj(plus));

  svg(above($num, $plus))
#+END_SRC

# [[file:img/show-module-with-outside.svg]]

*** Selective imports with an IIFE
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  (function({num}) {
    print(num.new(1).show())
  }(show({num, plus})))
#+END_SRC

#+RESULTS:
: 1

Of course, here =plus= is in context, but we would actually put it in
a =base= module as well.

The two forms have a subtle difference: in a =with= we can modify the
values of the scope object by assigning to them, but in the IIFE,
assigning to the arguments has no effect outside the function.  In our
two examples, we always pass a fresh module so there is no issue.

** Modifying an operation
#+BEGIN_SRC js
<<num>>
<<plus>>

num.eval = function() { return this.n * 2 }

print(num.new(1).eval())
print(plus.new(num.new(1), num.new(2)).eval())
#+END_SRC

#+RESULTS:
: 2
: 6

Previous version of =num.eval= is lost: we have no reference to it
anymore.

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>

  <<svg>>
  redirect('img/modify-num.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(1)),
                      ref(box(['new', 'eval: this.n * 2']), name('num'),
                          {reversed:true})))

  var $prev = box(['new', 'eval: this.n'])

  svg(above($e1, $prev))
#+END_SRC

# [[file:img/modify-num.svg]]

*** Non-destructive modification
#+NAME: double
#+BEGIN_SRC js
<<num>>
<<plus>>

var double = function(num_orig) {
  var num = {__proto__: num_orig,
    eval() { return num_orig.eval.call(this) * 2 }}
  return {num}
}
#+END_SRC

#+BEGIN_SRC js
<<double>>

with(double(num)) {
  with(double(num)) {
    print(plus.new(num.new(1), num.new(2)).eval())
  }
}
#+END_SRC

#+RESULTS:
: 12

Inside the inner-most =with=, the objects in scope are the modified
=num=, and the original =plus=.

#+BEGIN_SRC js :results silent :exports none
  <<svg>>
  redirect('img/modify-num-module.svg')
  var $num = ref(name('num'),
                proto(box(['eval: previous() * 2']),
                      proto(box(['eval: previous() * 2']),
                            box(['new', 'eval: this.n']))))

  var $plus = ref(name('plus'), obj(plus))

  svg(above($num, $plus))
#+END_SRC

# [[file:img/modify-num-module.svg]]

While after the =with=, =num= refers to the original, unmodified object.

#+BEGIN_SRC js :results silent :exports none
  <<plus>>
  <<svg>>
  redirect('img/modify-num-module-outside.svg')
  var $num = ref(name('num'), box(['new', 'eval: this.n']))
  var $plus = ref(name('plus'), obj(plus))

  svg(above($num, $plus))
#+END_SRC

# [[file:img/modify-num-module-outside.svg]]

** Passing state
Add a program counter incremented each time a data variant calls
=eval=.

#+NAME: state
#+BEGIN_SRC js
  var state = function(base, pc = 0) {
    var num = {__proto__: base.num,
               eval() { pc++; return base.num.eval.call(this) }}

    var plus = {__proto__: base.plus,
               eval() { pc++; return base.plus.eval.call(this) }}

    var getPC = () => pc

    return {num, plus, getPC}
  }
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<state>>

  with (state({num, plus})) {
    print(getPC())
    print(plus.new(num.new(1), num.new(2)).eval())
    print(getPC())
  }
#+END_SRC

#+RESULTS:
: 0
: 3
: 3

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<state>>

  var s = state({num,plus});

  <<svg>>
  redirect('img/state.svg')
  var $getPC = ref(name('getPC'), box(['() => pc']))
  var $num = ref(name('num'), proto(obj(s.num), obj(s.num.__proto__)))
  var $plus = ref(name('plus'), proto(obj(s.plus), obj(s.plus.__proto__)))

  svg(above($getPC, above($num, $plus)))
#+END_SRC

# [[file:img/state.svg]]

** All in one
Combine all the extensions without effort.

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>
  <<state>>
  <<double>>

  with (state({num,plus})) {
    with (double(num)) {
      with (show({num,plus})) {
        print(getPC())
        let n = plus.new(num.new(1), num.new(2))
        print(n.eval())
        print(getPC())
        print(n.show())
      }}}
#+END_SRC

#+RESULTS:
: 0
: 6
: 3
: 1+2

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>
  <<state>>
  <<double>>

  with (state({num,plus})) {
    with (double(num)) {
      with (show({num,plus})) {
        var pc = getPC
        var n = plus.new(num.new(1), num.new(2))
      }}}

    <<svg>>
    redirect('img/all-in-one.svg')

  var $getPC = ref(name('getPC'), box(['() => pc']))
  var $nl = ref(name('n.l'),
                proto(obj(n.l),
                      proto(obj(n.l.__proto__),
                            proto(box(['eval', '(from double)']),
                                  proto(box(['eval', '(from state)']),
                                        ref(obj(n.l.__proto__.__proto__.__proto__.__proto__), name('num'),
                                            {reversed:true}))))))
  var $n = ref(name('n'),
               proto(obj(n),
                     proto(obj(n.__proto__),
                           proto(box(['eval', '(from state)']),
                                 ref(obj(n.l.__proto__.__proto__.__proto__), name('plus'),
                                     {reversed:true})))))

  svg(above($getPC, above($n, $nl)))
#+END_SRC

# [[file:img/all-in-one.svg]]

* Dynamic scoping to modify Narcissus
From the DLS’15/SAC’16 paper.

** The idea: manipulating scopes
Looking back, we see that we are mostly concerned with scopes and the ability to
manipulate them to add or remove bindings.  So we can describe what we want,
abstractly, using scopes manipulation primitives.

A JavaScript subset to talk about scopes.   Think of scopes as JavaScript
objects, which are dictionaries with property lookup delegation.

Narcissus uses a module pattern.  Function + interface returned in an object.

#+BEGIN_SRC js
  var m = (function(){
      var a = 1
      var f = function(x) { return x + a }
      var g = function(x) { return f(x) }
      return {g}
  }())

  print(m.g(0))
#+END_SRC

#+RESULTS:
: 1
: undefined

A module defines its own scope, and exports some of them in an object.

Before the code runs.

#+BEGIN_SRC dot :file img/ex1-0.png
digraph {
  subgraph clusterglobal {
    label = "global";
    color = white;
    global [shape = record, label = ""];
  }
}
#+END_SRC

#+RESULTS:
[[file:img/ex1-0.svg]]

After the IIFE is executed.

#+BEGIN_SRC dot :file img/ex1-1.png
digraph {
  node [shape = record, height = 0];

  subgraph clusterglobal {
    label = "global"; color = white;
    global [label = "{{m|<m> •}}"];
  }

  return [label = "g|<g> •"];

  subgraph clusterIIFE {
    label = "IIFE scope"; color = white;
    IIFE [label = "{{a|1}|{f|<f> •}|{g|<g> •}}"];
  }


  subgraph {
    rank = same;
    f [label = "{{scope|<scope>•}|{code|x + a}}"];
    g [label = "{{scope|<scope>•}|{code|f(x)}}"];
  }

  global:m -> return [style = dashed];
  return:g -> g [style = dashed];
  IIFE:f -> f [style = dashed];
  IIFE:g -> g [style = dashed];
  IIFE -> global;
  f:scope -> IIFE [style = dashed];
  g:scope -> IIFE [style = dashed];

}
#+END_SRC

#+RESULTS:
[[file:img/ex1-1.svg]]

Ignoring =Object.prototype= and =Function.prototype=, since they are not
important for understanding the module pattern here.

Calling =m.g(0)=.

#+BEGIN_SRC dot :file img/ex1-2.png
digraph {
  node [shape = record, height = 0];

  subgraph clusterglobal {
    label = "global"; color = white;
    global [label = "{{m|<m> •}}"];
  }

  return [label = "g|<g> •"];

  subgraph clusterIIFE {
    label = "Activation of IIFE"; color = white;
    IIFE [label = "{{a|1}|{f|<f> •}|{g|<g> •}}"];
  }

  subgraph {
    rank = same;
    f [label = "{{scope|<scope>•}|{code|x + a}}"];
    g [label = "{{scope|<scope>•}|{code|f(x)}}"];
  }

  global:m -> return [style = dashed];
  return:g -> g [style = dashed];
  IIFE:f -> f [style = dashed];
  IIFE:g -> g [style = dashed];
  IIFE -> global;
  f:scope -> IIFE [style = dashed];
  g:scope -> IIFE [style = dashed];

  subgraph clusterAOG {
    label = "Activation of g"; color = white;
    AOG [label = "{{x|0}}"];
  }

  AOG -> IIFE;

  subgraph clusterAOF {
    label = "Activation of f"; color = white;
    AOF [label = "{{x|0}}"];
  }

  AOF -> IIFE;
}
#+END_SRC

#+RESULTS:
[[file:img/ex1-2.svg]]

To modify the bindings inside the module, we must be able to manipulate its
inner scope.  To change the meaning of =a= or =f= inside the module, we must
manipulate the activation object of the IIFE.

Given access to the inner scope, we can make the interpreter do what we want.

Let’s say that the scope of the module is accessible from the =m.scope=
property, then we can write:

#+BEGIN_SRC js
  var m = (function(){
      var a = 1
      var f = function(x) { return x + a }
      var g = function(x) { return f(x) }
      return {g}
  }())

  m.g(0) //: 1
  m.scope.a = 2
  m.g(0) //: 2
#+END_SRC

After the line =m.scope.a = 2=, we have:

#+BEGIN_SRC dot :file img/ex1-3.png
digraph {
  rankdir=BT;
  node [shape = record, height = 0];

  global [label = "m|<m>•"];
  IIFE [label = "{{a|2}|{f|<f>•}|{g|<g>•}}"];
  AOG [label = "x|0"];
  AOF [label = "x|0"];

  return [label = "{{g|<g>•}|{scope|<scope>•}}"];
  f [label = "{{scope|<scope>•}|{code|x + a}}"];
  g [label = "{{scope|<scope>•}|{code|f(x)}}"];


  subgraph clusterActivation {
    color = white;
    IIFE -> global;
  }

  AOG -> IIFE:s;
  AOF -> IIFE:s;

  subgraph clusterObjs {
    color = white;
    return -> g -> f [style=invis];
  }

  global:m -> return [style=dashed, constraint=true];

  return:g -> g:se [style = dashed, constraint=true];
  return:scope:s -> IIFE:ne [style = dashed, constraint=false];

  IIFE:f:e -> f:sw [style=dashed, weight=0, constraint=true];
  IIFE:g:e -> g:se [style=dashed, weight=0, constraint=true];

  f:scope:e -> IIFE:ne [style=dashed, constraint=false];
  g:scope:e -> IIFE:ne [style=dashed, constraint=false];
}
#+END_SRC

#+RESULTS:
[[file:img/ex1-3.svg]]

We can change bindings for =a=, =f=.

This destroys the original activation object, so we should fluid-let if we want
to undo the changes.  So we put a scope in front of the activation of the IIFE,
and we can change bindings and delete our changes.

#+BEGIN_SRC js
  var m = (function(){
      var a = 1
      var f = function(x) { return x + a }
      var g = function(x) { return f(x) }
      return {g}
  }())

  m.g(0) //: 1
  m.scope.a = 2
  m.g(0) //: 2
  delete m.scope.a
  m.g(0) //: 1
#+END_SRC

#+BEGIN_SRC dot :file img/ex2-1.png
digraph {
  rankdir=BT;
  node [shape = record, height = 0];

  global [label = "m|<m>•"];
  IIFE [label = "{{a|1}|{f|<f>•}|{g|<g>•}}"];
  frontScope [label = "a|2", color=deeppink2];
  AOG [label = "x|0"];
  AOF [label = "x|0"];

  return [label = "{{g|<g>•}|{scope|<scope>•}}"];
  f [label = "{{scope|<scope>•}|{code|x + a}}"];
  g [label = "{{scope|<scope>•}|{code|f(x)}}"];

  subgraph clusterActivation {
    color = white;
    frontScope -> IIFE [minlen=2];
    IIFE -> global;
  }

  subgraph clusterObjs {
    color = white;
    g -> f -> return [style=invis];
  }

  AOG -> frontScope;
  AOF -> frontScope;

  frontScope -> g [weight=0, style=invis];

  global:m -> return [style = dashed, constraint=false];
  return:g:e -> g:nw [style = dashed, constraint=false];
  IIFE -> f [style=dashed, constraint=false];
  IIFE:g -> g:nw [style = dashed, constraint=false];

  f:scope -> frontScope:e [style = dashed, constraint=false];
  g:scope:e -> frontScope:e [style = dashed, constraint=false];
  return:scope:e -> frontScope:e [style = dashed, constraint=false];
}
#+END_SRC

#+RESULTS:
[[file:img/ex2-1.svg]]

Second requirement: we need to write functions that make use of the bindings
inside the module.

Since we have a reference to the scope, we can tap into it to use the bindings.

Note that =m.scope.a= can potentially be instrumented, before or after we
redefine =f=.  This makes our instrumentation compatible with further extension.

#+BEGIN_SRC js
  var m = (function(){
      var a = 1
      var f = function(x) { return x + a }
      var g = function(x) { return f(x) }
      return {g}
  }())

  m.g(0) //: 1
  m.scope.f = function(x) { return x + 2 * m.scope.a }
  m.g(0) //: 2
  m.scope.a = 4
  m.g(0) //: 8
#+END_SRC

Can of course write the instrumentation as a module, and any further
instrumentation would have access to its scope.

However, there is potential for conflicts if two instrumentations redefine =f=
in the exposed scope.

Layered approach: scopes have parents from which they inherit bindings.  We can
inject our scope as a parent to the module scope, to pack instrumentation
bindings together.  Then we can easily toggle the instrumented bindings.

Multiple layers can be combined in this way.  Lookup is always deterministic, as
scopes have only one parent.  The latest layer takes precedence.
Instrumentations can insert themselves in any layer of the instrumentation cake.

#+BEGIN_SRC js
  var m = (function(){
      var a = 1
      var f = function(x) { return x + a }
      var g = function(x) { return f(x) }
      return {g}
  }())

  m.g(0) //: 1
  var layer = {a:2}
  addLayer(m.g.scope, layer)
  m.g(0) //: 2
  removeLayer(m.g.scope, layer)
  m.g(0) //: 1
#+END_SRC

#+BEGIN_SRC dot :file img/ex3-1.png
digraph {
  rankdir=BT;
  node [shape = record, height = 0];

  global [label = "{{m|<m>•}}"];
  return [label = "{{g|<g>•}|{scope|<scope>•}}"];
  IIFE [label = "{{a|1}|{f|<f>•}|{g|<g>•}}"];
  AOG [label = "{{x|0}}"];
  AOF [label = "{{x|0}}"];

  frontScope [label = ""];
  layer [label = "a|2", color=deeppink2];

  f [label = "{{scope|<scope>•}|{code|x + a}}"];
  g [label = "{{scope|<scope>•}|{code|f(x)}}"];

  subgraph clusterScopes {
    color = white;
    frontScope -> layer -> IIFE -> global;
  }

  subgraph clusterObjs {
    color = white;
    g -> f -> return [style=invis];
  }

  frontScope -> g [weight=0, style=invis];

  AOG -> frontScope;
  AOF -> frontScope;

  global:m -> return [style = dashed, constraint=false];
  return:g:e -> g:nw [style = dashed, constraint=false];
  IIFE -> f [style=dashed, constraint=false];
  IIFE:g -> g:nw [style = dashed, constraint=false];

  f:scope -> frontScope:e [style = dashed, constraint=false];
  g:scope:e -> frontScope:e [style = dashed, constraint=false];
  return:scope:e -> frontScope:e [style = dashed, constraint=false];
}
#+END_SRC

#+RESULTS:
[[file:img/ex3-1.svg]]


Can control which bindings can be overridden by adding yet another layer at the
bottom of the scope, which is not accessible to instrumenters.

Could also use a proxy object instead of returning the scope itself.

#+BEGIN_SRC dot :file img/ex4-1.png
digraph {
  rankdir=BT;
  node [shape = record, height = 0];

  global [label = "{{m|<m>•}}"];
  return [label = "{{g|<g>•}|{scope|<scope>•}}"];
  IIFE [label = "{{a|1}|{f|<f>•}|{g|<g>•}}"];
  AOG [label = "{{x|0}}"];
  AOF [label = "{{x|0}}"];

  frontScope [label = ""];
  layer [label = "a|2", color=deeppink2];
  protectedBindings [label = "f|<f>•"];

  f [label = "{{scope|<scope>•}|{code|x + a}}"];
  g [label = "{{scope|<scope>•}|{code|f(x)}}"];

  subgraph clusterScopes {
    color = white;
    protectedBindings -> frontScope -> layer -> IIFE -> global;
  }

  subgraph clusterObjs {
    color = white;
    g -> f -> return [style=invis];
  }

  frontScope -> g [weight=0, style=invis];

  AOG -> protectedBindings;
  AOF -> protectedBindings;

  global:m -> return [style = dashed, constraint=false];
  return:g:e -> g:nw [style = dashed, constraint=false];
  IIFE -> f [style=dashed, constraint=false];
  IIFE:g -> g:nw [style = dashed, constraint=false];

  f:scope -> protectedBindings:ne [style = dashed, constraint=false];
  g:scope:e -> protectedBindings:ne [style = dashed, constraint=false];
  return:scope:e -> frontScope:e [style = dashed, constraint=false];

  protectedBindings:f:e -> f:se [style = dashed, constraint=false];
}
#+END_SRC

#+RESULTS:
[[file:img/ex4-1.svg]]

This is all very pretty, but there is no exposed scope property in JavaScript.
How do we make this work?

** How to manipulate scopes in JavaScript
Here we find a way to approximate this scope manipulation using standard
JavaScript.

=with= is the mechanism of choice to manipulate scopes.  There are several ways
to use it once you realize you have a backdoor to the lexical closure.

=with= introduces a scope for the inner code using a plain JS object.

#+BEGIN_SRC js
var o = {a:42}
with (o) {
  function f() { return a }
}
print(f())
#+END_SRC

#+RESULTS:
: 42
: undefined

The scope introduced by =with= has the surrounding scope as a parent.

=with= is not dynamic scoping; it’s lexical scoping using a dynamic object.
Importantly, lookup is still done lexically, not through the stack.

#+BEGIN_SRC js
var o = {a:42}
function f() { return a }
with (o) {
  print(f())
}
#+END_SRC

#+RESULTS:
: ReferenceError: a is undefined

=with= creates a scope that can be used for instrumentation.

#+BEGIN_SRC js
var m = (function(){
  var scope = Object.create(null)
  with(scope) {
    var a = 1
    var f = function(x) { return x + a }
    var g = function(x) { return f(x) }
    return {g,scope}
  }
}())

print(m.g(0))
m.scope.a = 2
print(m.g(0))
#+END_SRC

#+RESULTS:
: 1
: 2
: undefined

=with= is harmless with empty binding object, only a performance penalty for an
additional (dynamically changing) scope to lookup.

Using =with= is not emulating perfectly the simplified situation exposed with
the diagrams earlier.

For requirement #2, it is not sufficient because we have a reference to the
/binding object/, and not the /lexical environment/ itself.  The binding object
/does not/ delegate property lookup to the activation object.

#+BEGIN_SRC js
var m = (function(){
  var scope = Object.create(null)
  with(scope) {
    function f() {}
  }
  return scope
}())
#+END_SRC

#+BEGIN_SRC dot :file img/with-lexenv-1.png
digraph {
  node [shape = record];
  rankdir=BT;

  global [label = "m|<m>•"];
  lexenv [label = ""];
  binding [label = ""];
  activation [label = "f|<f>•"];
  f [label = "{{scope|<scope>•}|{code|<code>}}"];

  subgraph clusterScopes {
    color = white;
    lexenv -> activation -> global;
  }

  global:m:s -> binding:nw [style = dashed, constraint=false];
  activation:f:e -> f:s [style = dashed];
  f:scope:e -> lexenv:ne [style = dashed];
  lexenv -> binding [style = dotted, constraint=false];
}
#+END_SRC

#+RESULTS:
[[file:img/with-lexenv-1.svg]]

To make the binding object behave like the lexical environment, must populate
its prototype.

#+BEGIN_SRC js
var m = (function(){
  var scope = Object.create(null)
  with(scope) {
    function f() {}
  }
  var outer = {__proto__:null, f}
  Object.setPrototypeOf(scope, outer);
  return scope
}())
#+END_SRC

#+BEGIN_SRC dot :file img/with-lexenv-2.png
digraph {
  node [shape = record];
  rankdir=BT;

  global [label = "m|<m>•"];
  lexenv [label = ""];
  binding [label = ""];
  activation [label = "f|<f>•"];
  f [label = "{{scope|<scope>•}|{code|<code>}}"];
  outer [label = "f|<f>•", color = deeppink2];

  subgraph clusterScopes {
    color = white;
    lexenv -> activation -> global;
  }

  global:m:s -> binding [style = dashed, constraint=false];
  activation:f:e -> f:s [style = dashed];
  f:scope:e -> lexenv:ne [style = dashed];
  binding -> outer [color = deeppink2]
  outer:f -> f:s [style = dashed];
  lexenv -> binding [style = dotted, constraint=false];

}
#+END_SRC

#+RESULTS:
[[file:img/with-lexenv-2.svg]]

Since =outer= is not the same object as the activation object of the module, we
can control which bindings are exposed, with different granularity (with
conventions).

It’s somewhat unfortunate and unnecessary duplication.  However, in practice on
Narcissus it amounts to 20 ‘dumb’ declarations.  And it means we cannot actually
remove the original bindings in the IIFE activation object, which is arguably a
good thing.

Cannot restrict which bindings are instrumentable; all bindings are,
potentially.  But we could actually do that if we exposed not the binding
object, but a proxy to it.  The proxy would filter the names we don’t want to be
exposed.  Convoluted, and we don’t have a use case.

Comparison to putting all bindings in global scope, rather than using this
pattern:
- global scope needs manual namespacing (i.e., prefixing bindings).
- overriding is easy, canceling an override needs to manually keep track of the
  overridden.
- open scope maintains an unalterable copy of the original bindings in module
  scope.
- in Narcissus, global scope would require a tedious rewriting.  Open scope only
  requires additions at the top and bottom of the file (local changes, easily
  removable).

A layered approach makes use of prototype delegation in the scope object.

#+BEGIN_SRC js
var m = (function(){
  var scope = Object.create(null)
  with(scope) {
  var a = 1
  function f(x) { return x + a }
  function g(x) { return f(x) }
  var outer = {f,g,a}
  Object.setPrototypeOf(scope, outer)
  return scope
  }
}())

function pushLayer(l, chain) {
  Object.setPrototypeOf(l, Object.getPrototypeOf(chain))
  Object.setPrototypeOf(chain, l)
}

function popLayer(l, chain) {
  while (chain && Object.getPrototypeOf(chain) !== l)
    chain = Object.getPrototypeOf(chain)

  if (Object.getPrototypeOf(chain))
    Object.setPrototypeOf(chain, Object.getPrototypeOf(Object.getPrototypeOf(chain)))
}

m.g(0) //: 1
var s = {a:2, f(x) { return x + 2 * m.a }}
pushLayer(s, m)
m.g(0) //: 4
var s2 = {f() { return -m.a }}
pushLayer(s2, m)
m.g(0) //: -2
popLayer(s, m)
m.g(0) //: -1
#+END_SRC

** Evaluation on Narcissus
Application of open scope pattern to Narcissus.

Other changes:
1. Salami the =switch= inside =execute= into functions.  Instrumentation can
   override behavior for each AST node type.
2. Refactor the =reflectClass= calls in a =populateEnvironment= function that
   can be called after instrumentation has extended the =globalBase= object.
3. Extract and name anonymous functions used to populate =Function.prototype=.
4. Add bindings to the returned object.
5. Add variable for prompt string (to be able to change it!).

Refactoring is correct, does not change behavior of interpreter (same output
with RAW and BASE for test262 suite).

Benchmarks setup: i5-3320M 2.60GHz, running test262 from mozilla-central,
through Narcissus on top of Spidermonkey.  Tests are mostly small programs,
only a handful take longer than 5s to execute.  Startup times might dominate
over actual =with= performance.  Octane benchmarks aim to measure performance
for “realistic applications”.

Performance is on par with straightforward instrumentation.  20% increase when
comparing non-instrumented interpreter.  10% increase when comparing
instrumented (analysis matters more).

| Interpreter     | time (seconds) |
|-----------------+----------------|
| raw (baseline)  |           1167 |
| base            |           1410 |
|-----------------+----------------|
| facets (Austin) |           1233 |
| facets (us)     |           1357 |

Instrumentation stands in its own file; 440 non-blank lines (+ 300 of
definitions).  Shorter in part because changes to PC are avoided by putting it
on ExecutionContext.

Facets instrumentation is terse; can at a glance see what parts are affected by
faceted evaluation.

Can check that facets instrumentation does not alter semantics of JS code when
program counter is empty.  (But this could have been done by Austin had they run
the test262 suite).

FlowR is another information flow analysis which can be easily expressed in its
own file, on top of base.

Tracing can be defined on the side, and applied to facets or base.

We have corrected bugs in Narcissus without touching the instrumented
evaluation, since we reduce copy-pasting.  (e.g., =S12.11_A1_T3.js=)

** Discussion
*** Insights
Let the bindings be dynamically changed, and that’s all you need.  Use bindings
for every bit of functionality (i.e., identify moving pieces and put them in
variables or functions).

JavaScript has mechanisms that promote modular instrumentation, though they are
often shunned because they can create subtle bugs.  =with= creates a local
dynamic scope, and this is precisely what you want when instrumenting.

Tension between exposing bindings and information hiding/encapsulation.  Good
practice argues for minimizing exposition to implementation details from client
code.  Open scope goes against this practice.  We would argue that
instrumentation is special case of “client code”, which definitely /needs/
access to implementation details, unless a clean interface is provided (which
often is not).  See also LVG-10: extensions for Firefox often used deep
modifications into Firefox code (monkey patching).  See also Minecraft:
extensions writers not afraid of modifying obfuscated Java bytecode.  Yes,
ideally there should be a clean interface for instrumenters.  No, in practice
there is none.  Open scope is not ideal, but practical.

*** Broad applications
Not only dynamic analyses, but any variation of the interpreter.  Strict mode
comes to mind.  As it applies to the module pattern, there is nothing specific
to an interpreter there.  Extending any module is possible.  Browsers, games,
and text editors all often provide custom interfaces for extension, but the open
scope can be a low-cost option for customization.

More generally, it applies to /customizing software/ at the source level.

*** Other JS interpreters
ES5.1 interpreter written using a module pattern:
- https://github.com/metaes/metaes/blob/master/metaes.js

Written as an object (in strict mode):
- https://github.com/NeilFraser/JS-Interpreter/blob/master/interpreter.js

Written as several modules (ES5 + flow analysis):
- http://www.jsflow.net/

*** Related work
**** Open modules
Module with open scope is not ‘open modules’ [Aldrich, ECOOP’05].  Open modules
offer a clearly-defined boundary between the module writer and the writer of an
extension to this module.  In our case, we do not presume to know what interface
an instrumentation would require, so we leave the doors wide open.  Also, open
modules is based on AOP, which we don’t require here.

The open scope is not a contract with the writer of an instrumentation; it’s a
convenience.  The instrumentation writer is responsible if the base writer
changes its bindings.  This is not modular, but unanticipated extension is not
modular.

**** Isn’t a visitor pattern enough?
In general, a good idea.  But Narcissus is not object-oriented, so it cannot
apply here.

Point of the approach is to minimize the changes to Narcissus.  Otherwise we
could just rewrite from scratch and design for extensibility.

**** Object-oriented interpreter
Other ways to instrument would depend on the form of the target interpreter.
Here we have a module pattern with lexical scoping.  What if the interpreter was
written as a class?  The function bindings would be available to extending
classes, and we could write the instrumentation using inheritance.

But rewriting Narcissus using a class was not an option: the instrumentation
ought to be lightweight.

Using dynamic inheritance, we would be able to inject a trace class and modify
running instances of interpreters.  This suggests that dynamic inheritance is in
some way similar to dynamic scoping.  If that is the case, then we can focus on
scope.

Just by comparing their effect on bindings, we can see that they both solve the
problem equally: =with= creates an indirection of binding just like =this= does
on object attributes.

**** ...AOP?
- No satisfying AOP framework we could apply.  AspectScript is costly because
  parsing + rewriting.  LVG-10 is custom modification of Microsoft’s JIT.
- Building our own would not be pragmatic.
- Open scope would not be applicable when you cannot modify base code.  AOP
  would be a better choice then.  Performance costs vary, but the highest cost
  is technical debt: who writes the AOP extension, who maintains it?  Open scope
  is just standard JavaScript.

**** Scoping strategies
Scoping strategies specify how aspects are applied dynamically.

Here we just manipulate bindings.  Essentially, an instrumentation is just
defining =around= advices.

Tanter also provided an intuition of scoping strategies outside the aspect
paradigm.  A scoping strategy for bindings is a triplet of two propagation
functions, and one filter (activation) function.

The two propagation functions determine if the binding propagates on 1) the call
stack, or 2) in delayed lambdas.  The filter function can toggle the binding for
any function execution event.

In our case, we do not initiate the call to the interpreter, and our code is not
lexically shared, so the propagation functions would not help us in overriding
the bindings.  The activation function is then useless, as it serves only to
toggle bindings present in the environment.

However, if we redefined =evaluate= we could override the bindings using
standard dynamic scoping semantics.  It is unclear whether the added
expressiveness of scoping strategies would be needed, or more useful.

*** Objections and downsides
**** =with= is deprecated
=with= is disallowed in Strict mode because it makes static analysis difficult
by violating lexical scope.  But this is precisely why we need it in this
pattern.  This evokes an interesting compromise in language design: security
(knowing in advance what the program will do) versus flexibility (leaving room
for the program to change meaning dynamically).  These two goals are
antagonists.

Since JS does not have dynamic scoping, =with= is very helpful in our case.  Not
sure we could do it as easily in, say, Python.

[[https://google-styleguide.googlecode.com/svn/trunk/javascriptguide.xml#with___{}][Google's style guide]] prohibits =with=.  As does [[http://javascript.crockford.com/code.html#with%2520statement][Douglas Crockford]].  The [[https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/with][MDN page]]
lists pro and contra arguments.

**** Open scope defeats the purpose of the module pattern
Module pattern is for hiding information, and exposing only what is required.

Open scope leaves a backdoor that exposes private bindings.

The two are antagonists.

Is open scope necessary when the interpreter is not written defensively?

Is open scope applicable when the interpreter uses another pattern for hiding
information?  (WeakMap, Symbols, or dunder prefix).

**** It’s not modular if there is no interface for instrumentation
Open scope means an instrumentation is free to change any binding, without an
explicit interface.  No interface means no contract between instrumentation
writer and module writer.  Instrumentation is brittle if module is liable to
change.

Attitude is more: “here is a free pass to change anything inside the module; you
are responsible if your instrumentation breaks in the future”.  Because it’s
more convenient to open scope rather than having to design an interface, or add
hooks everywhere.

**** It’s still ad-hoc
Some refactoring made to Narcissus was specific to the implementation (salami
the =switch=, =reflectClass=).  But the open scope pattern makes few hypotheses
on the language (must deal with names, bindings, and scopes).

True that we are not generic in the sense “now you can write the facets
implementation once, and run on any interpreter satisfying Our Genial
Interface”.  But that is a pipe dream, as different languages will require
adaptation of analysis (facets is incomplete on JS).  At least now you can
eyeball the implementation of facets and compare it to a spec document.

**** Application of open scope if not module pattern?
The idea is to manipulate scopes, so it is general.  /How/ to manipulate the
scope depends on the situation.  If the target is not a module pattern, then
maybe =with= is not the right tool to use.

If the target is OO, =this= also provides a kind dynamic scope.

**** Open scope only captures top-level declaration of module
Declarations inside functions are not captured, cannot be overridden.

True.  Such declarations should be moved to top-level of module for this pattern
to work.  Same goes for anonymous functions, which should be named.
Implementation of open scope pattern is responsibility of module writer; such
changes are required for the pattern.

Can be useful for preventing bindings to be overridden, or for hiding
information.

**** Aliasing inside the module can be problematic
Open scope allows an instrumentation to change bindings inside the module.  If a
binding is aliased inside the module, the instrumentation should override the
alias as well.  This creates redundant noise in the instrumentation.

#+BEGIN_EXAMPLE
a = V
b = a
#+END_EXAMPLE

#+BEGIN_EXAMPLE
a -> V
     ^
     |
b ---/
#+END_EXAMPLE

=a= and =b= are references to the same value =V=.  If the instrumentation want
to override the value =V=, it needs to override both bindings =a= and =b= in the
open scope.

#+BEGIN_EXAMPLE
a ---> V’
b ---> V’
#+END_EXAMPLE

However, it could be the case that only one binding should be overridden.

#+BEGIN_EXAMPLE
a ---> V’
b ---> V (no change)
#+END_EXAMPLE

If we need both facilities, we need a way to say that we override at the left of
the arrow (per binding) or at the right (per value).

That’s not an issue we have encountered in instrumenting Narcissus.

**** What about interferences between instrumentations?
Is
: njs -l flowr -l trace
the same as
: njs -l trace -l flowr

If not, what do you have to pay attention to when writing analyses?

**** Performance costs
Are expected due to the added indirection.

We could partially evaluate the interpreter + the activated instrumentations to
produce an interpreter without =with=, but with direct references to the
overloaded bindings.

We would then lose the ability to change the bindings dynamically, as far as I
can see.

* Synthèse
** Séparation des préoccupations: pourquoi?
- Séparation difficile à obtenir
- Difficile de définir les frontières
  - une préoccupation est rarement isolée du reste du programme
- Cause d’autres problèmes
  - appels implicites, perte de compréhension du flot de contrôle
- Pointcut ~ dynamic scoping ~ COMEFROM
  - mécanismes puissants mais peu connus
  - usage difficile à justifier hors cas vraiment spécifiques
- Mauvais sens des priorités
  - Un programme doit d’abord être correct.
  - Puis il doit être maintenable -> bonne documentation des choix de structure
  - Séparation des préoccupations = cerise sur le gâteau.  Mais où est le gâteau?

#+LATEX: \input{backmatter}
