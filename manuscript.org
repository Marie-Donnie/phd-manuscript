# -*- org-confirm-babel-evaluate: nil; org-babel-use-quick-and-dirty-noweb-expansion: t; org-image-actual-width: 300; ispell-local-dictionary: "french" -*-
#+STARTUP: nologdone
#+TITLE: Mécanismes de langages pour étendre des interpréteurs
#+AUTHOR: fmdkdd
#+LANGUAGE: fr
#+OPTIONS: tags:nil H:4 num:3 toc:3 ':t

#+MACRO: acr @@latex:\textsc{$1}@@
#+MACRO: color @@html:<span class="color $1">▬</span>@@ @@latex:\colorrule{$1}@@
#+MACRO: emph @@latex:\emph{$1}@@ @@html:<i>$1</i>@@
#+MACRO: ast arbre syntaxique abstrait
#+MACRO: adt type algébrique de données

#+HTML_DOCTYPE: html5
#+HTML_HEAD: <link rel="stylesheet" href="style.css">

#+TODO: BARE(b) UNSTABLE(u) TRANSLATE(t) | STABLE(d)

#+BEGIN_QUOTE
"I could tell you my adventures--beginning from this morning," said Alice a
little timidly: "but it's no use going back to yesterday, because I was a
different person then.'

"Explain all that," said the Mock Turtle.

"No, no! The adventures first," said the Gryphon in an impatient tone:
"explanations take such a dreadful time."
— [[cite:Car-66][Car-66]]
#+END_QUOTE

* Contents                                                   :TOC@4:noexport:
 - [[#acknowledgements][Acknowledgements]]
 - [[#abstract][Abstract]]
 - [[#résumé][Résumé]]
 - [[#bare-introduction][BARE Introduction]]
   - [[#problème-étendre-un-interprèteur-par-de-multiple-analyses][Problème: étendre un interprèteur par de multiple analyses]]
   - [[#but-mécanismes-pour-étendre-simplement-un-interpréteur-en-préservant-la-séparation-des-préoccupations][But: mécanismes pour étendre simplement un interpréteur en préservant la séparation des préoccupations]]
   - [[#portée-interpréteurs-en-javascript][Portée: interpréteurs en JavaScript]]
   - [[#survol][Survol]]
 - [[#contexte][Contexte]]
   - [[#unstable-de-lexécution-du-programme-au-code-source][UNSTABLE De l'exécution du programme au code source]]
   - [[#unstable-apprivoiser-le-flot-de-contrôle-la-programmation-structurée][UNSTABLE Apprivoiser le flot de contrôle: la programmation structurée]]
   - [[#stable-modéliser-le-monde-la-programmation-par-objets][STABLE Modéliser le monde: la programmation par objets]]
     - [[#classes-et-héritage-smalltalk][Classes et héritage: Smalltalk]]
     - [[#prototypes-et-délégation-self][Prototypes et délégation: Self]]
   - [[#réifier-le-langage-pour-séparer-les-préoccupations][Réifier le langage pour séparer les préoccupations]]
     - [[#translate-la-réflexion][TRANSLATE La réflexion]]
     - [[#stable-limplémentation-ouverte][STABLE L'implémentation ouverte]]
     - [[#stable-la-programmation-par-aspects][STABLE La programmation par aspects]]
       - [[#la-distinction-entre-les-composants-et-les-aspects][La distinction entre les composants et les aspects]]
       - [[#exprimer-les-aspects-par-un-langage-dédié][Exprimer les aspects par un langage dédié]]
       - [[#dautres-mécanismes-pour-la-programmation-par-aspects][D'autres mécanismes pour la programmation par aspects]]
       - [[#aspectj-jonctions-coupes-et-méthodes-daspect][AspectJ: jonctions, coupes, et méthodes d'aspect]]
       - [[#le-mécanisme-essentiel-pour-la-programmation-par-aspects][Le mécanisme essentiel pour la programmation par aspects]]
       - [[#les-limites-de-la-programmation-par-aspects][Les limites de la programmation par aspects]]
   - [[#stable-tisser-les-facettes-dun-artefact-dinnombrable-dimensions][STABLE Tisser les facettes d'un artefact d'innombrable dimensions]]
     - [[#la-programmation-littéraire][La programmation littéraire]]
     - [[#la-séplution][La séplution]]
     - [[#linformation-transparente][L'information transparente]]
   - [[#bare-limiter-les-effets-de-bord-la-programmation-fonctionnelle][BARE Limiter les effets de bord: la programmation fonctionnelle]]
   - [[#la-modularité-du-programme-nest-pas-la-modularité-du-code-source][La modularité du programme n'est pas la modularité du code source]]
     - [[#stable-la-modularité-selon-parnas][STABLE La modularité selon Parnas]]
       - [[#conclusions][Conclusions]]
     - [[#stable-la-théorie-de-naur-derrière-le-programme][STABLE La théorie de Naur derrière le programme]]
       - [[#conclusions-1][Conclusions]]
     - [[#notion-of-modularity][Notion of modularity]]
 - [[#le-problème-étendre-des-interpréteurs][Le problème: étendre des interpréteurs]]
   - [[#unstable-motivation-extension-ad-hoc-de-narcissus][UNSTABLE Motivation: extension ad-hoc de Narcissus]]
   - [[#bare-formalisation][BARE Formalisation]]
   - [[#le-problème-de-lexpression][Le problème de l'expression]]
 - [[#travaux-connexes][Travaux connexes]]
   - [[#les-mécanismes-de-recomposition-du-programme][Les mécanismes de recomposition du programme]]
     - [[#bare-le-problème-de-lexpression][BARE Le problème de l'expression]]
     - [[#bare-dynamic-binding][BARE Dynamic binding]]
     - [[#bare-building-from-modules][BARE Building from modules]]
     - [[#bare-building-with-monads][BARE Building with monads]]
     - [[#bare-bytecode-instrumentation][BARE Bytecode instrumentation]]
     - [[#superimposition][Superimposition]]
       - [[#bare-caesar][BARE Caesar]]
       - [[#translate-software-product-lines][TRANSLATE Software product lines]]
       - [[#translate-semantic-patches][TRANSLATE Semantic patches]]
   - [[#unstable-travaux-connexes-concernant-linstrumentation][UNSTABLE Travaux connexes concernant l'instrumentation]]
     - [[#bare-domain-specific-languages][BARE Domain-specific languages]]
     - [[#bare-scripting-languages][BARE Scripting languages]]
     - [[#translate-emacs][TRANSLATE Emacs]]
        - [[#mechanisms-for-extension][Mechanisms for extension]]
     - [[#translate-eclipse-and-other-ides][TRANSLATE Eclipse and other IDEs]]
       - [[#mechanisms-for-extension-1][Mechanisms for extension]]
     - [[#bare-web-browsers][BARE Web browsers]]
       - [[#mechanisms-for-extension-2][Mechanisms for extension]]
     - [[#translate-lua][TRANSLATE Lua]]
        - [[#mechanisms-for-extension-3][Mechanisms for extension]]
 - [[#unstable-variations-sur-un-interpréteur-de-lambda-calcul-extensible][UNSTABLE Variations sur un interpréteur de lambda-calcul extensible]]
 - [[#translate-construire-un-interpréteur-extensible][TRANSLATE Construire un interpréteur extensible]]
     - [[#finding-a-core-example][Finding a core example]]
     - [[#the-expression-problem][The expression problem]]
     - [[#the-expression-problem-with-a-twist][The expression problem, with a twist]]
     - [[#the-modular-instrumentation-problem][The modular instrumentation problem]]
   - [[#variations][Variations]]
     - [[#javascript][JavaScript]]
       - [[#split-oo-style-instrumented-interpreter-into-modules][Split OO-style instrumented interpreter into modules]]
       - [[#split-pattern-matching-instrumented-interpreter-into-modules][Split pattern-matching instrumented interpreter into modules]]
       - [[#summary-of-javascript-variations][Summary of JavaScript variations]]
   - [[#lisp][Lisp]]
   - [[#haskell][Haskell]]
     - [[#building-scaffolding-with-language-features][Building scaffolding with language features]]
       - [[#monadic-interpreters][Monadic interpreters]]
       - [[#either-data-type][Either data type]]
       - [[#type-classes][Type classes]]
       - [[#data-types-à-la-carte][Data types à la carte]]
       - [[#implicit-arguments][Implicit arguments]]
       - [[#facets-as-a-monad][Facets as a monad]]
     - [[#extending-the-syntax][Extending the syntax]]
   - [[#modular-monadic-interpreters][Modular monadic interpreters]]
     - [[#the-giants][The giants]]
       - [[#data-types--à-la-carte-2015][Data types  à la carte (2015)]]
       - [[#wadler--the-essence-of-functional-programming][Wadler — The essence of functional programming]]
     - [[#the-bigger-picture][The bigger picture]]
 - [[#translate-construire-un-interpréteur-par-modules][TRANSLATE Construire un interpréteur par modules]]
   - [[#ajouter-des-termes][Ajouter des termes]]
   - [[#ajouter-des-opérations][Ajouter des opérations]]
   - [[#modifier-des-opérations][Modifier des opérations]]
   - [[#passer-de-létat-aux-opérations][Passer de l'état aux opérations]]
 - [[#dynamic-scoping-to-build-interpreters][Dynamic scoping to build interpreters]]
   - [[#the-base-datatype][The base datatype]]
   - [[#adding-a-data-variant][Adding a data variant]]
   - [[#adding-an-operation][Adding an operation]]
   - [[#adding-an-operation-as-a-module][Adding an operation as a module]]
     - [[#a-use-case-for-with][A use-case for =with=]]
     - [[#selective-imports-with-an-iife][Selective imports with an IIFE]]
   - [[#modifying-an-operation][Modifying an operation]]
     - [[#non-destructive-modification][Non-destructive modification]]
   - [[#passing-state][Passing state]]
   - [[#all-in-one][All in one]]
 - [[#stable-Étendre-un-interpréteur-par-manipulation-de-portée][STABLE Étendre un interpréteur par manipulation de portée]]
   - [[#manipuler-la-portée-des-variables-pour-linstrumentation][Manipuler la portée des variables pour l'instrumentation]]
     - [[#la-portée-dans-le-motif-module][La portée dans le motif module]]
     - [[#ouvrir-le-motif-module][Ouvrir le motif module]]
     - [[#disposer-les-environnements-en-couches][Disposer les environnements en couches]]
   - [[#ouvrir-le-motif-module-en-javascript][Ouvrir le motif module en JavaScript]]
   - [[#Étendre-narcissus-par-manipulation-de-portée][Étendre Narcissus par manipulation de portée]]
     - [[#ajouter-lanalyse-dévaluation-multi-facettes][Ajouter l'analyse d'évaluation multi-facettes]]
     - [[#ajouter-lanalyse-de-trace][Ajouter l'analyse de trace]]
     - [[#ajouter-lanalyse-flowr][Ajouter l'analyse FlowR]]
     - [[#ajouter-lanalyse-denvironnements][Ajouter l'analyse d'environnements]]
     - [[#Évaluation][Évaluation]]
   - [[#discussion][Discussion]]
     - [[#la-tension-entre-flexibilité-et-sûreté][La tension entre flexibilité et sûreté]]
     - [[#dautres-façons-détendre-linterpréteur][D'autres façons d'étendre l'interpréteur]]
     - [[#pourquoi-ne-pas-utiliser-la-programmation-par-aspects][Pourquoi ne pas utiliser la programmation par aspects?]]
     - [[#appliquer-la-manipulation-de-portée-à-dautres-langages][Appliquer la manipulation de portée à d'autres langages]]
     - [[#inconvénients-de-la-manipulation-de-portée-avec-with][Inconvénients de la manipulation de portée avec ~with~]]
 - [[#bare-synthèse][BARE Synthèse]]
   - [[#bare-séparation-des-préoccupations-pourquoi][BARE Séparation des préoccupations: pourquoi?]]
   - [[#bare-locality-of-concerns-and-locality-of-execution-are-irreconciable-in-the-source-text][BARE Locality of concerns and locality of execution are irreconciable in the source text]]
   - [[#bare-late-binding-is-the-common-ingredient-to-all-modular-mechanisms][BARE Late binding is the common ingredient to all modular mechanisms]]
 - [[#références][Références]]

* Acknowledgements                                                 :noexport:
Raganwald for a Game of Life implementation showing off literate programming and
AOP, and getting me interested in finding better ways to structure programs.

Bret Victor for the realization of the dissonance between textual programming
languages and the dynamic processes they describe.  Also, for inspiration.

* Abstract                                                         :noexport:
Programs are hard to write, and harder to maintain.  Programs often are part of
an /ongoing/ development process: they evolve as requirements change, as new
features are added, and as defects are uncovered and fixed.  To ease
maintenance, programmers need tools.  New programming languages are created
every day to enhance the productivity of programmers, to simplify the conception
of programs.  Object-Oriented Programming, Metaprogramming, Aspect-Oriented
Programming, Feature-Oriented Programming, Functional Programming: all promise a
better way to write safer programs.

But how do these paradigms help /maintain/ programs?  And how do we write
extensible programs?  How do we extend existing ones?  What mechanisms can we
use?  What are the patterns of extensibility?  How do languages help or hinder
extensibility?  Answering these questions would lead us to design more
extensible programs, and to a better understanding extensibility itself.

To try to answer these questions, we take a simple, non-trivial example of
program to extend: an interpreter.  We start with an interpreter for a
lambda-calculus, and review the techniques used to extend the language with new
terms and operations; a setting similar to the Expression Problem.  We show how
to build interpreters incrementally using layers, and then we apply those
techniques to extend a full-blown JavaScript interpreter with dynamic analyses.

We find that /late-binding/ is a key ingredient to extensibility, but it is also
at odds with another important concern: safety.

* Résumé                                                           :noexport:
Il est difficile d'écrire un programme, mais il l'est encore plus de le
maintenir.  Les programmes sont souvent en développement /continu/: ils évoluent
quand les besoins changent, quand de nouvelles fonctionnalités sont ajoutées, et
quand leurs malfaçons sont découvertes et corrigées.  Pour faciliter cette
maintenance, les programmeurs ont besoin d'outils.  Tous le jours, de nouveaux
langages de programmation sont créés pour augmenter la productivité des
programmeurs, pour simplifier la conception des programmes.  La programmation
par objets, la meta-programmation, la programmation par aspects, la
programmation par fonctionnalité, la programmation fonctionnelle: tous
promettent une meilleure façon d'écrire des programmes plus sûrs.

Mais comment ces paradigmes aident à /maintenir/ les programmes?  Et comment
peut-on écrire des programmes extensibles?  Comment étendre des programmes
existants?  Quels mécanismes doit-on utiliser?  Quels sont les motifs de
l'extensibilité?  Comment les langages encouragent ou entravent l'extensibilité?
Répondre à ces questions nous permettrait de concevoir des programmes plus
extensibles, et d'obtenir une meilleure compréhension de l'extensibilité
en soi.

Pour tenter de répondre à ces questions, on prend un exemple de programme simple
et non-trivial: un interpréteur.  On commence par un interpréteur de
lambda-calcul, et l'on passe en revue les techniques utilisées pour étendre le
langage par de nouveaux termes et opérations; un cas similaire au problème de
l'Expression.  On montre comment construire des interpréteurs par couches
successives, et on applique ces techniques pour étendre un véritable
interpréteur JavaScript par des analyses dynamiques.

On observe que la /liaison tardive/ est un ingrédient clé de l'extensibilité,
mais que c'est en désaccord avec une autre propriété importante: la sûreté.

* BARE Introduction
** Problème: étendre un interprèteur par de multiple analyses
- Contexte de sécurité web.
- Scripts de pages web passent par un interpréteur.
- Sécuriser un script = analyser ses fonctionnalités
  - runtime monitoring
  - access control
  - logging
- Une analyse dynamique = une modification de l'interpréteur
- Modification du code source en conflit avec la séparation des préoccupations
  - perte d'extensibilité, perte de lisibilité, difficulté de maintenance...
- Ajouter une analyse devrait être simple
  - sans requérir des modifications invasives de l'interpréteur
  - maximiser la flexibilité, minimiser le coût d'adoption
- Ajouter une analyse ne doit pas impacter la lisibilité du code de l'interpréteur
  - séparation des préoccupations
- Analyses peuvent se composer sans effort particulier (lorsqu'elles
  n'interfèrent pas entre elles)

** But: mécanismes pour étendre simplement un interpréteur en préservant la séparation des préoccupations
- Trouver des mécanismes, des constructions (patterns) pour étendre des interpréteurs
- Améliorer la situation

** Portée: interpréteurs en JavaScript
- Mécanismes et patterns génériques, pas nécessairement liés à un langage
  particulier.
- Software engineering
- Point de vue du programmeur
  - Travail sur le code source, l'éditeur de programmes, les outils du programmeur

** Survol

* Contexte
Une chronologie de la modularité pour construire et faire évoluer des logiciels,
centrée sur les langages de programmations.

** UNSTABLE De l'exécution du programme au code source
Pour modifier l'exécution d'un programme, il est bon de savoir /comment/ le
programme est exécuté par la machine.  Du point de vue de la machine, un
programme est une simple liste d'instructions.  Une séquence d'additions, de
soustractions, d'écritures et lectures mémoire, et de sauts conditionnels.  Ces
instructions sont présentées à la machine sous la seule forme que son processeur
est capable de manipuler: le binaire, une suite de zéros et de uns.

#+BEGIN_aside
Un programme (extrait; point de vue d'une machine x86).  Un /bit/ correspond à
une seule valeur binaire: 0 ou 1.
#+END_aside

#+BEGIN_EXAMPLE
...00101011011100101011010101110101011111010100011101010010101011...
#+END_EXAMPLE

Le programme est alors exécuté instruction par instruction.  La machine lit une
instruction, puis effectue l'opération correspondante ; elle charge
l'instruction suivante, la lit, effectue l'opération, charge, lit, effectue,
etc.  Cette monomanie contribue à l'utilité que nous trouvons à la machine, car
son processeur agit certes simplement, mais il agit /vite/.  N'importe quel
processeur actuel est capable d'effectuer plusieurs milliards d'opérations par
seconde.  La machine ne différencie donc pas un programme d'un autre; tous se
mêlent en une immense suite binaire.

Le programmeur en revanche cherche à structurer cette suite de nombres.  Une
suite infinie de zéros et de uns est difficile à appréhender pour un cerveau
humain; il lui faut des repères, découper le programme en unités plus
digestibles par nos facultés organiques.  Un programmeur manipule donc rarement
des bits afin de déclarer ses intentions à la machine; il utilise plutôt un
langage de programmation.

#+CAPTION: Un programme (point de vue d'un programmeur connaissant le langage
#+CAPTION: JavaScript, vers 2015).
#+NAME: fib
#+BEGIN_SRC js
function fibonacci(n) {
  return n < 2 ? 1 : fibonacci(n-1) + fibonacci(n-2)
}

print(fibonacci(10))
#+END_SRC

#+ATTR_HTML: :style margin-top:-1rem
#+BEGIN_aside
Par opposition aux langages /ésotériques/ qui sont conçus davantage pour
démontrer la créativité de leur auteur que pour simplifier la conception de
programmes.  [[cite:Esolang][Esolang]] recense des spécimens tels que Brainfuck, Piet ou
Whitespace, qui ne manquent pas de nous rappeler qu'être Turing-complet n'est
pas un critère suffisant pour être un langage /utile/.
#+END_aside

Dans les langages de programmation /exotériques/, un programme est constitué
principalement de lettres plutôt que de nombres.  Les lettres, arrangées en
permutations judicieuses, nous servent à /nommer/ les objets de la computation
tels que les variables et constantes, les fonctions, les classes et autres
structures.  Les mots sont plus facilement prononçables, davantage mnémoniques,
et peuvent surtout communiquer l'intention du programmeur par homonymie.  On
peut ainsi, simplement en suivant les mots, supposer que le code [[fib]] calcule et
affiche le onzième nombre de la suite de Fibonacci, sans être vraiment certain
de la sémantique de JavaScript.  Les mots /évoquent/ immédiatement du sens alors
que les nombres doivent être décodés.

#+ATTR_HTML: :style margin-top:-5rem
#+ATTR_LATEX: :options [-5em]
#+BEGIN_aside
Les mots peuvent également évoquer un contresens; une définition maladroite–ou
intentionnelle–peut induire en erreur le lecteur.  Un ami m'a raconté avoir
passé tout un après-midi à pister une erreur dans du code écrit en C, pour
finalement découvrir dans un fichier inclus la ligne: ~#define true false~.
#+END_aside

Mais, si le programme ainsi décrit est plus confortable pour le programmeur, il
est incompréhensible par la machine en tant que tel.  Il faut au préalable
/traduire/ ce programme en code machine avant de pouvoir l'exécuter.  L'analogie
avec les langues naturelles est pertinente; si je souhaite communiquer le
contenu de ce manuscrit de thèse à des non-francophones, soit je rédige une
nouvelle version dans une langue qu'ils parlent (écrire le programme en langage
machine), soit j'emploie les services d'un traducteur.  Heureusement, la
traduction d'un programme est une tâche moins hasardeuse que la traduction d'un
document en langue naturelle.  Les langages de programmation sont définis de
façon à éliminer toute ambiguïté d'interprétation, ce qui permet d'effectuer la
traduction en langage machine automatiquement.

#+ATTR_HTML: :style margin-top:-7rem
#+ATTR_LATEX: :options [-7em]
#+BEGIN_aside
Une troisième option serait que les non-francophones apprennent le français pour
lire ce manuscrit.  En suivant l'analogie, cela correspond à une machine qui
exécuterait directement un programme JavaScript.  Les deux situations sont
également improbables.
#+END_aside

C'est le /compilateur/ qui traduit des programmes d'un langage de programmation
vers le langage machine.  Le compilateur lit le texte brut décrivant le
programme, vérifie que ce texte est conforme aux règles syntaxiques du langage
qu'il traduit, puis applique les règles de traduction en langage machine.  Un
compilateur permet donc au programmeur d'obtenir un programme exécutable par la
machine à partir de code écrit dans le langage de son choix.

# #+CAPTION: Le compilateur lit le programme, en extrait la structure, et produit
# #+CAPTION: du code machine.
# #+BEGIN_SRC dot :file img/compile-pipeline.png
# digraph {
#   rankdir=LR;
#   node [shape=record];
#   source [label = "print(1 + 1)"];
#   lex [label = "_print_ ( _1_ _+_ _1_ )"];
#   ast [label = "body { call { print, add { 1, 1 }}}"];
#   asm [label = "lda 1 - add 1 - psh - call $a0 - ... "];
#   bin [label = "0101011101..."];
#   source -> lex -> ast -> asm -> bin;
# }
# #+END_SRC

Notons que la compilation n'attache aucun sens particulier aux noms choisis par
le programmeur.  On pourrait écrire le programme de [[fib]], de manière totalement
équivalente:

#+BEGIN_SRC js
function xx(x){return x<2?1:xx(x-1)+xx(x-2)}print(xx(10))
#+END_SRC

#+ATTR_HTML: :style margin-top:-5rem
#+BEGIN_aside
À ce titre, les archives du concours international d'obfuscation de code C [[cite:IOCCC][IOCCC]]
recèlent d'exemples à suivre pour mystifier tout collaborateur.
#+END_aside

# Structures do not count (modules, classes, files).  Structure is for humans.

Le programme, n'a donc pas comme seule vocation à ordonner la machine, il doit
aussi être lisible par d'autres programmeurs.

Pour organiser un manuscrit, on recourt aux phrases, aux paragraphes.  Lorsqu'un
texte devient trop long, il devient plus pratique de le découper en unités
indépendantes.

# Other structures, classes, modules

# #+BEGIN_SRC dot :file img/bg-map.png
# digraph {
#   rankdir=LR;
#   node [shape=record];
#   source [label = "Code source"];
#   box [label = "Magie"];
#   effects [label = "Effets"];
#   source -> box -> effects;
# }
# #+END_SRC

# #+RESULTS:
# [[file:img/bg-map.png]]

# #+BEGIN_SRC dot :file img/bg-map2.png
# digraph {
#   rankdir=LR;
#   node [shape=record];
#   source [label = "Code source"];
#   box [label = "Interpréteur"];
#   effects [label = "Effets"];
#   source -> box -> effects;
# }
# #+END_SRC

# #+RESULTS:
# [[file:img/bg-map2.png]]

# #+BEGIN_SRC dot :file img/bg-map3.png
# digraph {
#   rankdir=LR;
#   node [shape=record];
#   source [label = "Code source"];
#   box [label = "Compilateur"];
#   bin [label = "Binaire"];
#   effects [label = "Effets"];
#   source -> box -> bin -> effects;
# }
# #+END_SRC

# #+RESULTS:
# [[file:img/bg-map3.png]]

** UNSTABLE Apprivoiser le flot de contrôle: la programmation structurée
#+BEGIN_epig
At the IFIP Congress in 1971 I had the pleasure of meeting Dr. Eiichi Goto of
Japan, who cheerfully complained that he was always being eliminated.
— Donald Knuth [[cite:Knu-74][Knu-74]]
#+END_epig

#+CAPTION: Eiichi Goto (gauche) et Furomu Komo devant le PC-1, vers 1958.
#+ATTR_HTML: :title すみません、高橋さん。
[[file:img/goto.jpg]]

La programmation structurée n'est pas un ensemble de techniques, ni une méthode
de programmation, mais plutôt un but à atteindre.  Dans sa célèbre lettre à
l'éditeur des communications de l'ACM, "Go To Statement Considered Harmful"
[[cite:Dij-68][Dij-68]], Dijkstra décrit ce but:

#+BEGIN_QUOTE
Our intellectual powers are rather geared to master static relations and
our powers to visualize processes evolving in time are relatively poorly
developed.  For that reason we should do (as wise programmers aware of our
limitations) our utmost to shorten the conceptual gap between the static program
and the dynamic process, to make the correspondence between the program (spread
out in text space) and the process (spread out in time) as trivial as possible.
#+END_QUOTE

#+ATTR_HTML: :style margin-top:-18rem
#+ATTR_LATEX: :options [-18em]
#+BEGIN_side-figure
[[file:img/dijkstra-process-program.svg]]

L'objet du programmeur est le processus effectué par la machine, et manipulé
indirectement via un programme.  Dijkstra tente de construire des programmes qui
reflètent le comportement du processus.
#+END_side-figure

Dijkstra commence par établir une distinction cruciale entre le /programme/ tapé
par le programmeur sous forme de texte, et le /processus/ engendré par ce
programme, et exécuté par la machine.  Le programmeur n'a pas de contrôle direct
sur le processus, il ne peut que modifier le programme.  Afin de mieux
comprendre le processus engendré par le programme, lorsque l'on écrit et relit
le code, il faut que le programme reflète au mieux ce processus.  Dijkstra
cherche donc à établir une correspondance claire entre les instructions
exécutées par la machine (la dimension temporelle) et les instructions dictées
par le programme (la dimension spatiale).  Et pour lui, ~goto~ est une
construction qui va à l'encontre de cette correspondance.

#+ATTR_HTML: :style margin-top:-8rem
#+BEGIN_side-figure
[[file:img/dijkstra-dimensions.svg]]

Le processus s'exécute dans le temps, et le programme décrit ce processus dans
l'espace.  Comme le programmeur manipule et lit le programme, celui-ci doit
refléter le plus directement possible le déroulement du processus.
#+END_side-figure

Pour Dijkstra, on comprend un programme en suivant les instructions une à une.
C'est facile à faire pour un programme qui contient juste une liste
d'affectations à des registres et de simples additions: on met son doigt sur la
première ligne, puis on suit les instructions, une à une, jusqu'à la dernière.
On peut comme ceci retrouver la valeur d'un registre particulier après la
dixième instruction.  Si on exécute le programme plusieurs fois, le registre à
cet endroit aura toujours la même valeur.  C'est un /invariant/ du
programme à cet endroit, qui peut être utile pour vérifier que le processus
correspond à une spécification donnée.  La correspondance entre le programme et
le processus est directe.

# See Hoare logic, and everything invariants are good for.  A practical example
# is to ensure that a function computing factorials is correct.  An inductive
# proof using the loop invariants can help prove correctness.

On peut continuer de localiser ces invariants dans des programmes plus
complexes.  Si l'on considère des conditions introduites par un ~if then else~
ou un ~cond~, suivre le flot d'exécution est encore simple: il suffit de prendre
la branche correspondante.  Pour les boucles (~while~, ~repeat A until B~), il
faudra prendre en compte la valeur actuelle de l'indice de boucle pour savoir
s'il faut répéter le corps de la boucle encore une fois, ou s'il faut sortir.
Pour les appels de procédures, on ne peut plus se contenter d'utiliser un seul
doigt pour suivre la ligne de code courante, il faut aussi tenir compte de la
pile d'appels en cours, pour savoir où continuer l'exécution lorsque la
procédure actuelle prend fin.

L'instruction courante, l'indice de boucle, et la pile d'appels forment ce que
Dijkstra appelle un système de coordonnées de l'exécution du processus.  On peut
localiser précisément un point d'exécution du processus en donnant une position
dans ce système de coordonnées.  Et à un point d'exécution du processus on peut
rattacher un invariant, ce qui permet de s'assurer de la correction du
programme.

Et c'est là que ~goto~ pose problème.  En utilisant un ~goto~, le processus peut
continuer l'exécution vers n'importe quelle ligne du programme: même au beau
milieu d'une procédure, ou d'une boucle.  Le système de coordonnée n'est plus
suffisant pour connaître avec certitude l'état du processus, les valeurs des
variables.  Il faudrait également savoir à tout moment le chemin exact emprunté
par le processus.  Mais ce n'est plus un système de coordonnées: on ne peut plus
raisonner /localement/ dans une boucle ou une procédure, il faut considérer
l'intégralité du programme.

Pour Dijkstra, ~goto~ est une construction qui permet trop facilement de
transformer le programme en véritable labyrinthe; d'obscurcir la correspondance
entre le programme et le processus.  C'est une construction qui va donc à
l'encontre du but qu'il a fixé en commençant la lettre.

Mais la programmation structurée ne se réduit pas à l'abolition des ~goto~ dans
un programme.  Au contraire, se concentrer sur le ~goto~ serait passer
complètement à côté du message initial; Knuth le remarque très justement [[cite:Knu-74][Knu-74]]:

#+BEGIN_QUOTE
There has been far too much emphasis on GO TO elimination instead of the really
important issues; people have a natural tendency to set up an easily understood
quantitative goal like the abolition of jumps, instead of working directly for a
qualitative goal like good program structure.
#+END_QUOTE

La programmation structurée tente simplement de résoudre les problèmes posés par
la complexité croissante des programmes.  Les programmes pour les premiers
ordinateurs, écrits en assembleur ou langage machine, avaient comme principal
objectif d'utiliser au mieux les capacités de l'ordinateur.  L'assembleur est un
langage flexible, qui offre notamment la possibilité de changer le programme
chargé en mémoire pendant l'exécution (/self-modifying code/).  Pour optimiser
l'utilisation de la mémoire, l'affectation des registres se fait à la main, en
prenant garde qu'aucun code n'écrase les registres d'un autre.  Plus les
machines deviennent rapides, et moins toutes ces techniques de programmation en
assembleur deviennent nécessaires.  Le frein vient surtout de notre capacité à
comprendre et gérer de larges programmes écrits dans des langages de plus
haut niveau.  Wirth [[cite:Wir-74a][Wir-74a]] relate ce changement de contraintes:

#+BEGIN_QUOTE
As the power of computers on the one side, and the complexity and size of the
programmer's task on the other continued to grow with a speed unmatched by any
other technological venture, it was gradually recognized that the true challenge
does not consist in pushing computers to their limits by saving bits and
microseconds, but in being capable of organizing large and complex programs, and
assuring that, they specify a process that for all admitted inputs produces the
desired results.  In short, it became clear that any amount of efficiency is
worthless if we cannot provide /reliability/.
#+END_QUOTE

La programmation structurée cherche donc à produire des programmes fiables avant
toute chose.  Wirth la décrit comme un mouvement, une attitude plutôt qu'une
liste de règles à suivre:

#+BEGIN_QUOTE
[Structured programming] is the expression of a conviction that the programmer's
knowledge must not consist of a bag of tricks and trade secrets, but of a
general intellectual ability to tackle problems systematically, and that
particular techniques should be replaced (or augmented) by a method.  At its
heart lies an /attitude/ rather than a recipe: the admission of the limitations
of our minds.  The recognition of these limitations can be used to our
advantage, if we carefully restrict ourselves to writing programs which we can
manage intellectually, where we fully understand the totality of their
implications.
#+END_QUOTE

Mais Wirth nous donne néanmoins des recettes.

# GOTO is about single entry, single exit

# that can be done by composing sequence, condition, and repetition

# this serves the overall view: hierarchical refinements, top-down approach








# For Dij, there is something else than the program.  But it's not the theory of
# Naur.  The dynamic process is what happens at runtime, while the theory is the
# knowledge the programmer has of that process.

Argues for a single entry point into procedures, and single exit point.  Not
jumping directly in the middle, or exiting prematurely.

Exemplified by ALGOL, and Pascal [[cite:Wir-74][Wir-74]] [[cite:Wir-74a][Wir-74a]].

On the legacy front, most programmers are cargo-culting the fear of GOTO (though
Knuth argues that it has its uses [[cite:Knu-74][Knu-74]]).  Few languages in use today propose
it.  However, the discipline of single-exit is more controversial, as most
modern languages offer constructs for early exits from procedures (return
statement) or from loops (break and continue statements, sometimes with
labels).

The fear of GOTO is an example of focusing on the wrong issue: structured
programming is a proposal for clearer programs.  Blindly removing all GOTOs and
labels from an unstructured program does not make it structured.  The focus is
on writing programs that clearly reflect their dynamic process.  As Parnas noted
[[cite:DBB+03][DBB+03]], modularity is solved by improving the design and documentation
processes, not by adding a "module" statement to the language.  The same
situation arises here.

Knuth finit sa défense du GOTO par imaginer les systèmes de manipulation de
programmes du futur:

#+BEGIN_QUOTE
Program manipulation systems appear to be a promising future tool which will
help programmers to improve their programs, and to enjoy doing it.  Standard
operating procedure nowadays is usually to hand code critical portions of a
routine in assembly language.  Let us hope such assemblers will die out, and we
will see several levels of language instead: At the highest levels we will be
able to write abstract programs, while at the lowest levels we will be able to
control storage and register allocation, and to suppress subscript range
checking, etc.  With an integrated system it will be possible to do debugging
and analysis of the transformed program using a higher level language for
communication.  All levels will, of course, exhibit program structure
syntactically so that our eyes can grasp it.
#+END_QUOTE

L'idée est tentante, mais peut-être trop enthousiaste.  Knuth écrit en 1974, et
je peux constater que 40 ans plus tard la situation n'est pas celle prédite.
Bien qu'il existe de nombreux langages de programmation, et beaucoup qui sont
qualifiés de "haut-niveau", aucun ne permet de manipuler différents niveaux
d'abstraction comme le décrit Knuth.  Aucun ne réconcilie la perte de contrôle
et d'efficacité avec la montée en abstraction.

Mais peut-être que Knuth s'en est lui-même rendu compte.  Le langage du futur
était annoncé pour 1984, l'année où il publie son système de Literate
Programming qui permet de mêler une description haut-niveau du programme en
toutes lettres, et le code machine bas-niveau.

** STABLE Modéliser le monde: la programmation par objets
#+BEGIN_full-figure
#+CAPTION: L'École d'Athènes. Raphaël, 1509--1510.
[[file:img/scuola.jpg]]
#+END_full-figure

*** Classes et héritage: Smalltalk
:PROPERTIES:
:CUSTOM_ID: Smalltalk
:END:
#+BEGIN_epig
Classification is the objectification of {{{emph(ness)}}}ness. — [[cite:Ing-81][Ing-81]]
#+END_epig

C'est avec le langage Smalltalk qui débute la programmation par objets [[cite:Ing-78][Ing-78]]
[[cite:Ing-81][Ing-81]] [[cite:Kay-93][Kay-93]].

Alan Kay, alors employé au laboratoire de recherche de Xerox à Palo Alto (PARC),
souhaite développer l'ordinateur personnel pour tous.  Il est inspiré par le NLS
(Online System) de Douglas Engelbart [[cite:EE-68][EE-68]], un système qui permet d'organiser
des documents de travail, de créer de multiples vues de ces documents, d'établir
des relations (prémices de l'hypertexte), et de collaborer à distance, dans le
but "d'accroître l'intelligence humaine".  Kay voit dans l'ordinateur personnel
à la fois un support qui permettrait à tout un chacun de décupler ses facultés
de raisonnement et d'analyse, mais aussi un catalyseur de créativité.  Chacun
pourrait créer une simulation interactive pour mieux comprendre un phénomène, ou
créer un outil pour résoudre un problème de la vie courante, mais aussi composer
des mélodies inédites, ou faire des esquisses improbables.  L'ordinateur
personnel serait à l'intellect ce que l'exosquelette est au corps humain.

En 1971, Kay crée le /Learning Research Group/ à PARC dans le but de construire
cet ordinateur.  Inspiré par les travaux de Piaget et Bruner sur le
développement intellectuel chez les enfants, il souhaite construire un
ordinateur intuitif, qui pourra servir aux enfants "de tout âge".  Programmer ce
système serait simple comme bonjour, et c'est pourquoi il nomme son langage de
programmation "Smalltalk".

Smalltalk est inspiré par Sketchpad [[cite:Sut-63][Sut-63]], SIMULA [[cite:DN-66][DN-66]], LISP [[cite:McC-60][McC-60]].  De
Sketchpad, il retient l'utilisation des contraintes et l'interactivité.  De
SIMULA, il tire la différence entre les classes de procédures et leurs
instances, et le mécanisme d'héritage.  Et de LISP, il emprunte le style de
construction récursif, l'absence de gestion manuelle de la mémoire, et
l'homoiconicité.

Pour Kay, les /objets/ de Smalltalk sont comme des cellules: des constituants
quasi-autonomes, capables de se répliquer:

#+BEGIN_QUOTE
For the first time I thought of the whole as the entire computer and wondered
why anyone would want to divide it up into weaker things called data structures
and procedures.  Why not divide it up into little computers, as time-sharing was
starting to?  But not in dozens.  Why not thousands of them, each simulating a
useful structure?
#+END_QUOTE

Ce qui se traduit, en Smalltalk, par le principe du "tout objet".  Un nombre
entier est un objet.  Une chaîne de caractère est un objet.  Une liste chaînée
est un objet.  Un fichier est un objet.  Une image est un objet.

Les objets ont leur propre mémoire: ils sont les seuls maîtres des données
qu'ils manipulent.  L'objet liste chaînée contient les références vers ses
objets cellules; l'objet fichier contient la référence vers le descripteur
fournit par le système d'exploitation.  On dit que les objets encapsulent ces
données: c'est une forme de type de donnée abstrait.

Pour communiquer entre eux, les objets s'envoient des /messages/.  Il n'y a pas
de procédures, seulement des /méthodes/ qui répondent à certains messages.  Pour
additionner deux entiers par exemple, on écrit ~2 + 3~, qu'il faut lire comme
"Passer le message ~+3~ à l'objet ~2~".  L'objet ~2~ contient une méthode qui
interprète le message ~+3~ comme une addition avec l'objet ~3~, ce qui retourne
un nouvel objet: ~5~.  On ne pense plus alors en termes d'opérateur (~+~) et
d'opérandes (~2~ et ~3~), mais d'un objet (~2~) qui reçoit un message (~+3~).
C'est le changement de paradigme majeur de la programmation par objets:

#+BEGIN_QUOTE
On the one hand, the act of assembling expressions into statements and then into
methods is not very different from conventional programming.  On the other hand,
the experience is totally different, for the objects which populate and traverse
the code are active entities, and writing expressions feels like organizing
trained animals rather than pushing boxes around.
#+END_QUOTE

Le passage de messages est une métaphore uniforme.  Pour additionner deux points
~p1~ et ~p2~, on écrira ~p1 + p2~.  Pour concaténer deux chaînes de caractères
~s1~ et ~s2~, on écrit de même ~s1 + s2~.  Le sens du message ~+~ dépend de
l'objet qui le reçoit et l'interprète.  C'est le mécanisme de /liaison
dynamique/.

#+CAPTION: L'addition de deux points fait appel à l'addition des coordonnées.
#+BEGIN_SRC smalltalk
+ t1
  [⇑ Point new x: x + t1 as PtX y: y + t1 as PtY]
#+END_SRC

Le programmeur ne crée pas des objets directement.  Il crée des /classes/.  Une
classe est un moule à partir duquel on peut créer des objets.  Une classe
contient les données privées manipulées par l'objet (ses /attributs/), et ses
méthodes, qui sont partagées entre toutes les /instances/ de cette classe.

Si un objet appartient à une classe, il observe le comportement décrit par cette
classe.  Mais les classes sont elles-mêmes des objets, et peuvent /hériter/
d'autres classes.  Lorsqu'une classe ~B~ hérite d'une classes ~A~, tous les
messages non capturés par la classes ~B~ sont traités par la classe ~A~.  La
classe ~Integer~ va par exemple hériter de la classe ~Number~, qui définit les
opérations de comparaison ~<~, ~>~ et ~=~.  Ces comparaisons sont définies à
partir de la soustraction, qui n'est pas un message interprété par ~Number~
directement, seulement par les classes qui en hérite: ~Integer~, mais aussi
~Float~ ou ~Natural~.  En conséquence, la classe ~Integer~ hérite du
comportement de la classe ~Number~, ce qui permet de réutiliser les
comportements, et de factoriser le code.

#+ATTR_HTML: :style margin-top:-16rem
#+ATTR_LATEX: :options [-26em]
#+BEGIN_side-figure
[[file:img/smalltalk0.svg]]

L'héritage permet aux classes ~Integer~ et ~Float~ de réutiliser les méthodes de
~Number~.
#+END_side-figure

#+CAPTION: Les comparaisons sur les nombres sont définiies dans la classe
#+CAPTION: ~Number~ en termes de la soustraction.  Code source tiré de
#+CAPTION: [[cite:Smalltalk78][Smalltalk78]].
#+BEGIN_SRC smalltalk
< t1
  [⇑ self - t1 < 0]
> t1
  [⇑ self - t1 > 0]
= t1
  [⇑ self - t1 = 0]
#+END_SRC

#+BEGIN_aside
Notons que, pour des raisons de performance, les nombres entiers ne sont pas
vraiment représentés par des objets dans la machine virtuelle.  Ingalls le
remarque avec humour: "The implementor must cheat, but not get caught".
#+END_aside

Pour Ingalls, la modularité dans un système complexe c'est de ne jamais dépendre
des détails internes d'un autre objet.  Les classes et le passage de message
contribuent à forcer cette barrière:

#+BEGIN_QUOTE
Objects are created and manipulated by sending messages.  The communication
metaphor supports the principle of modularity, since any attempt to examine or
alter the state of an object is sent as a message to that object, and the sender
need never know about internal representation.

[...]

The class is the natural unit of modularity, as it describes all the external
messages understood by its instances, as well as all the internal details about
methods for computing responses to messages and representation of data in the
instances.
#+END_QUOTE

Que le langage soit centré sur les objets a un avantage pour l'extensibilité.
Ingalls donne l'exemple d'ajouter un nouvel objet au système, et de faire les
modifications nécessaires pour représenter cet objet sur la sortie standard.
Dans un système qui contiendrait une fonction ~print~ centrale, c'est cette
fonction qu'il faudrait modifier.  Elle pourrait être large et compliquée, et il
faudrait faire attention à ne pas affecter l'affichage des autres objets.  En
Smalltalk, tous les objets sont affichables s'ils répondent au message ~printon:
s~.  Du coup, il suffit d'ajouter cette méthode dans le nouvel objet, ce qui
garantit de ne pas affecter le reste du système.  La nouvelle fonctionnalité est
localisée dans l'objet.

Et puisque les objets ne dépendent pas de la représentation interne d'autres
objets, on peut les recompiler séparémment.  Modifier ou ajouter un champ à la
classe ~Rectangle~ n'impacte que le code de l'objet, jamais le code extérieur.
Les objets peuvent être compilés indépendamment.

L'héritage de classes permet la réutilisation de code.  Mais une classe ne peut
avoir qu'un seul parent, ce qui force une hiérarchie des objets du système sous
forme d'arbre, qui peut s'avérer inflexible.  Dans l'exemple ci-contre, trois
classes ont des opérations en commun qui ne peuvent pas être partagées seulement
par l'héritage.  L'implémentation de Smalltalk pour l'ordinateur STAR de Xerox
permet à une classe de réutiliser les méthodes de plusieurs classes à la fois
par le mécanisme des /traits/ [[cite:CBL+82][CBL+82]].

#+ATTR_HTML: :style margin-top:-11rem
#+ATTR_LATEX: :options [-11em]
#+BEGIN_side-figure
[[file:img/smalltalk1.svg]]

Ces trois classes ont toutes deux à deux des méthodes en commun, mais
leur intersection est nulle.  Elles ne peuvent pas être factorisées
par l'héritage.
#+END_side-figure

Un trait décrit une fonctionnalité qui peut être réemployée par plusieurs
classes; il contient une ou plusieurs méthodes et leurs implémentations, ainsi
que les attributs utilisés par ces méthodes.  Une classe peut implémenter
plusieurs traits, ce qui transforme la hiérarchie en graphe dirigé acyclique.
Un trait peut en outre implémenter des traits à son tour, ce qui permet de
regrouper des traits souvent utilisés ensemble.

#+ATTR_HTML: :style margin-top:-9rem
#+BEGIN_side-figure
[[file:img/smalltalk2.svg]]

Une classe peut récupérer les méthodes de plusieurs traits à la fois, ce qui
permet de factoriser ces trois classes.
#+END_side-figure

Pour Kay, un des ingrédients clés de la flexibilité apportée par programmation
par objets est la /liaison tardive/ (/late binding/).  Le passage de messages en
est un exemple: l'opération effectuée dépend de l'objet qui reçoit le message,
et cet objet peut ne pas être déterminé avant l'exécution du programme, donc
l'opération devra être choisie dynamiquement.  De même, dans l'exemple de la
classe ~Number~, on fait référence au message ~-~, qui n'est pas défini sur
~Number~: c'est une forme de liaison tardive.  Autre exemple: la fonction ~eval~
en LISP permet d'exécuter du texte construit dynamiquement; les premières
versions de Smalltalk permettaient de traiter la syntaxe à l'exécution de la
même manière.

*** Prototypes et délégation: Self
On peut faire de la programmation par objets sans classes et sans héritage.  Les
langages /à prototypes/ poursuivent l'approche du tout objet et la généralise.

Taivalsaari [[cite:Tai-97][Tai-97]] voit, entre ces deux approches, une distinction
philosophique.  Les langages à classes (SIMULA, Smalltalk, Java) suivent l'école
d'Athènes: de Platon [[cite:Pla-98][Pla-98]] et d'Aristote [[cite:Ari-35][Ari-35]].  Platon distingue les
/formes/, qui sont les descriptions idéales des choses, des /instances/ de ces
formes, qui sont leur manifestation concrète et imparfaite.  Platon considère
que les formes sont plus réelles que leurs instances.  Aristote croit qu'il est
possible d'établir l'unique taxonomie correcte de toutes les choses naturelles.
Il classe une chose à partir de ses propriétés essentielles (le genre), et des
propriétés qui la distingue des autres choses du même genre (le différentiel).
On voit le parallèle avec l'héritage en programmation par objets: quand une
classe hérite, elle déclare son genre, et les méthodes additionnelles décrivent
son différentiel.

Aristote, comme Platon, voyaient la classification comme absolue: la taxonomie
est la seule vraie hiérarchie des choses.  À l'inverse, Wittgenstein la
considère subjective [[cite:Wit-53][Wit-53]].  Certains concepts sont difficiles à définir en
/intension/, par une liste de propriétés communes à toutes ses instances.
Wittgenstein propose la notion de /ressemblance familière/.  La chose n'est pas
déterminée par une définition absolue, mais par ses similarités avec des
prototypes représentatifs.

De cette distinction philosophique, Taivalsaari tire trois conclusions:

- Il n'y a pas de hiérarchie de classe optimale.  Le choix est subjectif et
  dépend non seulement des besoins de l'application, mais surtout de son
  concepteur.  Les langages à classes /nécessitent/ une hiérarchie, et en
  conséquence elle est souvent /inflexible/, difficile à modifier sans impacter
  une grande partie du système.

- Dans une hiérarchie de classes, ce sont les classes du milieu qui sont souvent
  les plus représentatives des objets de l'application.  Les classes les plus
  hautes sont trop abstraites, et les classes les plus basses trop spécifiques.

- Les prototypes correspondent davantage au processus de réflexion humain.  En
  abordant un domaine, on ne commence pas par établir des classes abstraites qui
  capturent l'essence des objets considérés.  On part d'exemples, de cas
  particuliers, que l'on relie entre eux par similarités.  Et ce n'est qu'après
  avoir accumulé de nombreux exemples que leurs similarités résonnent
  suffisamment pour qu'émergent des formes abstraites.  La généralisation est
  la dernière étape de la réflexion.


Ces conclusions suggèrent que les langages à prototypes sont plus flexibles pour
modéliser le monde.  Les classes sont trop rigides, trop abstraites; la
flexibilité découle des /prototypes/.  Ce point de vue fait écho à Lieberman
[[cite:Lie-86][Lie-86]], qui illustre la différence entre les deux mécanismes à l'aide d'un
éléphant nommé Clyde.

Si je rencontre Clyde l'éléphant, je vais construire un modèle mental de Clyde
qui contiendra tout ce que j'ai appris sur lui: sa taille, la couleur de sa
peau, son nombre de pattes, la forme de ses oreilles, etc.  Ce savoir, je peux
le réutiliser lorsque je rencontre un autre éléphant, Fred.  Mais comment ce
partage s'effectue exactement?  De deux façons: on peut utiliser les classes, ou
les prototypes.

En utilisant les classes, on construit le concept abstrait d'/éléphant/, qui
contient toutes les propriétés que l'on juge essentielles aux éléphants.  Clyde
et Fred sont des /instances/ de ce concept (cette classe); l'instantiation est
un premier mécanisme de partage du savoir.  Le second est l'héritage: si Fred et
Clyde ont des propriétés communes, non essentielles, comme des grandes oreilles,
alors je peux créer un sous-concept d'éléphant /africain/ qui hérite du
concept d'éléphant pour les représenter spécifiquement.

#+ATTR_HTML: :style margin-top:-14rem
#+ATTR_LATEX: :options [-14em]
#+BEGIN_side-figure
[[file:img/self0.svg]]

Dans des langages à classes, on réifie le concept d'éléphant (et même des
sous-concept) avant de pouvoir manipuler les objets qui représentent Clyde et
Fred.  Les deux mécanismes de partage sont l'héritage et l'instantiation.
#+END_side-figure

En utilisant les prototypes, je ne crée pas de concept.  Si Clyde est le
premier éléphant que je rencontre, il devient mon /prototype/ d'éléphant.  Si on
me demande la couleur d'un éléphant, je suppose qu'ils sont tous de la même
couleur que Clyde, et je réponds "gris".  Fred est aussi un éléphant, et il
ressemble beaucoup à Clyde.  Je lie les deux en disant que Clyde est le
prototype de Fred; il est /comme/ Clyde, mais avec un nom différent.  Si Fred a
d'autres particularités, s'il a la peau bleue, je l'inscris seulement dans sa
représentation.  Si on me demande la couleur de Fred, je n'ai besoin que
d'inspecter la représentation de Fred.  Mais si on me demande combien de pattes
a Fred, il en a autant que Clyde, son prototype.  L'information non spécifique
à Fred est sauvegardée par le prototype: c'est le mécanisme de /délégation/.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_side-figure
[[file:img/self1.svg]]

Dans les langages à prototype, on manipule directement les objets qui
représentent Clyde et Fred.  Le mécanisme essentiel de partage est la
délégation.
#+END_side-figure

Dony, Malenfant et Bardou [[cite:DMB-98][DMB-98]] recensent les langages à prototypes pour en
déterminer les caractéristiques.  Ils identifient notamment trois façons de
créer des objets.  On en a déjà vu deux: créer un objet /ex nihilo/ (Clyde), et
par /extension/ d'un autre (Fred à partir de Clyde).  La troisième façon est le
/clonage/: créer un nouvel objet qui possède des copies profondes des propriétés
de l'original.  Modifier le clone n'affecte pas l'original, alors que modifier
un objet créé par extension peut modifier son prototype.

Self [[cite:US-91][US-91]], est un langage à prototypes plutôt représentatif (un prototype de
cette classe de langages?).  Comme en Smalltalk, tout est objet.  Mais à
l'inverse de Smalltalk, il n'y a pas de classes.  Le programmeur ne crée pas une
hiérarchie de concepts, mais des familles d'objets.  Cette absence de structure
est un avantage dans un processus de développement interactif: on peut raffiner
les objets à l'exécution.  Mais c'est aussi un inconvénient en pratique:

#+BEGIN_QUOTE
Reducing the number of basic concepts in a language can make the language easier
to explain, understand, and use.  However, there is a tension between making the
language simpler and making the organization of a system manifest.  As the
variety of constructs decreases, so does the variety of linguistic clues to a
system's structure.

[...]

The absence of class-instance distinction may make it too hard to understand
which objects exist solely to provide shared information for other objects.
Perhaps SELF programmers will create entirely new organizational structures.  In
any case, SELF's flexibility poses a challenge to the programming environment;
it will have to include navigational and descriptive aids.
#+END_QUOTE

Les langages à prototypes sont particulièrement adaptés pour représenter des
objets qui ont un comportement unique.  Mais il arrive souvent que l'on doive
gérer de nombreux objets similaires.  Les concepteurs de Self décrivent une
façon de structurer ces objets à l'aide de traits [[cite:UCC+91][UCC+91]].  Comme en Smalltalk,
un trait est un objet qui regroupe les méthodes utilisées par plusieurs objets.
On crée ensuite un objet prototype qui étend ce trait, et fournit une
implémentation et des valeurs d'attributs par défaut.  Tous les objets de la
même famille étendent ce prototype.  Cette structure permet de séparer les
méthodes (le comportement) des attributs (l'état).

#+ATTR_HTML: :style margin-top:-13rem
#+BEGIN_side-figure
[[file:img/self2.svg]]
#+END_side-figure

Les traits de Self sont proches des traits de Smalltalk, a une distinction près.
En Smalltalk, le langage fait une distinction syntaxique entre classes, traits,
et instances, alors qu'en Self ce sont tous des objets qui ont juste des rôles
différents.

#+ATTR_HTML: :class todo
#+BEGIN_aside
Mode-switching better explained as mechanism later on.  Paragraph should
elaborate on the object/class distinctions.
#+END_aside

Les concepteurs de Self remarquent que le mécanisme de délégation est plus
général que l'héritage.  Le premier peut émuler le second, mais l'inverse n'est
pas possible.  Si le langage permet de modifier le lien de prototype après la
création d'un objet, on a une solution élégante au patron de conception /State/:
le /mode-switching/ (voir ??).

** Réifier le langage pour séparer les préoccupations
#+CAPTION: Le baron de Münchhausen évite la noyade par capillotraction.
#+CAPTION: Gustave Doré, 1862.
[[file:img/münchhausen.jpg]]

*** TRANSLATE La réflexion
En programmation par objets, on cherche une correspondance entre les acteurs du
domaine d'application et les objets manipulés par le programme.  Dans la
simulation d'un écosystème qui comprend de l'herbe, des moutons et des loups, on
trouve des objets qui représentent l'herbe, des objets moutons, et des objets
loups.  Un objet du programme est une /réification/ de l'objet de l'écosystème.

Mais un programme peut aussi réifier les acteurs de sa propre exécution, les
objets de la sémantique; on parle alors de /réflexion/.  La réflexion permet à
un programme d'altérer sa structure, ou de modifier la sémantique du langage,
pendant qu'il s'exécute.  Cette adaptation dynamique offre de nouvelles façons
de concevoir des programmes extensibles.

En Java, un programme peut obtenir les méthodes d'un objet via l'API de
réflexion:

#+BEGIN_SRC java :classname A :results verbatim :exports both :eval no-export -n -r
import java.lang.reflect.*;

class A {
  int n;

  void m1() {}
  int m2(int i) { return i * 2; }

  public static void main(String[] args) {
    Class a = A.class;

    System.out.println("Fields:");
    for (Field f: a.getDeclaredFields())
      System.out.println(f);

    System.out.println("\nMethods:");
    for (Method m: a.getDeclaredMethods())
      System.out.println(m);

    System.out.print("\nA.m2(3) = ");
    try {
      Method m2 = a.getDeclaredMethod("m2", int.class);
      Object anA = a.newInstance();                        (ref:synth)
      int result = (int) m2.invoke(anA, 3);
      System.out.println(result);
    } catch (Exception e) {}
  }
}
#+END_SRC

#+RESULTS:
: Fields:
: int A.n
:
: Methods:
: void A.m1()
: int A.m2(int)
: public static void A.main(java.lang.String[])
:
: A.m2(3) = 6

Ici on liste, à l'exécution, l'attribut ~n~ et les deux méthodes ~m1~ ~m2~ avec
leurs signatures complètes.  On voit que l'API de réflexion réifie les objets de
la programmation par objets: on a une classe ~Field~ pour les attributs, une
classe ~Method~, une classe ~Object~, et une classe ~Class~.

On peut non seulement inspecter ces objets, mais aussi en synthétiser des
instances, sans connaître leurs noms statiquement.  Ligne [[(synth)]] on crée une
instance de ~A~, ~anA~, et on invoque la méthode ~m2~ récupérée elle aussi par
réflexion.  Invoquer une méthode dynamiquement à partir d'une chaîne qui
contient son nom rend le programme plus flexible: ça permet par exemple
d'implémenter simplement l'appel de procédure distante (RPC).  On peut aussi
récupérer ou changer la valeur un attribut dynamiquement, ce qui est utile pour
écrire des objets qui font la passerelle avec des bases de données
relationnelles (ORM).

#+BEGIN_aside
On peut néanmoins synthétiser des classes en Java en utilisant un chargeur de
classes personnalisé, en utilisant des outils comme Javassist, ou en manipulant
la JVM directement [??].
#+END_aside

Mais l'API de réflexion de Java ne permet pas de synthétiser de nouveaux
éléments: on ne peut pas ajouter d'attributs ni de méthodes à une classe, ni
créer de classe dynamiquement.

Le protocole metaobjet de CommonLisp va plus loin [[cite:KRB-91][KRB-91]].  Le protocole
metaobjet régit les
façon dont le langage interprète le passage de message ou l'héritage [[cite:KRB-91][KRB-91]].

[exemple d'ajout d'une méthode au runtime]
[exemple de noSuchMethod]

La réflexion est un autre exemple de la liaison tardive mentionnée par Kay [[#Smalltalk][\sect]].

A interesting observation on binding is quoted from [[cite:MJD-96][MJD-96]]:
#+BEGIN_QUOTE
The general trend in the evolution of programming languages has been to postpone
formal binding times towards the running of programs, but to use more and more
sophisticated analysis and implementation techniques to bring actual times back
to the earlier stages.
#+END_QUOTE
Later binding = more runtime flexibility, but also less guarantees and less
performance.

Appliquée aux interpréteurs,

Un cas particulier est interpréteur meta-circulaire.








-----

[[cite:Tan-09][Tan-09]] gives a nice survey of reflection and its uses.  Useful distinctions are
made between /introspection/, /introcession/, /structural reflection/, and
/behavioral reflection/; also between a program (a textual description) and a
/computational system/ (a running process described by a program).

[[cite:DS-01][DS-01]] give a general method to reify selected parts of a meta-circular
interpreter.

[[cite:Ste-94a][Ste-94a]] studies object-oriented languages which support open implementation.
The open implementation of a language (the interpreter) is itself written in one
language called the /implementation language/, and its meta-level interface
allows the system to interpret a range of /engendered languages/.

[[cite:SW-96][SW-96]] describe three approaches to code non-functional requirements while
preserving the separation of concerns: systems-based, language-based, and
MOP-based.  They find that MOP-based solutions are more flexible, especially as
they can be applied to other domains without modifying the code.  However, they
consider non-functional requirements like persistence and atomicity.

Reflection for dynamic adaptation [[cite:DSC+99][DSC+99]].  Dynamic adaptation echoes the
motivation of open implementation: an application should adapt dynamically to
the need of the users, thereby enhancing performance.  This is mostly a concern
in systems software, operating systems and middlewares.  They use a memory
allocator example and compare using design patterns, DLLs and reflection.
Essentially, reflection is more flexible, but also less efficient.

[[cite:RC-02][RC-02]] illustrates how unanticipated dynamic adaptation can be achieved using
MOPs in Java.

Unifying AOP and OOP [[cite:RS-09a][RS-09a]].

[[cite:ADF-11][ADF-11]] proposes a proxy protocol for values.  A /virtual value/ is wrapped by a
proxy which has a handful of traps that are useful to override: when the value
is called as a function, when the value is used as a record, when the value is
used as an index in an array, when the value is used in a binary operation ...

They exhibit several scenarios where virtual values are useful: lazy evaluation,
revocable membranes, and tainting.  They modified Narcissus (again!) to add
their virtual values extension, but the implementation seems incomplete
regarding all operations available in JavaScript.

They motivate virtual values as a nice way to extend languages without having to
touch the interpreter.  Though they do not talk at all of the limitations of
this approach: can you write any extension that you would write by modifying the
interpreter with virtual values?  The only downsides they acknowledge are
performance hits and potential breakage of JS invariants (‘x*x' returning a
negative number, or ‘x === x' returning false).

It seems evident that virtual values are only hooks for values.  So you cannot
override any other part of the module which is not explicitly given by a trap.
Getting a trace of the interpreter execution is out.  Also, you need to specify
your analysis from the point of view of handler on values, not by altering the
interpreter semantics.

[[cite:KT-13][KT-13]] implements access control on JS objects through ES6 proxies.  Improves a
previous implementation which used code transformation; better performance, less
maintenance.

*** STABLE L'implémentation ouverte
La réflexion dans les langages à objets, et en particulier dans CLOS, permet à
des programmes de contrôler, d'inspecter, de modifier, et d'étendre la
sémantique du langage.

#+BEGIN_QUOTE
One meaning of the word reflect is to consider some subject matter.  Another is
to turn back something (e.g. light or sound).  Punning on these two meanings, we
get the notion of turning back one's consideration or considering one's own
activities as a subject matter.  Our ability as humans to reflect in this sense
has been credited, since Aristotle, with our success in adapting to new
situations and mastering our environment.  Naturally, it was widely conjectured
in the artificial intelligence community that by providing reflective
capabilities to computational systems, we would obtain systems with greater
plasticity and consequently, enhanced functionality. — [[cite:Rao-91][Rao-91]]
#+END_QUOTE

Mais la réflexion n'est pas restreinte aux implémentations de langages à objets,
ni aux langages de programmation.  Les membres de Xerox PARC se sont donc
intéressés à appliquer la réflexion sur à d'autres systèmes, ce qu'ils appellent
plus généralement "implémentation ouverte" [[cite:Rao-91][Rao-91]] [[cite:Kic-96][Kic-96]] [[cite:MLM+97][MLM+97]] [[cite:KLL+97][KLL+97]].

[[cite:Rao-91][Rao-91]] commence par remarquer que l'interprète d'un langage de programmation
engendre un processus à partir d'un programme, et que, parallèlement, tout
système définit une /interface/ qui peut être vue comme un langage; tout système
est donc un interprète.  Alors, la réflexion dans les langages de programmation
n'est qu'un cas particulier, et on peut l'appliquer à tout système qui expose
une interface.  Et comme un interprète réflexif permet au programme d'inspecter
et de modifier le processus engendré par l'interprète, il en va de même pour un
système plus général.

Mais à quoi ressemble un système réflexif, en général?  Rao définit un système
/à implémentation ouverte/ comme un système ayant deux interfaces: l'interface
de base, qui est l'interface classique associée au système, et l'interface de
meta-niveau, qui révèle en partie l'implémentation de l'interface de base.

#+ATTR_HTML: :style margin-top:-8rem
#+BEGIN_side-figure
[[file:img/sota1.svg]]

On va préférer ici le terme "interface d'implémentation" pour l'interface de
meta-niveau.
#+END_side-figure

Pour illustrer ces deux interfaces, il prend l'exemple d'un système de gestion
de fenêtre pour CLOS, appelé Silica.  Dans un système de gestion de fenêtre,
l'interface de base permet de construire des arbres de fenêtres, d'interroger et
de modifier ces arbres (pour accéder aux fenêtres, en ajouter, les retirer...),
et de lire et écrire dans les fenêtres.

Comment les fenêtres, et la structure d'arbre sont implémentés peut impacter
l'utilisateur du système de fenêtre.  En particulier, Rao prend l'exemple d'un
tableur qui créé une fenêtre pour chaque cellule de la feuille de calcul.  Ce
choix peut paraître curieux, mais est justifié par la réutilisation des
fonctionnalités du système de fenêtre (capture de la souris, peindre les
cellules en fonction de leur visibilité, ...) pour simplifier l'implémentation
du tableur.  Dans un système de fenêtre naïf, il faut allouer de la mémoire pour
chaque fenêtre, alors que dans le cas du tableur on pourrait partager bon nombre
de propriétés entre les fenêtres utilisées comme cellules.  De même, lorsque
qu'un clic de souris est détecté par le système de fenêtre, il faut parcourir
les arbres de fenêtres pour déterminer la fenêtre qui est actuellement au dessus
des autres, car les fenêtres peuvent se superposer.  Mais dans un tableur, les
cellules ne peuvent pas se superposer, donc on peut déterminer le clic de souris
directement grâce à ses coordonnées.  Ces optimisations ne sont possibles que si
le système de fenêtre laisse au programme client la possibilité de modifier son
implémentation.  En l'occurrence, Silica est un système à implémentation
ouverte, et Rao montre qu'il peut supporter ces optimisations en utilisant
l'interface d'implémentation.

Pour réaliser cette interface, Silica est construit en deux couches: une couche
qui implémente les fenêtres, pour fournir l'interface de base; et une couche qui
implémente les contrats, pour stipuler la politique d'utilisation des fenêtres
par le client de l'interface (comment redessiner une fenêtre, comment rapporter
les événements souris, comment gérer les sous-fenêtres, ...).

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_side-figure
[[file:img/sota2.svg]]

Une façon de fournir l'interface d'implémentation est de structurer le système
en deux parties, une de bas-niveau qui gère les détails d'implémentation et est
manipulable via l'interface d'implémentation, et une de haut-niveau qui utilise
la partie inférieure pour présenter l'interface de base.
#+END_side-figure

Le client de l'interface de base n'a pas besoin de modifier les contrats par
défaut.  En revanche, c'est par cette couche qu'il faut passer pour réaliser les
optimisations du tableur.

Mais il ne s'agit pas ici de simplement séparer le /mécanisme/ de la
/politique/; un système à implémentation ouverte ne propose pas nécessairement
toute liberté au client.  Tout comme un interprète peut n'exposer que certaines
parties du langage par une interface réflexive, ici aussi un système à
implémentation ouverte peut choisir l'étendue de son interface d'implémentation.

[[cite:KLL+97][KLL+97]] donne des recettes pour construire ces systèmes, en fonction des besoins
des programmes clients:

1. Le client n'a aucun contrôle: le système adapte son implémentation en
   observant le programme client.
2. Le client déclare son utilisation du système, et le système sélectionne une
   stratégie pré-établie.
3. Le client spécifie la stratégie parmi celles pré-établies par le système.
4. Le client fournit la stratégie au système.

Le degré 4 donne le plus de contrôle au client, mais on voit que d'autres styles
d'implémentation ouverte sont possibles.

-----

En somme, le concept d'implémentation ouverte répond au besoin de certains
programmes d'avoir à optimiser des choix d'implémentation du système qu'ils
utilisent.  Le système laisse au programme la possibilité de modifier ces choix,
statiquement ou même dynamiquement, via une interface d'implémentation.  Le
système peut être adapté à plus de cas qu'un système qui ne proposerait pas
d'interface d'implémentation; le système à implémentation ouverte promet
davantage de réutilisation de code.

On ne spécifie pas /comment/ cette interface d'implémentation est rendue
disponible par le système.  Dans les interprètes, la réflexion est souvent
utilisée, mais d'autres solutions sont envisageables suivant le système:
inversion de contrôle, patrons de conception Stratégie, n'importe quel moyen
d'exécuter le code du client pour modifier le processus considéré comme étant de
la responsabilité du module.

*** STABLE La programmation par aspects
Autre production du laboratoire PARC de Xerox, la programmation par aspects est
une technique qui pourrait être utilisée pour fournir l'interface
d'implémentation d'un système [[cite:KLM+97][KLM+97]] [[cite:MKL-97][MKL-97]].

La motivation derrière la programmation par aspects est très proche de la
motivation pour l'implémentation ouverte: les deux approches proposent de
nouvelles techniques pour organiser le code, pour qu'il soit à la fois simple à
écrire, facile à maintenir, et ne sacrifie pas l'efficacité.  Dans
l'implémentation ouverte l'idée est que le système expose une interface
secondaire qui donne un accès direct à son fonctionnement interne, ce qui permet
certaines optimisations.  Dans la programmation par aspects, l'idée est de
reconnaître que la décomposition d'un programme selon l'axe classique des
fonctions et des classes n'est pas suffisante pour maintenir ces trois qualités;
les aspects y remédient en introduisant une nouvelle dimension de composition.

**** La distinction entre les composants et les aspects
#+BEGIN_QUOTE
A design process and a programming language work well together when the
programming language provides abstraction and composition mechanisms that
cleanly support the kinds of units the design process breaks the system into.
— [[cite:KLM+97][KLM+97]]
#+END_QUOTE

Pour expliquer ce qu'est un aspect, [[cite:MKL-97][MKL-97]] part d'un exemple.  Le système qui
les intéresse prend une image en noir et blanc et lui applique une série de
filtres; c'est une partie d'un système de reconnaissance de caractères.  Ils
souhaitent que le système soit facile à développer et à maintenir, mais aussi
efficace, car les images traitées sont grandes, et les filtres rapidement
coûteux à calculer.

#+BEGIN_side-figure
[[file:img/aop1.svg]]
#+END_side-figure

Pour faciliter le développement, il suffit de créer des filtres composables.
D'abord des filtres basiques, comme ~Or~, ~And~, ~Not~, analogues des opérateurs
communs sur les bits, mais appliqués aux images, pixel par pixel:

#+BEGIN_SRC js
function Or(a, b) {
  var result = Image.new(a.width, a.height)

  for (var x=0; x < a.width; ++x) {
    for (var y=0; y < a.height; ++y) {
      result.pixel(x, y,
                   a.pixel(x, y) || b.pixel(x, y))
    }
  }

  return result
}
#+END_SRC

Puis, à partir de ces filtres élémentaires, on peut en définir des plus
complexes:

#+BEGIN_SRC js
var Remove = (a, b) => And(a, Not(b))
var TopEdge = (a) => Remove(a, Down(a))
var HorizontalEdge = (a) => Or(TopEdge(a),
                               BottomEdge(a))
...
#+END_SRC

On voit que les filtres complexes se définissent très facilement à partir des
filtres élémentaires.  Le code est clair, simple et direct, donc facile à écrire
et à maintenir.  Et pour ce faire, on n'a besoin que de composer des procédures.

En revanche, côté efficacité, c'est loin d'être optimal.  Tous les filtres
élémentaires, ~Or~, ~And~, ~Not~ créent une nouvelle image intermédiaire, et
bouclent sur les pixels de l'image.  Un filtre composite comme ~Remove~ appelle
deux filtres élémentaires, et donc va créer deux images intermédiaires, et
itérer deux fois sur tous les pixels de l'image: une fois dans ~And~, et une
fois dans ~Not~.  Le gaspillage est encore plus important pour les filtres plus
complexes, comme ~HorizontalEdge~.

Une version optimisée de ~Remove~ ne contiendrait qu'une seule boucle et ne
créerait qu'une seule image, le résultat.  Mais si on écrit cette version
optimisée directement, on perd alors la composition des filtres élémentaires.
Cherchant l'efficacité, les auteurs ont réécrit tous les filtres complexes
directement.  Le système efficace comporte trois optimisations majeures: la
fusion de boucle, l'utilisation d'un bassin d'objet pour éviter l'allocation
d'images intermédiaires, et la mémoisation des résultats.  Le système efficace
comporte 35213 lignes de code, alors que la version simple, mais non efficace,
en comporte 768; 5 fois moins de code à écrire, et à maintenir.  C'est d'autant
plus dommage que les versions optimisées suivent toujours le même motif:
fusionner les boucles, pour combiner les opérations à l'intérieur des boucles
plutôt qu'à l'extérieur.

Alors, peut-on réconcilier l'efficacité avec la simplicité de la composition des
filtres?  Peut-on arranger les procédures différemment, pour satisfaire les deux
buts à la fois?  Les auteurs pensent que non.  La composition hiérarchique des
filtres est incompatible avec la composition nécessaire pour optimiser les
boucles.  Composer les filtres hiérarchiquement c'est composer les
/fonctionnalités/ offertes par les filtres, de façon simple et homogène.  Mais
pour optimiser ces filtres complexes, il faut fusionner les boucles qui
appartiennent à des procédures différentes.  La composition hiérarchique est
réalisée par la composition des procédures du langage de programmation, mais la
seconde composition, qui dépasse l'unité de la procédure, n'est pas supportée
par le langage, et doit être réalisée manuellement.

#+ATTR_HTML: :style margin-top:-15rem
#+BEGIN_side-figure
[[file:img/aop2.svg]]

Dans ~Remove~, on ne peut pas fusionner les boucles de ~And~ et de ~Not~ sans
outrepasser les frontières imposées par les fonctions ({{{color(c0)}}}).  Il
faut un nouvel axe ({{{color(c1)}}}) de composition.
#+END_side-figure

C'est la distinction que les auteurs font entre /composants/ et /aspects/.  Si
une fonctionnalité du système peut être implémentée par une procédure (ou une
méthode, un objet, une API), alors c'est un composant; sinon, c'est un aspect.
Dans leur expérience, un aspect ne concerne pas la fonctionnalité principale du
système, mais plutôt des propriétés orthogonales ou complémentaires au système,
comme la performance, ou la gestion d'erreurs.  Le but de la programmation par
aspects est de pouvoir exprimer clairement ces deux catégories de
fonctionnalités:

#+BEGIN_QUOTE
The goal of AOP is to support the programmer in cleanly separating components
and aspects from each other, by providing mechanisms that make it possible to
abstract and compose them to produce the overall system.  This is in contrast to
[classic] programming, which supports programmers in separating only components
from each other by providing mechanisms that make it possible to abstract and
compose them to produce the overall system.
#+END_QUOTE

#+ATTR_HTML: :style margin-top:-11rem
#+BEGIN_aside
/AOP := Aspect-Oriented Programming/
#+END_aside

**** Exprimer les aspects par un langage dédié
Une implémentation par aspects reflète cette distinction.  Pour une application
écrite dans un langage classique, il y a trois éléments importants: le
programme, le langage dans lequel le programme est écrit, et le compilateur pour
ce langage.  Pour une application écrite en programmation par aspects, on
distingue: le programme des composants, le (ou les) programme des aspects, le
langage des composants, le langage des aspects, et un compilateur qui prend en
compte les deux langages en même temps, qu'on appelle le /tisseur d'aspects/.

Dans l'exemple des filtres d'images, le langage de composants est CommonLisp
avec des primitives de plus haut niveau.  Le filtre ~Or~ ne s'exprime plus avec
une boucle, mais avec une construction plus déclarative:

#+BEGIN_SRC lisp
(define-filter or! (a a)
  (pixelwise (a b) (aa bb) (or aa bb)))
#+END_SRC

Plutôt que de définir une fonction, on définit un /filtre/, qui itère sur chaque
pixel avec l'itérateur ~pixelwise~.

Les filtres ainsi définis sont passés au tisseur d'aspects, qui va interpréter
le code pour récolter les filtres et itérateurs utilisés: un filtre complexe,
comme ~Remove~, va faire appel aux filtres primitifs ~And~ et ~Not~; le tisseur
lui associe donc ces filtres.  Ce faisant, le tisseur construit la seconde
composition, la composition que le langage procédural ne pouvait pas exprimer.

Dans un second temps, le tisseur passe cette information aux programmes
d'aspects, qui vont pouvoir réaliser les trois optimisations; la fusion de
boucle, la mémoisation, et le bassin d'objets.  Chaque optimisation est un
programme d'aspect différent.  La fusion de boucle par exemple teste si les
arguments d'un filtre ont une itération compatible avec l'itération faite par le
filtre; si c'est le cas, on peut générer une seule boucle qui applique les
opérations des arguments et du filtre:

#+BEGIN_SRC lisp
(cond ((and (eq (loop-shape node) 'pixelwise)
            (eq (loop-shape node) 'pixelwise))
       (fuse loop input 'pixelwise
             ...)))
#+END_SRC

Enfin, dans ce système, le tisseur génère du code C pour chaque filtre.

Au final, l'implémentation par aspects a une performance comparable à
l'implémentation optimisée manuellement, en seulement 4500 lignes de code (dont
3500 pour le tisseur), soit seulement 13% du code de la version optimisée.  La
version par aspects permet de clairement séparer les composants des aspects, ce
qui atteint le but de simplicité d'écriture et de maintenance, mais sans
sacrifier l'efficacité.

Dans cet exemple, il est intéressant de noter la place importante du tisseur.
La version simple, mais inefficace, du programme ne comportait que 770 lignes.
En 1000 lignes, la version par aspects décrit les filtres, et les trois
programmes d'aspects qui correspondent aux optimisations.  Même ordre de
grandeur.  Mais pour que la version par aspects fonctionne, elle nécessite le
tisseur, et celui-ci fait 3 fois la taille du programme client.  Même si des
parties du tisseur peuvent être réutilisées pour d'autres programmes, le langage
reste spécifique à la création de filtres pour traiter des images en deux
dimensions.  On peut se poser la question: si l'on souhaite appliquer la
programmation par aspects, faut-il réimplémenter un tisseur pour chaque
application?  Y a-t-il d'autres façons de faire?

**** D'autres mécanismes pour la programmation par aspects
La caractéristique principale de la programmation par aspects est la distinction
entre composants et aspects.  Cette distinction, si elle apparaît au moment du
design du programme doit être maintenue dans son implémentation.  Tout
mécanisme, toute façon de réaliser cette distinction dans le programme est donc
compatible avec la programmation par aspects.  Un langage spécifique pour
l'application, qui permet de distinguer composants et aspects, est une façon de
faire.  La réflexion en est une autre.

Un système réflexif permet d'observer et de modifier le processus d'exécution du
programme, au delà de la séparation entre procédures.  Un système réflexif peut
observer la pile d'appels, ou tous les messages reçus par une classe, ou par
tous les objets du programme.  La réflexion permet de composer un programme
selon un axe complémentaire à celui des procédures; c'est donc un mécanisme de
programmation par aspects.  Les auteurs, certainement familiers avec le concept,
le note:

#+BEGIN_QUOTE
A reflective system provides the base language and (one or more) meta-languages
that provide control over the base language's semantics and implementation.  In
AOP terms, meta-languages are lower-level aspect languages whose join points are
the "hooks" that the reflective system provides.  AOP is a goal, for which
reflection is one powerful tool. — [[cite:KLM+97][KLM+97]]
#+END_QUOTE

Et il y a d'autres mécanismes connus.  Le ~try/catch~, ou le mécanisme
d'exceptions, transcende la frontière des procédures.  Les exceptions ne sont
qu'un cas particulier des continuations (~call/cc~ en Scheme), qui permettent de
manipuler le flot d'exécution du programme de façon arbitraire.  Les variables à
portée dynamique permettent de changer le comportement d'une procédure qu'on
n'appelle pas directement; c'est une forme de recomposition, mais moins
puissante que les précédentes.

Les auteurs de la programmation par aspects justifient la présence de ces
mécanismes par le besoin des programmeurs d'exprimer les aspects dans des
langages qui proposent de n'assembler que des composants.  Mais bien qu'ils
permettent d'exprimer des aspects, ces mécanismes sont considérés de bas-niveau,
au même titre que la réflexion.  Un langage dédié d'aspects, comme celui utilisé
pour optimiser les filtres d'image, est déclaratif, et correspond plus
directement à l'intention du programmeur.  Fournir les outils qui permettent au
programmeur d'exprimer cette intention est un des but de la programmation par
aspects:

#+BEGIN_QUOTE
When we say "separation of concerns" we mean the idea that it should be possible
to work with the design or implementation of a system in the natural units of
concern – concept, goal, team structure etc. – rather than in units imposed on
us by the tools we are using.  We would like the modularity of a system to
reflect the way "we want to think about it" rather than the way the language or
other tools force us to think about it.  In software, Parnas is generally
credited with this idea [[cite:Par-72][Par-72]] [[cite:Par-74][Par-74]]. — [[cite:KHH+01][KHH+01]]
#+END_QUOTE

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_aside
On retrouve ce souci dans l'implémentation ouverte; les auteurs préfèrent une
interface d'implémentation où le programmeur déclare l'optimisation souhaitée
plutôt qu'une qui fournit les mécanismes pour la réaliser directement.
#+END_aside

Néanmoins, créer un langage dédié pour chaque application est un coût qui peut
être trop élevé pour adopter la programmation par aspects.  Et tous les langages
ne supportent pas les mécanismes bas-niveau qui pourraient servir à exprimer des
aspects.  Ce sont les raisons qui vraisemblablement motivent le développement
d'AspectJ [[cite:KHH+01][KHH+01]].

**** AspectJ: jonctions, coupes, et méthodes d'aspect
#+BEGIN_QUOTE
AspectJ is intended to be a practical AOP language that provides, in a Java
compatible package, a solid and well-worked-out set of AOP features.
#+END_QUOTE

AspectJ est un langage d'aspect pour Java.  Dans un système par aspects, il y a
un langage d'aspects, ici AspectJ, et un langage de composants, ici ce sera
Java.  AspectJ est un langage d'aspect /général/, qui fournit des mécanismes de
recomposition applicables à tout programme Java.  Et pour ce faire, AspectJ
propose deux types de recomposition.

En premier, les déclarations inter-types permettent de définir, au sein d'un
même aspect, des attributs et des méthodes pour une ou plusieurs classes [[cite:Asp-03][Asp-03]].
Ces attributs augmentent les classes au moment de la compilation.  La classe
est, en Java, et comme dans d'autres langages à classe pour la programmation
objet, l'unité de composition des composants.  Elle décrit les attributs et les
méthodes partagées par toutes ses instances; c'est la structure statique du
programme.  Comme c'est une unité de composition, une classe n'a pas vocation à
pouvoir affecter les attributs et méthodes d'une autre classe.  Mais un aspect
est une unité de composition qui /traverse/ la composition des composants, et
c'est donc naturel de permettre, dans un aspect, d'ajouter attributs et méthodes
à plusieurs classes.  C'est une recomposition de la structure statique.

#+CAPTION: Une déclaration inter-type qui ajoute un attribut et une méthode sur
#+CAPTION: la classe ~Point~.
#+BEGIN_SRC java
public int Point.x = 0;
public int Point.getX() { return this.x; }
#+END_SRC

Le second type de recomposition d'AspectJ permet de modifier la structure
/dynamique/ du programme; de modifier le comportement du programme à
l'exécution.  C'est rendu possible par trois constructions:

- les jonctions (/joinpoints/) sont des points d'exécution du programme bien
  définis: l'appel d'une méthode, l'affectation d'un attribut, la création d'une
  instance de classe, etc.
- les coupes (/pointcuts/) permettent de cibler un ensemble de jonctions, et
  d'en extraire de l'information comme le nom de la méthode appelée, et ses
  arguments.
- les méthodes d'aspect (/advice/) contiennent le code à exécuter lorsque le
  programme atteint les jonctions décrites par la coupe de la méthode.  Une
  méthode d'aspect décrit également /comment/ le code doit s'insérer dans le
  processus d'exécution: avant, après, ou autour de la jonction.

Le programmeur ne manipule pas les jonctions directement, mais utilise les
coupes pour les cibler dans des méthodes d'aspect.  On pourra ainsi écrire:

#+BEGIN_SRC java
after(): calls(void Point.setX(int)) {
  System.out.println("setX was called")
}
#+END_SRC

pour déclencher l'affichage sur la sortie standard après (~after~) tout appel
(~calls~) de la méthode ~Point.setX(int)~ dans le programme.

Les coupes sont des prédicats, et peuvent être combinées avec les opérateurs
booléens ~&&~, ~||~, et ~!~:

#+BEGIN_SRC java
receptions(void Point.setX(int))
|| receptions(void Point.setY(int))

calls(void FigureElement.incrXY(int, int))
&& !instanceof(FigureElement)
#+END_SRC

Ce qui permet de capturer des ensembles de jonctions pertinents pour cibler une
préoccupation particulière.  AspectJ permet d'ailleurs de nommer les coupes pour
plus de clarté:

#+BEGIN_SRC java
pointcut moves():
  receptions(void FigureElement.incrXY(int, int))
  || receptions(void Point.setX(int))
  || receptions(void Point.setY(int));

after(): moves() {
  ...
}
#+END_SRC

Une coupe nommée peut être utilisée par une méthode d'aspect, mais aussi par
n'importe quelle autre coupe, ce qui encourage la composition, et réduit la
duplication de code.

Les jonctions exposées par AspectJ évoquent fortement les réifications d'un
protocole méta-objet: appel et exécution de méthode, création d'une instance.
Mais il y a d'autres coupes intéressantes qui sont applicables à n'importe quel
langage à procédures: ~withincode(method)~ permet de cibler n'importe quelle
jonction qui émane de la méthode donnée.  C'est particulièrement utile associé à
d'autres coupes:

#+BEGIN_SRC java
call(void m()) && withincode(void m())
#+END_SRC

Là on capture uniquement les appels récursifs directs de ~m~.  Inversement, si
l'on souhaite capturer les appels /non/ récursifs à ~m~, on peut utiliser
~cflow~:

#+BEGIN_SRC java
call(void m()) && !cflow(call(void m()))
#+END_SRC

#+ATTR_HTML: :style margin-top:-5rem
#+BEGIN_side-figure
[[file:img/aop3.svg]]

Les appels non récursifs de ~m~ sont capturés ({{{color(c3)}}}) par la coupe, et
les appels récursifs échouent le test de ~cflow~ ({{{color(c2)}}}).
#+END_side-figure

~cflow(coupe)~ capture toutes les jonctions qui sont dans le flot de contrôle de
la coupe passée en argument; c'est à dire, ici, toutes les jonctions qui
existent après qu'au moins un appel de ~m~ soit présent sur la pile d'appels.
Dans l'exemple, la coupe composée capture les appels de ~m~ lorsqu'aucun appel à
~m~ n'existe sur la pile, ce qui exclut les appels récursifs (directs et
indirects).

À travers les différents types de jonctions observées, le langage d'aspect
d'AspectJ permet de capturer de nombreux points d'exécution du programme, et
d'exécuter du code au delà des frontières délimitées par les classes et les
méthodes de Java.  On voit bien comment le langage permet de séparer les
composants des aspects, et ce pour n'importe quelle application.  Puisqu'ici,
contrairement au langage spécifique d'aspects du premier papier, le mécanisme
est général; il s'applique à n'importe quel programme Java.

Les auteurs donnent l'exemple d'un aspect qui enregistre toutes les erreurs
levées par toutes les méthodes publiques d'un package:

#+BEGIN_SRC java
aspect SimpleErrorLogging {
  Log log = new Log();

  pointcut publicEntries():
    receptions(public * com.xerox.printers.*.*(..));

  after() throwing(Error e): publicEntries() {
    log.write(e)
  }
}
#+END_SRC

Cet aspect est équivalent à rajouter enrober chaque appel dans un ~try/catch~ et
appeler ~log.write~ sur l'erreur levée.  Mais ici c'est fait en 8 lignes, pour
toutes les classes du package, même les classes futures.

Un désavantage du langage spécifique c'est qu'il faut réécrire le programme des
composants dans le nouveau langage à composants.  Ici, le langage d'aspects
manipule directement le langage à composant déjà existant: Java.  Un programmeur
peut donc directement appliquer la programmation par aspects à un programme Java
existant, simplement en ajoutant des programmes d'aspect, et en invoquant le
tisseur à la compilation.  Aucune modification du programme des composants n'est
nécessaire, mais pourtant ce programme va être modifié par des aspects.  Les
déclarations inter-types peuvent étendre des classes sans modifier le code de
celles-ci.  Et les méthodes d'aspect vont modifier le comportement à l'exécution
de plusieurs classes, ici encore sans en altérer le code.

La programmation par aspect nous donne donc deux façons de modifier un
programme: dans les filtres d'images, la modification est /explicite/, car il
faut réécrire les filtres pour expliciter quel type de boucle est utilisée; mais
dans AspectJ, le programme est modifié /implicitement/ par les aspects.

En revanche, dans les deux cas, les programmes des composants /ne dépendent pas
des programmes d'aspects/ pour fonctionner.  Dans l'exemple de l'aspect de
journalisation des erreurs, on pourrait compiler le programme sans cet aspect
(sans tisser l'aspect), et on obtiendrait un programme qui a les mêmes
fonctionnalités, mais sans la journalisation.  Aucune modification du code du
programme n'est nécessaire pour activer ou désactiver la journalisation; c'est
uniquement le choix des aspects à activer au moment du tissage qui détermine les
fonctionnalités du programme final.  De même, dans les filtres d'image, ne pas
inclure les programmes d'aspects n'impacte que l'efficacité du programme, mais
pas son résultat.  On dira alors que, dans ces deux cas, les aspects sont
/oubliables/ (/obliviousness property/).

#+ATTR_HTML: :style margin-top:-18rem
#+BEGIN_side-figure
[[file:img/aop4.svg]]

Le programme peut être compilé avec ou sans les aspects, et engendrer deux
exécutables différents.
#+END_side-figure

**** Le mécanisme essentiel pour la programmation par aspects
D'être oubliable est une propriété qui peut être considérée comme
caractéristique de la programmation par aspects [[cite:FF-04][FF-04]].  Filman et Friedman
cherchent à identifier les systèmes qui suivent la programmation par aspects.
Il semble clair que lorsqu'on utilise AspectJ, on fait de la programmation par
aspect.  Mais si on construit un système similaire à celui des filtres d'images,
avec un langage dédié pour les filtres, est-ce que c'est un système d'aspects?
À l'inverse, il ne s'agit pas de rajouter une construction à un langage qui
permet de créer des "aspects", qui regroupent des méthodes, pour en faire un
langage de programmation par aspects.

Dans la publication d'origine [[cite:KLM+97][KLM+97]], le seul critère de définition d'un système
par aspects est qu'il permet d'exprimer séparément les composants des aspects,
des unités issues de la phase de conception du système.  Mais Filman et Friedman
ne s'intéressent pas tant à la /méthodologie/ de la programmation par aspects,
qu'aux /mécanismes/ qui la supportent:

#+BEGIN_QUOTE
Here we address the structural essence of AOP, not its application—somewhat
similar to the difference between defining object-oriented programming systems
in terms of polymorphic methods and inheritance versus waxing euphoric about
objects as the appropriate way to model the world.

[...]

Understanding something involves both understanding how it works (mechanism) and
what it's good for (methodology).  In computer science, we're rarely shy about
grandiose methodological claims (see, for example, the literature of AI or the
Internet).  But mechanism is important – appreciating mechanisms leads to
improved mechanisms, recognition of commonalities and isomorphisms, and plain
old clarity about what's actually happening.

— [[cite:FF-04][FF-04]]
#+END_QUOTE

D'après eux, pour faire de la programmation par aspect il suffit de pouvoir
exprimer des déclarations de la forme:

#+BEGIN_QUOTE
Dans les programmes P, quand la condition C est vraie, réaliser l'action A.
#+END_QUOTE

Autrement dit, il suffit de disposer d'un ~COMEFROM~ [[cite:Cla-73][Cla-73]].  Le ~COMEFROM~ est
le dual du ~GOTO~: un ~GOTO 10~ transfère le contrôle à la ligne 10, alors qu'un
~COMEFROM 10~ /capture/ le contrôle après l'exécution de la ligne 10.

#+ATTR_HTML: :style margin-top:-3rem
#+BEGIN_side-figure
[[file:img/aop5.svg]]

~COMEFROM~ est le dual de ~GOTO~.
#+END_side-figure

#+BEGIN_EXAMPLE fortran
10 J=1
11 COME FROM 20
12 WRITE (6,40) J STOP
13 COME FROM 10
20 J=J+2
40 FORMAT (14)
#+END_EXAMPLE

Dans cet exemple, les lignes exécutées sont, dans l'ordre: 10, 13, 20, 11, 12
(~STOP~ termine le programme).  On voit tout de suite qu'un ~COMEFROM~ rend le
code encore plus difficile à suivre qu'un ~GOTO~.  C'est bien l'intention de son
auteur, qui voit le débat autour du ~GOTO~ avec humour:

#+BEGIN_QUOTE
Nearly six years after publication of Dijkstra's now-famous letter, the subject
of GOTO-less programming still stirs considerable controversy.

The author has developed a new language construct on which, he believes, both
the pro- and the anti-GOTO factions can agree.
#+END_QUOTE

Le ~COMEFROM~ est donc une plaisanterie élaborée, qui n'a a priori rien à voir
avec la programmation par aspects.  Mais on s'intéresse ici aux mécanismes, et
le mécanisme du ~COMEFROM~ — la possibilité de capturer le contrôle — est
l'essence des jonctions et des coupes proposées par AspectJ.

Bien sûr, le ~COMEFROM~ décrit par Clark ne permet de venir que d'une autre
ligne du programme, alors qu'AspectJ permet de capturer le contrôle lors
d'appels de méthodes, la lecture ou l'affectation d'une variable.  Mais en
réalité, le ~COMEFROM~ de Clark est adapté à la granularité offerte par le
langage: la ligne.  Dans AspectJ, l'unité de composition est la méthode, et la
classe.  Un ~COMEFROM~ pour Java pourrait ressembler à ce qu'AspectJ offre à
travers le langage des coupes de jonctions.

En ne prenant en compte que le mécanisme, on peut déterminer si d'autres
systèmes ou langages, qui ne sont pas présentés comme des systèmes d'aspects,
permettent tout de même de faire de la programmation par aspects.  En
particulier, les systèmes à événements peuvent être vus comme équivalents à un
~COMEFROM~; si les composants ne communiquent qu'à travers des événements, on
peut écrire des aspects qui capturent ces événements, sans impacter le reste du
système (les aspects sont oubliables): il s'agit simplement de souscrire à ces
événements.  Un système à événements fournit donc un mécanisme qui peut être
utilisé pour faire de la programmation par aspects.

**** Les limites de la programmation par aspects
Le but de la programmation par aspects est louable: distinguer les composants
des aspects dans le programme, c'est mettre de l'ordre, c'est séparer les choses
différentes, et regrouper les fonctionnalités similaires.  Le bénéfice, c'est
qu'un code source ordonné est, a priori, plus simple, et donc plus facile à
maintenir.  Mais il y a aussi des inconvénients à faire cette distinction
[[cite:Ste-06][Ste-06]].

Le fait que les aspects capturent l'exécution de façon implicite nuit à la
compréhension modulaire du programme.  On ne peut plus raisonner sur le
comportement d'un module seulement en regardant son code, il faut prendre en
compte tous les aspects qui peuvent intervenir pendant l'exécution du code du
module.  La compréhension du programme devient nécessairement globale: il faut
connaître tous les aspects qui peuvent s'y appliquer, et en prenant en compte
les aspects qui pourraient être activés dynamiquement.

Une façon de mitiger ce problème est de faire apparaître, dans l'environnement
de développement du programmeur, les endroits du code où des aspects pourraient
s'exécuter; l'article qui introduit AspectJ montre une telle fonctionnalité dans
Emacs.  Les aspects deviennent explicites, mais l'intégrité du module demeure
compromise.  Le système nécessite toujours une analyse globale pour être
compris, et les aspects traversent encore la barrière du module, ce qui empêche
de développer le module indépendamment des aspects.

Les aspects capturent les points d'exécution du module, ce qui revient à dire
que tout module expose une interface /implicite/ qui est utilisée par les
aspects.  Que cette interface soit implicite rend les aspects fragiles: tout
changement du code peut rendre des aspects inopérables.  Mais c'est aussi le
fait qu'elle soit implicite qui permet aux aspects d'être oubliables, et donc de
pouvoir s'appliquer à plusieurs points d'exécution sans avoir à modifier le code
des composants.  L'interface implicite est donc un choix de conception du
système, à adopter si le besoin de modifier plusieurs modules en un seul lieu
(dans un aspect) surpasse le besoin de comprendre et développer les modules
indépendamment.

-----

La contribution principale de la programmation par aspects est de distinguer
deux types de programmes qui cohabitent dans un même système: les composants, en
charge de la fonctionnalité du système, et les aspects, qui décrivent des
préoccupations orthogonales à la fonctionnalité principale.  La programmation
par aspects est donc avant tout une méthodologie.

Le langage des jonctions et des coupes, popularisé par AspectJ, est le mécanisme
principalement associé à la programmation par aspects.  Mais il y d'autres
façons de réaliser la séparation des composants et des aspects: on peut utiliser
la programmation événementielle, les ~try/catch~, les variables à portée
dynamique, ou créer un langage dédié pour les composants et un pour les aspects.
Dans le monde de la programmation fonctionnelle, les monades sont aussi un moyen
de séparer les préoccupations orthogonales [[cite:Meu-97][Meu-97]] [[cite:HO-07][HO-07]].

Il ne faut donc pas mélanger la méthodologie—le but philosophique de la
programmation par aspects—et les (multiples) mécanismes qui permettent
d'atteindre ce but.  En particulier, on peut faire de la programmation par
aspects sans AspectJ, sans jonctions, sans coupes, et sans méthodes d'aspects.

Enfin, la programmation par aspects n'est pas une balle en argent.  Les
jonctions et les coupes sont oubliables par les composants, mais sacrifient le
raisonnement modulaire.  Les autres mécanismes qui permettent de séparer les
composants des aspects ont également cette caractéristique: recomposer le flot
d'exécution dynamiquement rend le programme moins prévisible, et moins
compréhensible.  Tous ces mécanismes présentent donc des avantages et des
inconvénients, et le choix du bon compromis est dans les mains du concepteur du
système.

** STABLE Tisser les facettes d'un artefact d'innombrable dimensions
#+BEGIN_epig
WEB may be only for the subset of computer scientists who like to write and to
explain what they are doing. — [[cite:Knu-84][Knu-84]]
#+END_epig

#+CAPTION: Tapis turc "Holbein", fin du XVe siècle.
[[file:img/rug.jpg]]

La programmation par aspects fait apparaître la distinction entre deux facettes
de la fonctionnalité d'un programme: les composants, essentiels, et les aspects,
oubliables.  Puisque ces unités existent dans la phase de conception, la
programmation par aspects permet de les exprimer directement dans le code.  Le
tisseur d'aspects regroupe ces unités pour produire un programme exécutable.

Les méthodes de cette section réalisent également une distinction de même ordre:
le code source n'est plus organisé seulement en fonctions, classes, modules et
fichiers, mais selon de multiples axes transversaux.  Toutes ces méthodes ont en
commun de considérer le programme comme un objet aux nombreuses dimensions, qui
ne sont pas réconciliables sur un seul axe de décomposition.  Chaque dimension
du programme est spécifiée séparément, et l'exécutable final est tissé à partir
des ces fils différents.

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_aside
Cette définition convient tout aussi bien à la programmation par aspects, qui,
si elle ne faisait pas partie de la section précédente, trouverait sa place dans
celle-ci.  Les thèmes de ce chapitre sont un autre exemple d'objet aux
nombreuses dimensions, irréconciliables sous une seule décomposition.
#+END_aside

*** La programmation littéraire
La programmation structurée recommandait une discipline de construction du
programme du haut vers le bas, par raffinements successifs.  On commence par une
description haut-niveau, qu'on enrichit petit à petit jusqu'à avoir spécifié le
programme dans le moindre détail.

Knuth [[cite:Knu-84][Knu-84]] remarque que sa façon de construire les programmes va souvent
l'encontre de ce principe:

#+BEGIN_QUOTE
I knew that I often created major parts of programs in a "bottom-up" fashion,
starting with the definitions of basic procedures and data structures and
gradually building more and more powerful subroutines.  I had the feeling that
top-down and bottom-up were opposing methodologies: one more suitable for
program exposition and the other more suitable for program creation.
#+END_QUOTE

Mais les deux approches ne sont pas nécessairement antagonistes si l'on prend du
recul.  Ces approches supposent une structure hiérarchique, que le programme est
un arbre.  Knuth considère qu'on peut voir un programme aussi bien comme un
réseau, comme une /toile d'araignée/.  Ce qui importe, c'est d'exprimer les
relations qui forment ce réseau:

#+BEGIN_QUOTE
A complex piece of software consists of simple parts and simple relations
between those parts; the programmer's task is to state those parts and those
relationships, in whatever order is best for human comprehension – not in some
rigidly determined order like top-down or bottom-up.
#+END_QUOTE

#+ATTR_HTML: :style margin-top:-7rem
#+BEGIN_side-figure
[[file:img/knuth0.svg]]

Un programme ressemble davantage à un réseau qu'à un arbre.
#+END_side-figure

WEB est le système de programmation littéraire développé par Knuth pour
permettre de tisser ce réseau.  En WEB, un programmeur écrit un document qui
contient du Pascal et du TeX mêlés.  Il ne s'agit pas simplement d'écrire des
commentaires autour du code; plutôt l'inverse.  Knuth souhaite décrire le
programme /avant tout/ pour un humain, et en second lieu pour la machine:

#+BEGIN_QUOTE
Instead of imagining that our main task is to instruct a /computer/ what to do,
let us concentrate rather on explaining to /human beings/ what we want to do.
#+END_QUOTE

La machine n'a que faire des considérations de style, ou de l'ordre de
présentation du programme; le code source suffit.  Pour l'humain en revanche,
les explications sont très utiles pour comprendre le fonctionnement du programme
et les intentions de son auteur.

Knuth écrit un programme WEB comme il écrit un article: il explique les grandes
lignes, puis fournit les détails dans un second temps.  L'exemple qu'il donne
est un programme qui liste les 1000 premiers nombres premiers:

#+BEGIN_EXAMPLE -n -r
@* Printing primes: An example of \WEB.
The following program is essentially the same as
Edsger Dijkstra's "first example of step-wise
program composition." ...

@<Program to print...@>=
program print_primets(output);
const @!m = 1000;
@<Other constants of the program@>@;               (ref:lit0)
var @<Variables of the program@>@;
begin @<Print the first |m| prime numbers@>;
end.
#+END_EXAMPLE

La première partie est écrite en TeX; le ~@*~ dénote une section.  Toutes les
commande usuelles de TeX sont disponibles, ce qui permet en particulier d'écrire
des mathématiques sans être limité par la table des caractères ASCII.

La seconde partie est le programme Pascal.  Ces 6 lignes couvrent l'intégralité
du programme, mais le contenu principal est décrit dans d'autres sections.  Par
exemple, la ligne [[(lit0)]] fait référence à une section qui contient toutes
les constantes déclarées par le programme.  La ligne [[(lit0)]] donne au lecteur
du programme une idée du code qui va être exécuté ici (des affectations de
constantes), mais le programmeur n'a pas à donner ces détails maintenant.  Tout
ce que le lecteur voit pour l'instant, c'est la structure du programme.

Plus loin dans le document, les constantes sont définies:

#+BEGIN_SRC pascal
@<Other constants of the program@>=
rr = 50;
cc = 4;
ww = 10;
#+END_SRC

C'est le code qui va être inséré à la ligne [[(lit0)]] de l'exemple précédent.  Les
constantes sont décrites, mais dans un second temps.  D'après Knuth, c'est
l'ordre "psychologiquement correct": l'ordre dans lequel on comprend mieux le
programme.  Le langage WEB permet d'écrire le programme dans cet ordre.

D'ailleurs, c'est possible de rajouter du code à une section ultérieurement,
une sorte d'/addendum/:

#+BEGIN_SRC pascal
@<Other constats of the program@>+=
ord_max = 30;
#+END_SRC

En l'occurrence, cette constante est définie /après/ sa première apparition
dans du code.  Un peu au dessus, on peut lire:

#+BEGIN_SRC pascal
@<Variables of the program@>+=
ord: 2..ord_max;
#+END_SRC

La valeur maximale que peut prendre ~ord~ est d'abord justifiée
mathématiquement, avant d'être définie en code.  À travers le ~+=~, du code peut
être rajouté à une section, de n'importe quel point du document source.  C'est
particulièrement adapté pour les constantes et les variables, qui en Pascal
doivent être déclarées au début du programme.  Mais toutes les variables ne sont
pas utilisées par toutes les parties du programme; d'où le mécanisme de
concaténation de WEB qui permet de regrouper les variables avec le morceau de
code qui les utilise.  C'est une sorte de regroupement des préoccupations.

Le document source contient donc du TeX et du Pascal.  À partir de ce source,
deux documents sont générés par deux programmes différents: TANGLE et WEAVE.
TANGLE reconstruit le programme Pascal monolithique; il parcoure les sections et
remplace les références par leur contenu, afin d'émettre du code qui pourra être
accepté par un compilateur Pascal.  WEAVE produit le document lisible par un
humain et imprimable.  Il numérote les sections et introduit des liens pour
chaque référence; il indique également quelles sections utilise quel code, pour
permettre au lecteur de s'y retrouver sur un format papier.  Ce document est le
programme littéraire: une exposition structurée du programme, qui contient à
la fois tout le programme et les explications de chaque morceau.

#+ATTR_HTML: :style margin-top:-15rem
#+BEGIN_side-figure
[[file:img/knuth1.svg]]

À partir d'un programme littéraire WEB, WEAVE produit un document imprimable, et
TANGLE produit l'exécutable.
#+END_side-figure

Il y a deux différences majeures entre un programme littéraire et un programme
agrémenté de commentaires classiques.  Dans un programme classique, la structure
du code est imposée par le langage de programmation, ainsi que l'ordre
d'apparition des fonctions ou des variables.  Lire le code pour le comprendre
demande de suivre les appels, et de savoir distinguer les parties essentielles à
la fonctionnalité des détails annexes (comme le traitement d'erreurs, ou de cas
particuliers rares).  Dans le programme littéraire, l'ordre est imposé par le
discours, l'explication du programme.  La structure est donc complètement
renversée: l'explication domine, et le programme est secondaire; il découle de
l'explication, et n'est pas nécessaire à la compréhension du programme.

Mais le programme littéraire, par le mécanisme des sections, permet également de
recomposer les éléments du programme.  Les variables peuvent être définies à
plusieurs endroits du document.  Les traitements des erreurs communes à plusieurs
fonctions peuvent être relocalisés et décrits dans une même annexe.  Seuls les
langages de programmation qui ont des macros suffisamment flexibles peuvent
permettre cette recomposition du code.  Un programme littéraire vient ajouter
cette fonctionnalité au dessus de n'importe quel langage.

#+ATTR_HTML: :style margin-top:-3rem
#+BEGIN_aside
WEB est définit pour le langage Pascal.  Mais d'autres implémentations de la
programmation littéraire peuvent s'appliquer à tous les langages.  Noweb et Org
mode sont deux exemples.
#+END_aside

La programmation littéraire permet de séparer les préoccupations.  Knuth
n'utilise pas le terme, mais donne un exemple générique d'une procédure ~update~
qui met à jour une structure de données:

#+BEGIN_EXAMPLE
procedure update;
begin if (input data is invalid) then
  (Issue an error message and try to recover);
(Update the data structure);
end.
#+END_EXAMPLE

Dans un programme classique, le code pour traiter l'erreur et le code pour
mettre à jour la structure sont tous deux placés dans la procédure.  Or, si le
code de la mise à jour est trivial (une ou deux lignes), et que le traitement de
l'erreur ne l'est pas (10-15 lignes), il y a un déséquilibre.  Traiter le cas
particulier prend une place disproportionnée par rapport à la fonctionnalité
essentielle de la procédure.  Knuth pense qu'un programmeur, /inconsciemment/,
tente de minimiser le code du traitement d'erreur pour rétablir une proportion
en accord avec l'importance de chaque morceau de code.  En conséquence, le
traitement de l'erreur sera peut-être incomplet.

Mais dans un programme littéraire, le code du traitement de l'erreur n'est que
mentionné dans la procédure.  Les détails apparaîtront plus loin, dans une
section dédiée, où le programmeur a tout le loisir d'écrire le traitement
d'erreur le plus exhaustif sans entraîner un quelconque déséquilibre.

Il faut certainement fournir plus d'efforts pour écrire un programme littéraire.
Là où un programme classique peut se contenter du code et de quelques
commentaires, un programme littéraire encourage de documenter le raisonnement
derrière le code, ce qui demande de clarifier toutes les décisions faites par
le programmeur.  Knuth nous rappelle que ce n'est pas forcément un désavantage
de la méthode:

#+BEGIN_QUOTE
I had known for a long time that the programs I construct for publication in a
book, or the programs that I construct in front of a class, have tended to be
comparatively free of errors, because I am forced to clarify my thoughts as I do
the programming.  By contrast, when writing for myself alone, I have often taken
shortcuts that proved later to be dreadful mistakes.  It's harder for me to fool
myself in such ways when I'm writing a WEB program, because I'm in "expository
mode" (analogous to classroom lecturing) whenever a WEB is being spun.  Ergo,
less debugging time.
#+END_QUOTE

-----

La programmation littéraire change les priorités du programmeur.  Dans un
programme classique, on construit une suite d'instructions valides pour la
machine d'abord, et un programme compréhensible par autrui en second lieu.  Dans
un programme littéraire, la priorité est de communiquer un raisonnement à
l'humain, et dans un second temps produire un programme pour la machine.

WEB est le langage proposé par Knuth qui permet de recomposer les programmes
Pascal selon le sens "psychologiquement correct"; le sens de l'explication
naturel d'un article ou livre technique.  WEB permet de définir du code
partiellement, qui contient des références vers les parties manquantes.  Le
"code source" mêle prose (en TeX) et code (en Pascal), et deux compilateurs
traversent la source pour produire soit un document imprimable pour le lecteur,
soit un programme exécutable par la machine.

*** La séplution
:PROPERTIES:
:CUSTOM_ID: hyperj
:END:
La programmation par aspects a pour but la séparation des composants et des
aspects.  C'est à dire, la séparation de /certaines/ préoccupations: les
préoccupations transversales, orthogonales à la fonctionnalité du code.  En
programmation par aspects, on a deux axes de composition du programme: l'axe
des composants, pour la fonctionnalité principale, et l'axe des aspects, pour
les fonctionnalités transversales.  L'axe des aspects est toujours inférieur à
l'axe des composants: ce dernier est l'axe dominant, qui impose la structure du
programme, ce qui peut limiter les choix de composition du programme.  D'autres
recompositions sont possibles, comme celle proposée par Hyper/J [[cite:TOH+99][TOH+99]] [[cite:TO-00][TO-00]].

Hyper/J est une implémentation de la séparation pluridimensionnelle des
préoccupations (/séplution/) pour Java, tout comme AspectJ est une
implémentation de la programmation par aspects pour Java.  La séplution est un
paradigme qui permet la recomposition des préoccupations selon plusieurs axes,
et de façon symétrique, contrairement à la programmation par aspects.

Dans [[cite:TOH+99][TOH+99]], les auteurs motivent la séplution avec l'exemple d'un système qui
permet de construire des programmes dans un langage simple, une instance du
problème de l'expression.  Le système permet de créer des expressions
arithmétiques, qui sont des additions ou des soustractions de sous-expressions,
soit des nombres.

Pour implémenter ce système, ils proposent de représenter chaque nœud de l'arbre
syntaxique par une classe: une classe ~Expression~, une classe ~Number~, une
classe ~Plus~, etc.  Chaque classe implémente trois méthodes, qui correspondent
à trois traitements différents de l'arbre syntaxique: ~eval~ évalue la valeur
arithmétique de l'expression, ~display~ affiche l'expression, et ~check~ vérifie
qui l'expression est bien formée.

#+BEGIN_side-figure
[[file:img/hyperj1.svg]]

Les trois traitements communs à chaque nœud de l'arbre de syntaxe sont trois
préoccupations différentes, éparpillées dans chaque classe.  L'unité
d'abstraction, la classe, ne correspond pas à l'unité des traitements.
#+END_side-figure

Immédiatement, ils constatent que chaque traitement peut être considéré comme
une préoccupation, et que selon cette décomposition en classes, ces
préoccupations sont toutes éparpillées dans les méthodes correspondantes des
différentes classes.  Il n'y a pas de vue centralisée d'une préoccupation.

Cet éparpillement est un problème lorsqu'ils cherchent à étendre la
fonctionnalité du système.  Modifier une fonctionnalité nécessite de toucher à
une méthode dans différentes classes.  Ajouter la synchronisation des expressions
dans une base de donnée requiert d'ajouter des appels dans les constructeurs et
accesseurs de chaque classe.  Les modifications ont un impact élevé sur
l'implémentation, que les auteurs souhaitent minimiser:

#+BEGIN_QUOTE
The goal of low impact of changes requires /additive/, rather than /invasive/,
change.  Yet conceptually simple changes, like those in the expression system,
often have widespread and invasive effects, both within the modified artifact
and on related pieces of other artifacts.  This is primarily because units of
change often do not match the units of abstraction and encapsulation within the
artifcats.  Thus, additive changes in one artifact, like requirements, may not
translate to additive changes in other artifacts, like design and code.
#+END_QUOTE

La séplution doit donc permettre de modifier le système sans avoir à modifier sa
structure en profondeur.  Une nouvelle fonctionnalité devrait se traduire par un
ajout de code, et pas par de la modification de code existant.  Et pour réaliser
ce but, l'exemple du problème de l'expression montre que les unités
d'abstraction classiques sont insuffisantes; elles imposent toutes une dimension
de composition.  Et même si elles permettent plusieurs dimensions de
composition, comme la programmation par aspects, il y a toujours une dimension
qui prime sur l'autre.  C'est un constat qu'ils nomment la "tyrannie de la
décomposition dominante".

Pour remédier à cette tyranie, ils proposent l'/hypercoupe/: une façon de
regrouper tous les artefacts du système qui touchent à une même préoccupation.
Une hypercoupe contient le code source, la description informelle de la
fonctionnalité, sa spécification, son design, sa documentation, etc.

Continuant l'exemple, on peut regrouper les fonctionnalités d'évaluation,
d'affichage, de vérification et de synchronisation par des hypercoupes
différentes.  Le papier illustre à travers des diagrammes UML ce qu'Hyper/J
permet de faire en Java.

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_side-figure
[[file:img/hyperj2.svg]]

Recomposition en utilisant des hypercoupes pour chaque traitement.  Les trois
traitements sont isolés par chaque hypercoupe, mais les classes sont communes à
toutes.
#+END_side-figure

Une hypercoupe n'isole pas nécessairement une partie du système.  Les unités
d'abstraction que sont les classes et méthodes peuvent apparaître dans plusieurs
hypercoupes.  C'est le cas ici: la classe ~Expression~ apparaît dans toutes les
hypercoupes.  Les hypercoupes sont des organisations, des vues différentes du
même système, et certaines parties du systèmes sont pertinentes pour plusieurs
vues.

Une fois les hypercoupes définies, il faut également décrire comment elles se
composent pour former le système final: comment composer la documentation pour
produire un document unique, et comment composer le code pour produire un
exécutable.  Cette composition dépend évidemment des artefacts considérés.

En Hyper/J, la composition du code source se fait sur le nom de classe ou de
méthodes: deux classes ayant le même nom dans deux hypercoupes différentes
combineront leur contenu, et récursivement pour leurs méthodes.  Pour les
méthodes, on peut aussi spécifier qu'il faut surcharger la méthode de même nom
de l'hypercoupe précédente, sans pour autant utiliser l'héritage de classe.

-----

La séplution a pour but de minimiser les modifications à apporter au code source
lorsqu'une nouvelle fonctionnalité est implémentée, et d'abolir l'asymétrie de
la décomposition dominante.  L'idée importante de la séplution est de
reconnaître qu'une préoccupation a plusieurs dimensions dans le programme, et
n'apparaît pas seulement dans le code source.  Les hypercoupes sont des
projections de cet objet pluridimensionnel qui regroupent tous les artefacts
relatifs à une préoccupation: le code source, mais aussi les documents de
conception.  La séplution est donc une méthodologie de développement, du design
à l'implémentation, dont les unités d'abstraction sont les fonctionnalités du
système.

#+ATTR_HTML: :style margin-top:-6rem
#+BEGIN_aside
Une méthodologie qui est proche des domaines de la programmation par
fonctionnalité (/Feature-Oriented Programming/) et de l'ingénierie des modèles
(/Model-Driven Engineering/).  Domaines qui sont ne sont pas les objets de ce
document.
#+END_aside

Hyper/J est une implémentation de la séplution pour Java.  Un programmeur écrit
des hypercoupes, et précise leur composition avant de les passer au compilateur,
qui génère du bytecode Java.  Le mécanisme de composition est une instance de
/superimposition/.

*** L'information transparente
Les mécanismes de décomposition sont insuffisants pour anticiper les changements
d'un programme.  La programmation littéraire, AspectJ et Hyper/J proposent de
nouvelles façons d'organiser le code source pour y remédier, mais adopter ces
techniques demande d'adopter un nouveau paradigme, et de nouveaux outils.  Que
ce soit du TeX, des aspects, ou des hypercoupes, il faut réécrire une bonne
partie du code source et suivre la discipline prescrite afin de bénéficier de
ses vertus.  Mais le plus grand risque, au long terme, c'est d'adopter de
nouveaux outils, des compilateurs qui non seulement sont spécifiques à un seul
langage (WEB vise Pascal, et AspectJ et Hyper/J sont faits pour Java), mais qui
pourraient tomber en désuétude faute d'engouement.  L'/information transparente/
[[cite:Gri-01][Gri-01]] est une alternative pragmatique qui s'applique à n'importe quel langage,
en utilisant des outils standards.

La motivation de l'auteur est de mieux appréhender des changements de logiciels
qui dépassent la décomposition modulaire offerte par les langage de
programmation classiques.  Une décomposition modulaire adéquate doit minimiser
ce type de changements, mais, quelle que soit la décomposition, elle ne peut pas
convenir à tous les axes d'évolution.

Le principe de l'information transparente est d'exploiter les similarités du
code, et en particulier les similarités /cohérentes/:

#+BEGIN_QUOTE
Code elements likely to be changed together as part of a complete, consistent
change should look similar, and code elements unlikely to be changed together
should look different.
#+END_QUOTE

L'auteur donne l'exemple d'un outil de planification qui visait des programmes
écrits en C et qu'il a fallu adapter pour des programmes écrits en Ada.  L'outil
construit un arbre syntaxique des programmes C, et élabore un diagramme à partir
de l'arbre.  Pour pouvoir faire la même chose pour des programmes en Ada, il
faut capturer les boucles.  En C, il faut par exemple capturer ~while~.  En Ada,
la construction est ~loop~.  Puisque le code de l'outil est /cohérent/, les
variables et les fonctions qui manipulent le ~while~ de C s'appellent ~WHILE~,
ou ~WhileStmt~.  Du coup, il suffit de faire une simple recherche sur tous les
fichiers du projet, avec ~grep~, pour trouver tous les endroits qu'il faudra
adapter pour les programmes Ada.

Il rapporte un autre exemple: intégrer le support du standard Unicode dans des
logiciels qui ne traitent que de l'ASCII.  En Unicode, un octet ne correspond
pas nécessairement à un caractère, contrairement à l'ASCII. Pour compter le
nombre de caractères d'une chaîne en ASCII, il suffit de compter les octets.
Mais en Unicode, les deux ne sont plus équivalents.  Il faut donc distinguer les
deux cas d'utilisation: est-ce que ce ~int~ compte les caractères, ou les
octets?  Le type de la variable ne suffit pas pour donner cette information.  En
revanche, les programmeurs du logiciel discernent les deux cas à travers les
noms de variable: une variable préfixée par ~cch~ contient un nombre de
caractères, et un préfixe ~cb~ indique un nombre d'octets.  Puisque cette
nomenclature est respectée par tout le projet, elle peut être utilisée pour
distinguer les deux cas d'usage, et faciliter le passage à Unicode.

#+ATTR_HTML: :style margin-top:-6rem
#+BEGIN_aside
Une forme de notation hongroise, où le type /logique/ de la variable est indiqué
plutôt que le type utilisé par le compilateur.
#+END_aside

Ce principe ne s'applique pas seulement au code.  Dans les commentaires de
programmes, il est courant de de trouver les mots ~TODO~, ~FIXME~ ou ~REFACTOR~
pour indiquer un problème que l'auteur, dans le feu de l'action, note pour une
correction ultérieure.  Ces mots-clés jouxtent le code auquel ils s'appliquent,
ce qui permet à tout programmeur qui va voir le code de les prendre en compte.
Mais on peut aussi facilement obtenir une liste de tous les problèmes du projet
en faisant une simple recherche de texte.  L'environnement de développement
Eclipse affiche une liste de toutes les tâches du projet, construite en glanant
ces mots-clés.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_aside
TODO: add Eclipse ref
#+END_aside

#+ATTR_HTML: :style margin-top:2rem
#+BEGIN_side-figure
[[file:img/griswold0.svg]]

Les similarités d'un projet peuvent être récupérées par des outils standard.
Ici, ~grep~ permet de créer différentes vues du projet.
#+END_side-figure

L'information transparente permet de recréer de la localité grâce à la
similarité des morceaux de code.  Tous les morceaux affectés par le même
changement sont localisés par leur forme: que ce soit l'emploi de variables de
même nom, ou ayant un préfixe commun, ou la présence d'un mot-clé, ou même une
similarité dans l'arbre de syntaxe:

#+BEGIN_QUOTE
When the code relating to a particular change is not localized to a module, an
information-transparent module design allows a programmer to use available
software tools to economically identify the related code, easing the
change. That is, the “signature” of the changing design decision can be used to
create locality out of similarity, providing a module view of the code that
crosscuts its explicit textual, procedural, module, and file structures.  This
signature is one or more shared characteristics of the code to be changed, such
as the use of particular variables, data structures, language features, or
system resources.
#+END_QUOTE

L'avantage de cette approche c'est qu'elle peut s'appliquer à n'importe quel
langage de programmation—même à n'importe quel fichier texte—puisqu'elle repose
sur des conventions et des outils standards.  Elle demande moins de changements
structurels, ou en tout cas des changements qui n'affectent généralement pas la
sémantique du code, ce qui la rend plus économique à adopter qu'un nouveau
paradigme.

En contrepartie, les conventions sont fragiles.  En utilisant une notation
hongroise pour guider l'adaptation d'Unicode, il n'y aucune garantie
d'exhaustivité ou de correction.  Peut-être que plusieurs variables comptent des
caractères et s'appellent simplement ~i~.  Ou pire, peut-être qu'une variable
préfixée ~cch~ est en réalité utilisée pour compter des octets!  L'information
transparente facilite la localisation des similarités, mais n'offre aucune
garantie, juste un compromis pragmatique.

# Not sure that any tool or programming language could guard against "logical"
# mistakes made by the programmer.  But that is an insight best left to the
# Synthesis section, along with a well-chosen XKCD strip.

** BARE Limiter les effets de bord: la programmation fonctionnelle
Parallel evolution to OO.

Pure functions, immutability, side-step the complications of side-effects
entirely.

Functional is simple, because referential transparency.

Real programs need side effects, they are delimited by monads, effect systems.

Functional languages historically have the more advanced type systems.  Strong
static guarantes, but less runtime flexibility.

** La modularité du programme n'est pas la modularité du code source
*** STABLE La modularité selon Parnas
David Parnas est fréquemment cité comme référence pour la notion de modularité
d'un système, en particulier l'article "On the Critera to be Used In Decomposing
Systems into Modules" [[cite:Par-72][Par-72]].  Mais Parnas s'intéresse davantage à la phase de
/conception/ d'un système qu'à la phase d'implémentation.

Dans l'article, il décrit deux décompositions en modules d'un même système
d'indexation ; un exemple didactique qui peut être implémenté "par un bon
programmeur en une ou deux semaines".  La première décomposition comporte 5
modules, la seconde 6.  Les deux décompositions sont supposées produire des
programmes équivalents: qui fournissent les même fonctionnalités.  Mais si les
deux programmes sont équivalents, quel intérêt à choisir une décomposition
plutôt qu'une autre?  La réponse vient en s'intéressant aux choix
d'implémentation qui ont été laissés en suspens.

Suivant le format d'entrée des données, ou l'emplacement mémoire de sauvegarde
des données, il faudra modifier certaines parties du programme en conséquence.
Et c'est là que les deux designs ne sont plus équivalents: le second changement
touche tous les modules de la première décomposition, alors que les changements
sont restreints à un seul module dans la seconde décomposition.  Le second
design est donc mieux adapté aux changements potentiels anticipés par le
concepteur.

#+ATTR_HTML: :style margin-top:-11rem
#+ATTR_LATEX: :options [-11em]
#+BEGIN_side-figure
[[file:img/parnas-0.svg]]

Dans le second design, les changements sont restreints à un seul module.
#+END_side-figure

#+ATTR_HTML: :style margin-top:5rem
#+BEGIN_side-figure
[[file:img/flowchart.svg]]

Un logigramme.  Une façon courante de concevoir des programmes de 1950 à 1970.
#+END_side-figure

Le second design est aussi plus facile à développer et plus facile à comprendre.
Les modules du premier design correspondent aux étapes de traitement du système:
Parnas nous dit qu'il suffit de dessiner le logigramme pour obtenir les 5
modules.  Le résultat c'est que tous ces modules ont de nombreuses dépendances
entre eux, ce qui pousse les programmeurs à avoir une compréhension globale du
système pour pouvoir le développer.  En revanche, il a élaboré le second design
en suivant le principe d'/encapsulation/: les modules ne correspondent plus à
des étapes de traitement, mais plutôt à des décisions de design, des
responsabilités:

#+BEGIN_QUOTE
Every module in the second decomposition is characterized by its knowledge of a
design decision which it hides from all others.  Its interface or definition was
chosen to reveal as little as possible about its inner workings.
#+END_QUOTE

#+ATTR_HTML: :style margin-top:-1rem
#+BEGIN_aside
Dans le même article, Parnas conseille aussi de mettre dans un même module une
structure de données et ses procédures d'accès et de modification.  Une notion
de type abstrait de donnée qu'il ne nomme pas comme telle.
#+END_aside

Si l'on suit ce critère pour décomposer un système en modules, alors on peut
qualifier le système de /modulaire/.  Parnas liste les trois avantages attendus
d'un système modulaire:

#+BEGIN_QUOTE
(1) managerial—development time should be shortened because separate groups
would work on each module with little need for communication: (2) product
flexibility—it should be possible to make drastic changes to one module without
a need to change others; (3) comprehensibility—it should be possible to study
the system one module at a time.
#+END_QUOTE

Le premier design a beau comporter des modules, il n'a pas ces avantages.  Il
n'est donc pas modulaire.  Sur un second exemple de système, un compilateur et
interpréteur pour le même langage, il conclut là encore qu'une décomposition
suivant le critère d'encapsulation est supérieure à une décomposition qui suit
les étapes de traitement.  Un interpréteur et un compilateur résolvent en partie
les mêmes problèmes, et la décomposition préconisée par Parnas permet de
/réutiliser/ les modules d'un système à l'autre.

#+BEGIN_side-figure
[[file:img/parnas-overhead.svg]]

L'encapsulation cause de l'indirection, ce qui peut réduire l'efficacité du
programme en augmentant les appels de procédures sur la pile.
#+END_side-figure

Néanmoins, il y a un obstacle à utiliser une décomposition dont les modules
adhèrent strictement à l'encapsulation: l'efficacité du système.  Dans une telle
décomposition, les appels de procédures sont plus nombreux et comportent
davantage d'instructions.  Là où le design qui suit le logigramme peut accéder
directement à la mémoire pour modifier les données utilisées par un autre
module, dans la décomposition suivant Parnas chaque module est responsable de
ses propres données; il faut forcément passer par les procédures d'accès et de
modification du module.  Ces procédures peuvent en plus effectuer des
vérifications sur leurs paramètres, qui ne sont peut-être pas nécessaires dans
tous les cas, ce qui rajoute encore des instructions que la machine devra
effectuer.

Parnas reconnaît ce problème d'efficacité, et suggère de concevoir un outil qui
transforme le code de la décomposition modulaire en code qui suit davantage le
fil d'exécution.  Le concepteur peut travailler sur la représentation modulaire,
et l'outil se charge de générer du code efficace.  Dans le code généré par cet
outil, les appels de procédures inutiles sont supprimés, et l'efficacité n'est
pas sacrifiée.  En revanche, la décomposition modulaire ne serait plus
apparente.  Il propose donc de conserver le programme sous ses plusieurs formes
et de développer des outils pour passer d'une forme à l'autre.

#+ATTR_HTML: :style margin-top:-8rem
#+BEGIN_aside
Ce ne sont que des suggestions, et l'article ne dit pas comment réaliser ces
transformations.  On trouve des mises en œuvre de ces idées dans le Literate
Programming de Knuth, et dans l'AOP de Xerox.
#+END_aside

Parnas met donc en avant le critère d'encapsulation pour concevoir un système
modulaire.  Mais surtout, il insiste sur l'importance de la phase de conception
du système, indépendamment de son implémentation en code.  Parnas ne croit
d'ailleurs pas que le choix du langage de programmation puisse rendre un système
modulaire.  Un point qu'il développe dans "Why Software Jewels are Rare" [[cite:Par-96][Par-96]].

Un "logiciel joyau" est un programme "bien structuré écrit dans un style
homogène, sans bidouilles, développé tel que chaque composant est simple et
organisé, et conçu pour que le produit soit facile à changer".  Parnas donne
plusieurs raisons qui explique, selon lui, la rareté de ces joyaux: le logiciel
existe pour répondre à un besoin, et la structure interne du programme n'est pas
un besoin du client; un logiciel est plus utile à l'utilisateur s'il répond à
plusieurs besoins, ce qui implique d'enrichir ses fonctionnalités, au détriment
de la simplicité du programme.  Les contraintes matérielles ne laissent que peu
de place à l'élégance structurelle du code.  Parnas rapporte notamment son échec
à vouloir mettre à jour un système d'ordinateur de vol pour l'armée américaine;
la machine ciblée possédait un matériel très limité qui demandait une
optimisation manuelle de l'utilisation des registres, mais Parnas et son équipe
désiraient s'abstraire de ces détails matériels:

#+BEGIN_QUOTE
Near-optimal register allocation was essential to fitting the program into a
very small memory.  One of our design goals had been to achieve hardware
independence for most of our code.  To achieve hardware independence on the
specified processor, we needed an effective register allocation algorithm.  The
previous software for this task had been sucessful because none of the code was
portable and register allocation was done by hand.  We never found the necessary
register allocation algorithm.
#+END_QUOTE

Et contrairement à ce que l'on pourrait penser, la loi de Moore ne résout pas le
problème.  Si les machines possèdent deux fois plus de registres et sont deux
fois plus rapides, alors on leur demandera de faire deux fois plus de calculs
(ou de les faire deux fois plus rapidement).  Parnas le note:

#+ATTR_HTML: :style margin-top:-5rem
#+BEGIN_aside
Phénomène relaté avec humour par Wirth [[cite:Wir-95][Wir-95]]: "Software expands to fill the
available memory"; "Software is getting slower more rapidly than hardware
becomes faster".
#+END_aside

#+BEGIN_QUOTE
Although today's machines are far better than the one we were using, goals have
expanded and competitive pressures often limit the resources available.  Few of
today's designers are free to ignore performance requirements and hardware
limitations.
#+END_QUOTE

Mais surtout, il faut se méfier des marchands de panacée:

#+BEGIN_QUOTE
Sometimes new languages are used in the design of jewels, and authors may
attribute a product's success to the use of a particular language or type of
language.  Here, I have grave doubts.  I have lost count of the number of
languages that have been introduced to me as the solution to the software
problems that everyone experiences.  First, I was told to use Fortran instead of
an assembler language.  Later, others advocated Algol-60 and its derivatives as
the cure to the ugly software resulting from Fortran.  Of course, NPL, later
known as PL/I, was going to provide an even better solution.  The list goes on.
Wirth promotes Oberon while hundreds of people are telling me that an
object-oriented language must be used to get clean software.  I no longer
believe such claims.  The issue is design, not programming language.

[...]

We should not ignore the fact that most modern languages have inherent
disadvantages.  A language that supports certain approach to software design
often compels us to use a particular implementation of a design principle, one
that may be inappropriate for the task at hand.  For example, many languages
that support modules, abstract data types, and object classes require the use of
subroutines where macro expansion might be a better choice.  Moreover, languages
that prevent programming errors, a goal advanced by some inveterate language
designers, are as feasible as knives that can cut meat but not hands.  We need
sharp tools to do good work.
#+END_QUOTE

Un langage de programmation est un outil parmi d'autres, et aucun outil ne peut
résoudre tous les problèmes.  C'est au concepteur de choisir l'outil adapté qui
est le plus à même de résoudre le problème qu'il a en vue.  Choisir un langage
n'affranchit pas le concepteur d'avoir à se poser des questions difficiles sur
la structure du système.  En 2003, Parnas déplore cette tendance [[cite:DBB+03][DBB+03]]:

#+ATTR_HTML: :style margin-top:18rem
#+BEGIN_aside
Cette dernière phrase est cible directement les trois autres panélistes qui lui
font face, qui sont les instigateurs respectifs de l'AOP, du FOP, et d'Hyper/J.
#+END_aside

#+BEGIN_QUOTE
To a man with a hammer, everything looks like a nail.  To a Computer Scientist,
everything looks like a language design problem.  Languages and compilers are,
in their opinion, the only way to drive an idea into practice.

My early work clearly treated modularisation as a design issue, not a language
issue.  A module was a work assignment, not a subroutine or other language
element.  Although some tools could make the job easier, no special tools were
needed to use the principal, just discipline and skill.  When language designers
caught on to the idea, they assumed that modules had to be subroutines, or
collections of subroutines, and introduced unreasonable restrictions on the
design.  They also spread the false impression that the important thing was to
learn the language; in truth, the important thing is to learn how to design and
document.  We are still trying to undo the damage caused by the early treatment
of modularity as a language issue and, sadly, we still try to do it by inventing
languages and tools.
#+END_QUOTE

Parnas réitère: le langage de programmation seul ne rend pas le programme
modulaire.  C'est aussi facile de créer un programme non-modulaire dans un
langage de haut niveau qu'en assembleur.  On ne pourra donc pas rendre les
programmes modulaires simplement en fournissant de meilleurs languages ou
outils.

Alors, quelle est la bonne direction à suivre?  D'après Parnas, il faut insister
sur la phase de conception du système; c'est là où les limites entre modules
apparaissent, et là où les décisions de design doivent être prises:

#+BEGIN_QUOTE
My engineering teacher laid down some basic rules:

1. Design before implementing.
2. Document your design.
3. Review and analyze the documented design.
4. Review implementation for consistency with the design.

There rules apply to software as least as much as they do to circuits or
machines.
#+END_QUOTE

# Missing illustrations for the main points of this section:
# - encapsulation

**** Conclusions
Parnas est souvent cité pour ses travaux sur la modularité, à juste titre, mais
son message principal n'est pas toujours bien entendu.  On trouve dans l'article
séminal de 1972 les avantages d'un système modulaire, et un critère pour les
concevoir: le critère d'encapsulation.  Quelles parties du système ont besoin de
pouvoir être changées?  D'une machine à l'autre, ou en réponse à des besoins
futurs?  Ces parties déterminent les modules du système selon le critère
d'encapsulation.  De ce critère découlent des notions de séparation des
préoccupations et de type de données abstrait (qu'il ne nomme pas).
L'encapsulation est un critère supérieur à la façon usuelle de découper les
programmes selon le processus de traitement des données, en suivant un
logigramme.

Parnas nous permet donc de /qualifier/ la modularité d'un système.  Un système
simplement découpé en modules arbitraires n'est pas modulaire.  Mais si pour
changer la base de données utilisée par le système il suffit de modifier le code
d'un seul module, alors ce système est modulaire /par rapport/ à la base de
donné utilisée.  La base de donnée initiale est un choix de conception, mais un
choix qui peut être changé ultérieurement sans demander la refonte totale du
système.  Le système est modulaire /par rapport/ à un ensemble de choix qui
peuvent être altérés ultérieurement.

On peut donc voir la modularité comme une des fonctionnalités du système; une
fonctionnalité à destination des programmeurs chargés de la maintenance du
système, plutôt qu'une fonctionnalité destinée aux utilisateurs.  Et comme toute
fonctionnalité, la modularité impose un coût de complexité au système.  Si le
programme est flexible, s'il permet de changer la base de donnée utilisée, alors
il faut que le reste du système soit capable de fonctionner non plus avec une
seule base de donnée, mais avec plusieurs bases différentes.  Gérer cette
compatibilité impose de traiter plus de cas, ce qui inévitablement se traduit en
code.  De plus, pour implémenter un système modulaire, on fera souvent appel à
des interfaces, des indirections, des appels de procédures qui encapsulent les
décisions de conception.  Et ces interfaces ont souvent un impact négatif sur
l'efficacité du programme.  Ces compromis modularité/complexité et
modularité/efficacité me semblent inévitables.

*** STABLE La théorie de Naur derrière le programme
Un point de vue qui complémente celui de Parnas est celui de Peter Naur.  Dans
"Programming as Theory Building" [[cite:Nau-85][Nau-85]] il attache, comme Parnas, beaucoup
d'importance à la réflexion du programmeur dans l'élaboration d'un système.
Et il s'intéresse également à la modification de programmes, aux difficultés qui
apparaissent quand on essaye d'ajouter des fonctionnalités non prévues
initialement.

Pour Naur, la tâche principal du programmeur n'est pas de produire du code
source.  Un programmeur avant tout construit une /théorie/ du problème que le
programme doit résoudre.  Lors de la conception et de l'implémentation, le
programmeur construit sa connaissance du problème, du domaine d'application, et
des outils.  Cette connaissance est naturellement interne, et toute production
externe (code source, documentation, diagrammes) ne fait la refléter qu'en
partie.

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_side-figure
[[file:img/naur-theory.svg]]

Pour Naur, le code source et la documentation ne sont que des produits
secondaires de la programmation; mais l'activité principale du programmeur est
de construire une connaissance du problème, une /théorie/.
#+END_side-figure

Ce point de vue, Naur le tire de sa propre expérience à construire de larges
systèmes.  Il donne l'exemple d'un compilateur développé par un groupe A pour
une machine X.  Le compilateur fonctionne très bien, et un autre groupe de
programmeurs, le groupe B, souhaite étendre légèrement le langage, et réutiliser
ce compilateur pour une machine Y.  Le groupe B planifie les
changements à apporter au compilateur après avoir étudié sa structure, et
vient discuter des changements avec le groupe A qui offre son soutient:

#+BEGIN_QUOTE
In several major cases it turned out that the solutions suggested by group B
were found by group A to make no use of the facilities that were not only
inherent in the structure of the existing compiler but were discussed at length
in its documentation, and to be based instead on additions to that structure in
the form of patches that effectively destroyed its power and simplicity.  The
members of group A were able to spot these cases instantly and could propose
simple and effective solutions, framed entirely within the existing structure.
#+END_QUOTE

Le groupe B avait le code source et la documentation, et du temps pour les
étudier.  Malgré cela, les modifications du compilateur qu'il envisage sont
jugées inadéquates par le groupe A, qui est capable de proposer rapidement des
modifications plus simples et directes.  Naur en conclut que la documentation et
le code source n'ont pas été suffisants pour communiquer toutes les décisions de
conception au groupe B, pour communiquer la /théorie/ du compilateur.

"Théorie" ici n'a pas un sens abstrait; si un programmeur du groupe A possède la
théorie du compilateur, c'est que non seulement il sait comment le compilateur
fonctionne dans le détail, mais il est aussi capable de l'expliquer à quelqu'un
du groupe B, de répondre à des questions sur le compilateur, de débattre sur les
choix d'implémentation.  Avoir la théorie du programme, c'est avoir internalisé
une connaissance /pratique/, et être capable d'appliquer cette connaissance à
d'autres problème connexes:

#+BEGIN_QUOTE
The notion of theory employed here is explicitly /not/ confined to what may be
called the most general or abstract part of the insight.  For example, to have
Newton's theory of mechanics as understood here it is not enough to understand
the central laws, such as that force equals mass times acceleration.  In
addition, as described in more detail by Kuhn, the person having the theory must
have an understanding of the manner in which the central laws apply to certain
aspects of reality, so as to be able to recognize and apply the theory to other
similar aspects.
#+END_QUOTE

Pourquoi s'intéresser à cette notion de théorie?  Parce qu'elle permet de mieux
comprendre comment modifier un programme.  Naur commence par constater que
modifier des programmes est une pratique courante, motivée par l'économie.  Si
on a déjà un programme qui fait à peu près ce qu'on veut obtenir, c'est
raisonnable de penser que modifier ce programme sera plus rapide que d'en
recréer un complètement nouveau.  Naur remarque que ce raisonnement ne considère
que la facilité de modifier le /texte/ du programme.  En effet, le code source
d'un programme sous forme texte est facilement modifiable; bien plus qu'une
construction physique comme un immeuble, un pont, ou une voiture.  Si l'on
considère plutôt le point de vue de Naur — que le programmeur manipule avant
tout une théorie — on ne peut pas modifier le programme sans prendre en compte
la théorie sous-jacente:

#+BEGIN_QUOTE
What is needed in a modification, first of all, is a confrontation of the
existing solution with the demands called for by the desired modification.  In
this confrontation the degree and kind of similarity between the capabalities of
the existing solution and the new demands has to be determined.  The point is
that the kind of similarity that has to be recognized is accessible to the human
beings who possess the theory of the program, although entirely outside the
reach of what can be determined by rules, since even the criteria on which to
judge it cannot be formulated.
#+END_QUOTE

Si l'on se contente de modifier le code source sans considérer la théorie, on se
retrouve dans le cas du groupe B; à étendre le compilateur de façon
tarabiscotée, sans tirer partie de sa structure:

#+BEGIN_QUOTE
For a program to retain its quality it is mandatory that each modification is
firmly grounded in the theory of it.  Indeed, the very notion of qualities such
as simplicity and good structure can only be understood in terms of the theory
of the program, since the characterize the actual program text in relation to
such program texts that might have been written to achieve the same execution
behaviour, but which exist only as possibilities in the programmer's
understanding.
#+END_QUOTE

Donc, si l'on souhaite modifier le programme, il faut vraiment tenter de
modifier la théorie.  Pour cela, il faut d'abord avoir accès à cette théorie.
Si on n'a pas accès aux développeurs du programme, les possesseurs de sa
théorie, il faut tenter de la recréer à partir de code source et de la
documentation.  Naur appelle cela la "résurrection de programme".  À ses yeux,
c'est un acte sans espoir:

#+ATTR_HTML: :style margin-top: -4.5rem
#+BEGIN_aside
En filant la métaphore, l'ensemble des techniques qui permettent de modifier un
programme abandonné sans se soucier de sa théorie pourrait s'appeler
"nécromancie de programme".
#+END_aside

#+BEGIN_QUOTE
A very important consequence of the Theory Building View is that program
revival, that is re-establishing the theory of a program merely from the
documentation, is strictly impossible.

[...] building a theory to fit and support an existing program text is a
difficult, frustrating, and time consuming activity.  The new programmer is
likely to feel torn between loyalty to the existing program text, with whatever
obscurities and weaknesses it may contain, and the new theory that he or she has
to build up, and which, for better or worse, most likely will differ from the
original theory behind the program text.
#+END_QUOTE

Il concède que revivre un programme de cette façon peut être utile dans des
circonstances particulières, mais en étant bien conscient de l'effort à fournir
pour obtenir un résultat probablement de qualité inférieure.

L'alternative qu'il conseille, est de toujours recréer le programme:

#+BEGIN_QUOTE
In preference to program revival, the existing program text should be discarded
and the new-formed programmer team should be given the opportunity to solve the
given problem afresh.  Such a procedure is more likely to produce a viable
program than program revival, and at a no higher, and possibly lower, cost.
#+END_QUOTE

Modifier le code source du programme n'est facile qu'en apparence; car modifier
la fonctionnalité du programme demande de reconstruire sa théorie, afin de
s'assurer que les changements envisagés auront bien les effets voulus.
Puisqu'il faut reconstruire la théorie du problème initial, puis l'adapter au
problème qui intéresse le programmeur, l'effort sera moindre en construisant une
théorie directement adaptée au nouveau problème.

Naur attaque une autre idée populaire du génie logiciel: qu'une méthode de
programmation, un ensemble de règles à observer, peut être supérieure à une
autre.  Une méthode préconise les étapes du processus de développement, ou les
documents à produire et dans quel ordre.  Mais pour Naur, le seul point qui
importe c'est le développement de la théorie du programme, et aucune méthode ne
peut garantir une construction correcte:

#+BEGIN_QUOTE
A method implies a claim that program development can and should proceed as a
sequence of actions of certain kinds, each action leading to a particular kind
of documented result.  In the Theory Buiding View what matters most is the
building of the theory, while production of documents is secondary.  In buiding
the theory there can be no particular sequence of actions, for the reason that a
theory held by a person has no inherent division into parts and no inherent
ordering.  Rather, the person possessing a theory will be able to produce
presentations of various sorts on the basis of it, in response to questions or
demands.
#+END_QUOTE

Pour les mêmes raisons, le choix du langage de programmation, ou d'un formalisme
particulier ne peut remplacer la construction de la théorie:

#+BEGIN_QUOTE
As to the use of particular kinds of notation or formalization, again this can
only be a secondary issue since the primary item, the theory, is not, and cannot
be, expressed, and so no question of the form of its expression arises.
#+END_QUOTE

Quoi dire alors aux programmeurs novices?  Comment leur faire comprendre ces
notions, comment les amener à construire d'élégantes théories pour créer des
programmes flexibles et efficaces?  Naur recommande de les former principalement
de façon organique: en travaillant avec des programmeurs chevronnés, ils
absorberont les connaissances nécessaires par osmose.

#+BEGIN_QUOTE
What remains is the effect of methods in the eduction of programmers.  Indeed,
on this view the quality of the theory built by the programmer will depend to a
large extent on the programmer's familiarity with model solutions of typical
problems, with techniques of description and verification, and with principles
of structuring systems consisting of many parts in complicated interactions.

While skills such as the mastery of notations, data representations, and data
processes, remain important, the primary emphasis would have to turn in the
direction of furthering the understanding and talent for theory formation.  To
what extent this can be taught at all must remain an open question.  The most
hopeful approach would be to have the student work on concrete problems under
guidance, in an active and constructive environment.
#+END_QUOTE

**** Conclusions
Naur établit une distinction importante entre le source code d'un programme, et
la connaissance que le programmeur a de son fonctionnement.  C'est une
distinction intuitive pour quiconque a une expérience même modeste de
programmation.  Cette distinction nous permet notamment de comprendre pourquoi
modifier un programme n'est pas simplement modifier du texte, modifier son code
source.  Il faut prendre en compte les choix faits au moment du design du
programme, et considérer comment les changements que l'on souhaite apporter
affectent ces choix.  Comprendre la théorie.

Naur estime que la théorie n'est pas communicable; qu'elle reste toujours
interne au programmeur qui la construit.  Mais à l'évidence le programmeur est
toujours capable de la communiquer /en partie/.  La théorie, c'est savoir
comment le programme fonctionne, pourquoi telle partie est nécessaire, comment
étendre le programme... Naur dit lui-même plusieurs fois qu'un programmeur qui
possède la théorie est capable de répondre à ces questions sur le programme,
c'est bien qu'il y a une personne qui doit interpréter ces réponses et
reconstruire la théorie.  Quand bien même la théorie ne peut être communiquée
/exactement/ comme telle d'un programmeur à l'autre, il suffit d'en communiquer
une partie suffisante pour satisfaire les besoins de l'autre.

Le message général est proche de Parnas: l'important dans la programmation ce ne
sont pas les formalismes, les langages ou les outils; c'est de réfléchir, de
comprendre le problème, et construire une solution qui satisfait les contraintes
données.   Et c'est cette aptitude à résoudre le problème qui devraient être
enseignée principalement aux novices.  En cela, la programmation n'est pas
différente d'autres activités:

#+BEGIN_QUOTE
This problem of education of new programmers in an existing theory of a program
is quite similar to that of the educational problem of other activities where
the knowledge of how to do certain things dominates over the knowledge that
certain things are the case, such as writing and playing a music instrument.
The most important educational activity is the student's doing the relevant
things under suitable supervision and guidance.  In the case of programming the
activity should include discussions of the relation between the program and the
relevant aspects and activities of the real world, and of the limits set on the
real world matters dealt with by the program.
#+END_QUOTE

# What does it mean for us?  Naur says we can't modify programs correctly.
# Program necromancy is the dark arts of resurrecting programs for quick fixes.

*** Notion of modularity
Notion of modularity [[cite:OGK+11][OGK+11]].  Modularity is rooted in classical logic thinking.
Classical logic is inflexible, incompatible with the realities of software.
Especially, information hiding is not the silver bullet.  Approaches to software
development that seem to break information hiding, and even oppose modular
reasoning, have their virtues.  Those can be thought of using nonclassical
logics.

* Le problème: étendre des interpréteurs
:PROPERTIES:
:CUSTOM_ID: problem
:END:
** UNSTABLE Motivation: extension ad-hoc de Narcissus
Narcissus est un interpréteur JavaScript écrit et maintenu par Mozilla
[[cite:Narcissus][Narcissus]].  Narcissus est écrit en JavaScript, et est meta-circulaire: il
utilise l'environnement hôte pour son implémentation (p.ex., l'objet ~String~
exposé au code client n'est pas réimplémenté par Narcissus, mais est une simple
façade de l'objet ~String~ hôte).  Narcissus est une implémentation relativement
légère (environ 6000 lignes de code) du standard ECMAScript [[cite:ECM-99][ECM-99]], qui permet
de rapidement prototyper des fonctionnalités expérimentales pour le langage.

En 2012, Austin et Flanagan se sont servi de Narcissus pour implémenter leur
analyse d'évaluation multi-facettes [[cite:AF-12][AF-12]], une analyse dynamique de flot
d'information qui permet à une valeur d'être étiquetée par une autorité qui a
des droits d'écriture et de lecture pour cette valeur.  Lorsqu'une valeur
étiquetée est utilisée dans une expression, son étiquette est propagée au
résultat de l'expression, ce qui préserve les permissions de l'autorité sur le
résultat.  Dans l'analyse multi-facettes, chaque valeur étiquetée a deux
facettes: une facette contient la valeur "privée" à destination de l'autorité,
une autre facettes contient la valeur "publique" destinée à des observateurs
tiers non autorisés.  Dans une expression, les facettes sont toutes deux
évaluées en même temps afin de produire les deux facettes du résultat.  Afin de
suivre les étiquettes même lors de branchements (des flots /indirects/),
l'évaluation multi-facettes maintient une liste des embranchements suivis lors
de l'exécution; cette liste est appelée /program counter/ (PC).

#+ATTR_HTML: :style margin-top:-12rem
#+BEGIN_side-figure
[[file:img/a-facet.svg]]

Une valeur à deux facettes.
#+END_side-figure

Par exemple, dans le code suivant, si le paramètre ~x~ est ~true~, alors la
fonction ~f~ retourne ~true~.  En revanche, si on fait de ~x~ une valeur à
facettes avec une valeur privée ~true~ et une valeur publique ~false~ (qu'on
écrit ~true:false~), alors le premier ~if~ sera exécuté deux fois: une fois pour
chaque facette de la condition.  Après le second ~if~, la fonction retourne la
valeur ~true:false~.  Un observateur non autorisé n'a accès qu'à la valeur
publique du résultat, et n'est donc pas capable d'inférer la valeur privée de
~x~, même à travers un flot indirect d'information.

#+ATTR_HTML: :style margin-top:-13rem
#+BEGIN_side-figure
[[file:img/fenton-example.svg]]
#+END_side-figure

Pour donner une idée de l'échelle du projet, Narcissus fait 6000 lignes de code,
et les deux plus gros fichiers sont le parseur (1600 lignes) et le fichier
principal de interpréteur, "jsexec" (1300 lignes).  Ce fichier principal
contient la logique pour interpréter des arbres de syntaxe abstraits, et pour
mettre en place l'environnement d'exécution des programmes clients.  Les
changements effectués pour l'implémentation de l'évaluation multi-facettes sont
restreints à ce fichier principal; 640 lignes sont affectées, soit la moitié.

#+ATTR_HTML: :style margin-top:-6rem
#+BEGIN_aside
On compte les lignes de code physiques, commentaires inclus mais sans compter
les lignes vides.
#+END_aside

Pour réaliser l'instrumentation de Narcissus pour l'évaluation multi-facettes,
les auteurs ont modifié directement le code source de l'interpréteur Narcissus.
On peut obtenir l'ensemble des changements effectués en extrayant un /diff/ des
deux versions.  La figure [[visual-narcissus-diff]] donne une vue d'ensemble des
changements.

#+ATTR_HTML: :style margin-top:-5rem
#+BEGIN_aside
Le /diff/ est extrait des HEAD de [[https://github.com/taustin/narcissus][github/taustin/narcissus]] et
[[https://github.com/taustin/ZaphodFacets][github/taustin/ZaphodFacets]].
#+END_aside

#+NAME: visual-narcissus-diff
#+CAPTION: Visualisation des modifications apportées par l'instrumentation de
#+CAPTION: Narcissus pour l'évaluation multi-facettes.  Le diff de "jsexec" y
#+CAPTION: est représenté intégralement, coupés en colonnes de même taille.
#+CAPTION: Chaque ligne est colorée suivant le type de changement dont elle fait
#+CAPTION: partie.  Les lignes grises ne sont pas affectées par
#+CAPTION: l'instrumentation.  [[file:img/narcissus-diff-legend.svg]]
[[file:img/narcissus-diff.svg]]

On constate immédiatement que les changements effectués par l'instrumentation
touchent de nombreuses parties du code de l'interpréteur, sans être restreints à
une ou deux régions particulières.  Les changements sont *éparpillés* dans le
code.  De plus, les changements de même nature—appartenant à la même
catégorie—ne sont pas regroupés.  Résultat, il devient difficile de comprendre
les effets de l'instrumentation à l'œil nu, ou de s'assurer de sa justesse par
rapport à une spécification formelle.  Il devient difficile également, sans
connaissances avancées de Narcissus et de l'évaluation multi-facettes, de savoir
si une ligne de code de l'interpréteur instrumenté concerne l'interprétation
décrite par le standard ECMAScript, ou si elle concerne l'évaluation
multi-facettes.  Le code de l'interpréteur instrumenté ne comporte aucune
information qui permet de les distinguer.

Autre point important: l'instrumentation *duplique tout le code* de
l'interpréteur.  C'est une solution simple pour créer un interpréteur qui
supporte l'évaluation multi-facettes.  En revanche, la duplication de code a un
impact important sur la maintenance à long terme: plus de double du code doit
être maintenu.  Les changements requis dans le code source pour corriger un bug
dans Narcissus, ou pour ajouter une fonctionnalité doivent désormais être
répétés dans l'instrumentation.  Le coût de maintenance devient prohibitif
lorsque /plusieurs/ instrumentations sont envisagées.

En observant le diff de plus près, on peut distinguer quatre catégories de
changements: les imports/exports, l'ajout du paramètre /program counter/,
séparer l'évaluation des valeurs à facettes, et les ajouts à l'objet ~global~.

De nouvelles définitions ont besoin d'être importées dans le module de
l'interpréteur, et une nouvelle fonction est exportée.  Ce sont de simples
ajouts qui sont localisés en début et en fin de fichier respectivement.  Voici
comment ils se présentent dans le code:

#+BEGIN_SRC diff
+ var FacetedValue = Zaphod.facets.FacetedValue;
+ var ProgramCounter = Zaphod.facets.ProgramCounter;
...

-      test: test
+      test: test,
+      getPC: getGC
#+END_SRC

#+ATTR_HTML: :style margin-top:-11rem
#+BEGIN_aside
Le symbole ~-~ marque une ligne supprimée de l'interpréteur; le symbole ~+~
marque une ligne ajoutée par l'instrumentation.  L'absence de marque indique une
ligne commune aux deux versions.
#+END_aside

Les changements effectués pour accommoder le /program counter/ utilisé par
l'analyse.  D'abord, le constructeur de l'objet ~ExecutionContext~ est étendu
pour accepter un argument supplémentaire: la valeur courante du /program
counter/, ~pc~.  Voici un extrait du diff qui illustre ce
changement:

#+BEGIN_SRC diff
- function ExecutionContext(type, version) {
+ function ExecutionContext(type, pc, version {
+   this.pc = pc;
#+END_SRC

Dans Narcissus, une instance de l'objet ~ExecutionContext~ est créée lorsque le
contrôle est transféré à du code client exécutable: lors de l'entrée dans une
fonction, lors d'un appel à ~eval~, ou lors de l'exécution d'un programme
entier.  L'objet ~ExecutionContext~ contient les variables importantes pour
l'exécution du code; en particulier l'environnement lexical utilisé pour
résoudre les noms de variables du code exécuté par ce contexte.  L'objet
~ExecutionContext~ est une réification du concept éponyme de la spécification
ECMAScript.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_aside
Voir la section 10.3 de la spécification.
#+END_aside

Puisque la signature du constructeur d'~ExecutionContext~ est étendue, tous ses
appels doivent être modifiés en conséquence pour fournir une valeur correcte
pour le paramètre /program counter/.  Il y a plus de 80 instances de ce simple
changement dans l'instrumentation.  En voici deux exemples:

#+BEGIN_SRC diff
- x2 = new ExecutionContext(MODULE_CODE);
+ x2 = new ExecutionContext(MODULE_CODE, x.pc);

- getValue(execute(n.children[0], x));
+ getValue(execute(n.children[0], x), pc);
#+END_SRC

Les changements effectués dans l'exécution de l'arbre de syntaxe abstrait (AST)
pour propager les étiquettes sur les valeurs à facettes.  Par exemple,
additionner deux valeurs à facettes devrait produire une nouvelle valeur à
facettes.  Dans l'implémentation, plutôt que de simplement additionner les deux
opérandes, l'interpréteur doit maintenant d'abord inspecter l'opérande gauche,
et si c'est une valeur à facettes, il faudra ajouter la valeur de l'opérande
droite à chaque facette.  Bien sûr, l'opérande droite peut également être une
valeur à facettes, et il faut alors séparer l'évaluation à nouveau.
L'interpréteur Narcissus ne contient aucun code pour gérer l'addition de deux
valeurs à facettes, donc l'instrumentation doit ajouter la logique nécessaire.
Pour ce faire, chaque évaluation d'une opération est enrobée dans un appel à la
fonction ~evaluateEach~ qui teste si une valeur est à facettes, et appelle
récursivement la fonction d'évaluation sur chaque facette si c'est le cas.  25
appels à ~evaluateEach~ ont été ainsi ajoutés dans l'instrumentation.  Le code
suivant donne la forme générale de ces changements:

#+BEGIN_SRC diff
- var v = getValue(node.a)
+ evaluteEach(getValue(node.a), function(v,x) {
    ... do something with v ...
+ }
#+END_SRC

À la première ligne on récupère une valeur d'un nœud de l'AST (p.ex., l'opérande
gauche d'une assignation, ou la condition d'un ~if~) puis on fait quelque chose
avec cette valeur.  Sur la seconde ligne, on récupère la même valeur, mais cette
fois on /sépare/ l'évaluation en appelant ~evaluateEach~ avec cette valeur comme
premier argument, et comme second une fonction qui opère sur une valeur simple.

Les changements effectués sur l'environnement d'exécution de code client.  Dans
un programme JavaScript, l'environnement d'exécution fournit un objet ~global~
qui contient les définitions de base comme ~Array~, ~Math~, ~String~ et
~Object~.  Puisque Narcissus est métacirculaire, il réutilise l'objet global de
son environnement hôte pour construire l'objet global de l'environnement client.
Ceci est fait en trois étapes.  Premièrement, Narcissus crée un objet
~globalBase~ avec les propriétés qui surchargeront celle de l'environnement
hôte.  Deuxièmement, il crée un objet ~global~ client à partir de l'objet
~global~ de son environnement hôte, et met toutes les propriétés de ~globalBase~
dans cet objet ~global~ client.  Troisièmement, il ajoute à cet objet ~global~
client les versions réfléchies de certains objets de base (~Array~, ~String~,
~Function~).

L'instrumentation de l'évaluation multi-facettes enrichit l'objet ~global~
client en ajoutant 50 propriétés à ~globalBase~, comme la suivante:

#+BEGIN_SRC diff
   var globalBase = {
   ...
+  isFacetedValue: function(v) {
+    return (v instanceof FacetedValue);
+  },
#+END_SRC

L'instrumentation change également la propriété ~String~ de ~globalBase~ pour
suivre les valeurs étiquetées passées en argument du constructeur de chaînes de
caractères.

Le fait que la plupart des changements appartiennent à une de ces quatre
catégories indique qu'il y a un potentiel de factorisation.  Si l'on souhaite
rendre Narcissus extensibles, afin de pouvoir définir l'évaluation
multi-facettes sans duplication de code, il faut trouver des façons d'exprimer
les changements de ces quatre catégories.  Mais si nous souhaites exprimer
d'autres analyses, et étendre d'autres interpréteurs, il faut tenter des trouver
des mécanismes génériques qui pourront être réemployés pour ces autres cas.

Pour prendre un peu de recul par rapport à l'instrumentation de Narcissus, on
peut s'intéresser à la définition formelle de l'évaluation multi-facettes.
Celle-ci est donnée sous forme d'une sémantique opérationnelle d'un langage
proche du lambda-calcul: \lambda^{facet}.  La sémantique de ce langage est d'abord donnée
sans considérer l'évaluation multi-facettes, et suit une définition usuelle d'un
lambda-calcul en /call-by-value/; \lambda^{facet} contient en plus des constantes, des
références mutables, et une valeur absorbante pour faire écho au ~undefined~ de
JavaScript.

Dans un second temps, une sémantique alternative est présentée qui introduit les
changements nécessaires pour l'évaluation multi-facettes.  Il s'agit d'une copie
de la première sémantique, avec quelques changements et ajouts.  On y retrouve
les deux des quatre catégories de changements dégagées du diff.  Le /program
counter/ accompagne chaque règle d'évaluation, et de nouvelles règles sont
ajoutées pour séparer l'évaluation de valeurs à facette en deux parties.

Les règles de la sémantique instrumentées tiennent sur une page; un
interpréteur pour cette sémantique est donc considérablement plus petit qu'un
interpréteur JavaScript complet, ce qui en fait un excellent choix pour tester
des mécanismes d'extensibilité.

** BARE Formalisation
Spec -change-> Spec'

A change adds a variation to the Spec of the program.  If we have a formal spec,
we can apply this change on it and have a changed spec.  Change is the delta
between the two specs.

The program is implemented.  We have some code that generates a runtime process
that behaves according to the spec.

Spec -> Code -> Program -> Spec

The change causes a change in Code.  Interestingly, a small change in Spec can
have a large impact on Code.

One the goal of the programmer is to make sure the program is correct: acts
according to the spec.  The programmer writes Code, to create Program.  When a
change of Spec arises, it's best if the change to Code is self-contained.

The question is then: how to minimize the impact of change?  How to construct
Code to minimize impact?  What patterns to use?  Do languages make a difference?

Spec changes can be anticipated, or not.  Anticipated Spec changes can be
factored into Code design.  Unanticipated Spec changes cannot; they might have a
small or large impact, depending.

We are not tacking just any Program or Spec, but interpreters.  And a specific
kind of change: adding analyses.  How is that more specific?

Some solutions will be specific to that use case.  Some will apply to any
program.  Lessons learned will be valuable for all.

** Le problème de l'expression
Comment étendre un interpréteur n'est pas un nouveau problème.  Wadler [[cite:Wad-98][Wad-98]]
nomme un cas spécifique d'extension qui s'applique aux langages fonctionnels:

#+BEGIN_aside
Les deux contraintes de recompilation et de typage statique ne sont pas
nécessaires pour rendre le problème intéressant.  Elle le rendent certes plus
difficile, mais l'extension d'interpréteur est avant tout un problème
d'extensibilité, et s'applique aussi bien à des langages interprétés (non
compilés), et typés dynamiquement.
#+END_aside

#+BEGIN_QUOTE
The Expression Problem is a new name for an old problem.  The goal is to define
a datatype by cases, where one can add new cases to the datatype and new
functions over the datatype, without recompiling existing code, and while
retaining static type safety (e.g., no casts).
#+END_QUOTE

Dans un langage fonctionnel, une façon courante d'implémenter un interpréteur
est en utilisant un /{{{adt}}}/ pour représenter les termes.  En Haskell par
exemple, on définit un simple langage arithmétique par le type algébrique
suivant:

#+BEGIN_SRC haskell
data Term = Constant Int
          | Plus Term Term
          | Mult Term Term
#+END_SRC

~Term~ est un type qui a trois variantes: un terme est soit une constante
(~Constant~, un entier), une addition (~Plus~), ou une multiplication.

La syntaxe du type algébrique correspond à la façon usuelle de définir des
grammaires: la forme de Backus-Naur (BNF).  Une BNF pour le langage arithmétique
pourrait être:

#+BEGIN_EXAMPLE
<term> ::= <int> | <term> + <term> | <term> * <term>
<int>  ::= 0 | 1 | 2 ...
#+END_EXAMPLE

Ici encore, un terme a trois variantes.  L'addition et la multiplication
contiennent d'autres termes, et la variante constante ne contient qu'un nombre
terminal.  On voit que le type algébrique est suffisamment proche de la BNF pour
rendre la traduction en code Haskell triviale.  Les grammaires pour les langages
de programmation étant souvent définies par une BNF, le type algébrique est la
solution la plus évidente pour les représenter.

~Constant~, ~Plus~ et ~Mult~ sont les /constructeurs/ du type ~Term~, qui
permettent de créer des expressions du langage.  L'expression 2 + 5 * 3 sera
construite par:

: Plus (Constant 2) (Mult (Constant 5) (Constant 3))

#+ATTR_HTML: :class todo
#+BEGIN_aside
AST illustration
#+END_aside

Cette valeur représente l'expression 2 + 5 * 3, et nous donne en même temps un
{{{ast}}} de cette expression.

Pour évaluer l'expression arithmétique, il faut évaluer l'arbre syntaxique et
produire un nombre en retour.  C'est le rôle de l'interpréteur du langage.  Dans
la terminologie de Wadler, l'évaluation est une fonction du le type algébrique;
on parlera plutôt d'/opérations/.

La principale est l'opération d'évaluation qui donne un sens à chaque terme du
langage.  Par exemple, on souhaite évaluer les expressions de notre langage
arithmétique en suivant le sens usuel de l'addition et de la multiplication
d'entiers.  C'est à dire, pour évaluer ~Plus t1 t2~, on évalue ~t1~ et ~t2~ pour
obtenir des entiers, puis on les additionne.  En Haskell, on écrira l'évaluation
en traitant les trois variantes séparément, par /pattern matching/:

#+BEGIN_SRC haskell
eval :: Term -> Int
eval (Constant n) = n
eval (Plus t1 t2) = eval t1 + eval t2
eval (Mult t1 t2) = eval t1 * eval t2
#+END_SRC

La première ligne donne le type de l'opération: elle consomme un terme pour
produire un entier.  Chaque variante de terme est traitée explicitement sur
chaque ligne.  On remarque que l'évaluation est récursive pour ~Plus~ et
~Mult~.  La définition du type algébrique l'était déjà: ~Term~ est utilisé dans
sa propre définition.  Les grammaires de langage sont souvent récursives, pour
permettre la construction de grandes expressions à partir de simples éléments.  Même
un langage aussi simple que celui-ci exhibe cette récursion.

On peut maintenant évaluer l'expression 2 + 5 * 3:

: > eval Plus (Constant 2) (Mult (Constant 5) (Constant 3))
: 17

Un interpréteur peut définir plusieurs opérations.  Wadler prend l'exemple d'un
/pretty-printer/, une opération d'affichage des arbres syntaxiques dont la
sortie est plus amicale pour l'œil humain.  Typiquement, le pretty printer d'un
langage transforme l'arbre syntaxique en code source que le programmeur a
l'habitude de lire.  Appliqué aux expressions arithmétiques, il faut transformer
le ~Plus~, qui est préfixe, en ~+~ infixe, et faire disparaître le mot ~Constant~
qui n'apporte pas d'information capitale.

#+BEGIN_SRC haskell
pp :: Term -> String
pp (Constant n) = show n
pp (Plus t1 t2) = pp t1 ++ " + " ++ pp t2
pp (Mult t1 t2) = pp t1 ++ " * " ++ pp t2
#+END_SRC

#+BEGIN_aside
On peut voir le pretty printer comme la réciproque de l'analyseur syntaxique.
#+END_aside

Si on affiche ainsi l'arbre syntaxique de l'expression 2 + 5 * 3, on obtient:

: > pp Plus (Constant 2) (Mult (Constant 5) (Constant 3))
: 2 + 5 * 3

L'implémentation de l'affichage a la même structure que de l'implémentation de
l'évaluation.  On traite les trois variantes du type algébrique dans trois
lignes différentes.  Les deux variantes ~Plus~ et ~Mult~ sont définies
récursivement, mais pas la variante ~Constant~, qui est un terminal dans la
grammaire.  L'opération traite l'arbre syntaxique en partant de la racine, puis
s'applique récursivement sur les nœuds de l'arbre jusqu'aux feuilles.  Ce
processus, commun aux interpréteurs, est parfois appelé une /descente
récursive/.

Dans le problème de l'expression, on cherche à étendre le langage et son
interpréteur de deux façons: en rajoutant des nouveaux termes au langage, et en
rajoutant de nouvelles opérations.

Avec ~pp~, on a déjà vu comment ajouter une nouvelle opération à l'interpréteur:
il suffit de définir son comportement pour toutes les variantes des termes du
langage.  ~Term~ a 3 variantes, donc ~pp~ définit 3 cas.  Notons aussi que pour
ajouter l'opération il suffit d'/ajouter/ du code à l'interpréteur.  Aucune
modification au code existant n'est requise.  C'est une distinction importante,
car il est souvent plus facile d'ajouter du code que de modifier du code
existant.

On peut caractériser la complexité d'un changement de l'interpréteur comme la
complexité d'un algorithme.  L'opération ~pp~ comporte autant de ligne qu'il y a
de variantes de ~Term~, plus une pour sa signature.  S'il y a /v/ variantes, une
opération aura une complexité de l'ordre de /v/ lignes de code (O(v)).  On note
que ces lignes sont des additions par un '+': +O(v).

#+BEGIN_aside
On notera les lignes qui modifient une définition existante par '~', et les
lignes supprimées par un '-'.
#+END_aside

Étendre le langage demande de modifier le code existant.  Si on veut rajouter un
terme pour la soustraction, il faut commencer par étendre le type algébrique
~Term~:

#+BEGIN_SRC haskell
data Term = Constant Int
          | Plus Term Term
          | Mult Term Term
~         | Minus Term Term
#+END_SRC

C'est une ligne de code ajoutée.  Mais c'est surtout une /redéfinition/ du type
algébrique.  Typiquement, ajouter une variante induit une recompilation.  On
compte donc cette ligne comme une modification, et non un simple ajout.

Mais il faut aussi traiter cette nouvelle variante dans les opérations
existantes:

#+BEGIN_side-figure
#+CAPTION: Dans cette décomposition, les opérations implémentent du code pour
#+CAPTION: chaque variante du langage (sens des flêches).  Ajouter une opération
#+CAPTION: (à gauche) n'affecte pas les unités existantes, mais ajouter un terme
#+CAPTION: (à droite) nécessite de modifier toutes les opérations.
[[file:img/expr-pb-1.svg]]
#+END_side-figure

#+BEGIN_SRC haskell
  eval :: Term -> Int
~ eval (Minus t1 t2) = eval t1 - eval t2

  pp :: Term -> String
~ pp (Minus t1 t2) = pp t1 ++ " - " ++ pp t2
#+END_SRC

Au total, si on a ~p~ opérations dans l'interpréteur, le changement a une
complexité de ~O(p).

Les deux changements sont linéaires en lignes de code ajoutées.  Mais ajouter
une opération et ajouter un terme ne sont pas des changements similaires pour
autant.  Ajouter un terme demande de /modifier/ des définitions existantes dans
l'interpréteur, alors que pour ajouter une opération il suffit d'adjoindre une
nouvelle fonction.

Le problème de l'expression touche à la fois à la modularité et à
l'extensibilité.  À l'extensibilité, parce qu'on cherche à étendre
l'interpréteur.  Et à la modularité, parce que les termes et les opérations
peuvent être considérés comme des /unités/ indépendantes, des modules.  Pour
respecter la modularité, on cherche à ce que chaque extension soit restreinte à
un module.  Dans l'exemple en Haskell, ajouter une opération revient à ajouter
un module sans affecter les autres, mais ajouter un terme requiert de modifier
plusieurs modules.

#+BEGIN_side-figure
#+CAPTION: Dans cette décomposition, les termes contiennent le code des
#+CAPTION: opérations.  Ajouter un terme n'affecte pas les unités existantes,
#+CAPTION: mais ajouter une opération nécessite de modifier tous les termes.
[[file:img/expr-pb-0.svg]]
#+END_side-figure

Le problème n'est pas exclusif à Haskell.  Dans [[#hyperj]], on a vu exactement la
situation inverse en Java.  Là aussi on définissait un interpréteur, mais cette
fois chaque terme était représenté par un objet qui encapsulait les définitions
des opérations pour ce terme.  En conséquence, ajouter un terme correspondait à
ajouter un objet, donc ajouter un module.  Mais pour ajouter une opération il
faillait modifier tous les objets des termes existants.  Le choix de l'unité
principale de décomposition (termes ou opérations), favorise l'un ou l'autre
type d'extension.

#+ATTR_HTML: :style margin-top:4rem
#+BEGIN_side-figure
#+CAPTION: La décomposition est un choix d'implémentation; elle n'est pas
#+CAPTION: imposée par le langage.  Ajouter une opération ou ajouter un terme
#+CAPTION: revient à définir une arête dans le graphe ({{{color(c0)}}}).
#+CAPTION: L'implémentation détermine juste comment cette arête est définie dans
#+CAPTION: le programme.
[[file:img/expr-pb-2.svg]]
#+END_side-figure

Le langage de programmation /favorise/ une décomposition, mais il n'empêche pas
l'autre.  Canoniquement, la décomposition par opérations est plus évidente en
Haskell (et autres langages de la famille de ML), tandis que la décomposition
par termes est plus évidente en Java (et autres langages objets).  Mais, suivant
la façon dont l'interpréteur est construit, un langage peut supporter l'une ou
l'autre décomposition, voire les deux.  Il suffit de voir que les nombreuses
solutions proposées au problème de l'expression nécessitent rarement un nouveau
langage, mais exploitent plutôt les mécanismes existants d'une nouvelle façon.

Et ce sont ces différents mécanismes qui nous intéressent.  Les mécanismes qui
aident à résoudre le problème de l'expression sont des mécanismes qui /a priori/
facilitent l'extension de l'interpréteur.  Peut-être que certains de ces
mécanismes ne sont utiles que pour les extensions spécifiques à ce problème.
Mais dans l'ensemble, quelques-uns seront applicables à d'autres scénarios
d'extension.

[

- Different from /instrumentation/ which modifies existing terms and operations.

]


* Travaux connexes
** Les mécanismes de recomposition du programme
*** BARE Le problème de l'expression
Wadler, Odersky, Krishnamurthi, Oliveira (expression families) ...

*** BARE Dynamic binding
Introduced by McCarthy's LISP [[cite:McC-60][McC-60]] as a bug.  Can be emulated by passing a
dynamic environment in lexical binding [[cite:Que-03][Que-03]].

Implicit parameters [[cite:LLM+00][LLM+00]] provide dynamic scoping for Haskell (though they lose
their first-class privileges).

[[cite:Mor-98][Mor-98]] gives a syntactic theory of dynamic binding, and prove that dynamic
binding adds expressiveness to a purely functional language.  They give examples
in Perl, TeX, Common Lisp and Bash.

[[cite:Tan-09a][Tan-09a]] generalizes dynamic and static binding by making explicit the two
dimensions of propagation of bindings (call stack and delayed lambdas), and
offering a filter function to toggle the activation of a propagated binding.

Some use-cases are mentioned, but none are demonstrated in the paper.  The
proposal is not motivated enough by concrete applications that would be
difficult to solve using existing mechanisms.  Also, the work is really focused
on the binding semantics of Scheme, which reduce its applicability.

*** BARE Building from modules
Findler & Flatt, Newspeak

*** BARE Building with monads
Wadler, Steele, Spinoza, Swierstra, Rúnar, ...

Free algebras, free monads.  Basically reify data in a way that is accepted by
the type system of the underlying language to allow unanticipated extension.

[[cite:OC-12][OC-12]] gives Java code with generics for solving the expression problem using
/object algebras/.  Object algebras are akin to a free algebra.  Instead of
locking down the actual objects used as expressions too early, they leave them
open using abstract factories.  Providing a factory when evaluating the
expression gives you either integer evaluation, or pretty-printing.

Their solution is applicable to Java with generics, without significant
syntactic overhead (less than related work).  And, they leverage the type system
to capture erroneous composition.

Aspects and monads are sometimes both viewed as mechanisms to achieve modularity
in software [[cite:DBB+03][DBB+03]] [[cite:HO-07][HO-07]] [[cite:Meu-97][Meu-97]].

De Meuter shows we can use a monad to memoize ~fib~, so efficiency concern is
cleanly separated.  But a function is enough to memoize ~fib~, so the example is
not very convincing.  Well, he does not write ~fib~ as a function, but as a
method in a custom OO language embedded in Scheme.  Then, he is just
intercepting the method name in the ~bind~ of the "interpreter".  Which is kinda
like reflection.

Hofer and Ostermann are more on point, noting that common monads correspond to
concerns that can be written as aspects: error handling (Error monad), dynamic
scoping (Reader monad), continuations (Continuation monad).  Their conclusion is
AOP != monad: monads manipulate computation on a finer granularity that AOP
can't do.  On the other hand, AOP is more declarative, and can come in different
degrees of obliviousness, but monads are always explicit.

I think the better angle is to just say "Monads are a mecanism touted as
separating concerns and promoting modularity", and that's enough to consider
them.  Maybe mention that some have related them to aspects, but there is no
sign of equivalence between the mecanisms.

*** BARE Bytecode instrumentation
Ansaloni.  Targets bytecode, which is low-level code.

Jinliner [[cite:TSN+02][TSN+02]] can insert code into the bytecode of a Java program.  Allows to
alter the behavior of a program with no access to its source code.  Inserts code
after/before point of interest.

[[cite:BRG+14][BRG+14]] instruments the bytecode interpreter of WebKit to enable information flow
tracking.  Bytecode instrumentation is difficult, because you lose high-level
details of the source code like "when does an if block ends".  They have to
build a control-flow graph to know when to discard program counters used by the
information flow analysis.  Also, instrumenting the bytecode is specific to the
bytecode compiler of WebKit (there is no standard, unlike Java).

*** Superimposition
**** BARE Caesar
Caesar has everything and the kitchen sink.
[[cite:AGM+06][AGM+06]]

CaesarJ regroups virtual classes, mixins, pointcut-advice and binding classes.
All these mechanisms are brought together to allow composition along many axis.

But overall, I failed to see the problems that it solved.  Both papers [[cite:AGM+06][AGM+06]]
[[cite:MO-02][MO-02]] are dense and opaque; the examples are too complicated to make sense of
the benefit brought by the new mechanisms.

**** TRANSLATE Software product lines
[[cite:ABK+13][ABK+13]] provides a well-rounded survey of the field.

An engineering methodology to create and maintain variants of a software
product, with optional features (analogy with car assembly lines, which allow
for adding optional features while reusing the same assembly process).

Inspired by the similar evolution in the mass production of consumer goods.
From handcrafting to mass production, to mass customization: product lines that
cover a spectrum of variations.  Examples abound: cars, multi-flavored
detergent, phones, Subway sandwiches ...  Software product lines are the
realization of mass customization for software products (yeah!).

A product line engineering platform combines all the artifacts, documentation
and methodologies of a family of products.  The goal of PLE is to manage the
/commonality/ and /variability/ of a product family.  PLE is not specific to
software.

Properties of a SPL:
- binding time (composition can happen at compile-time, load-time or run-time)
- language solution vs. tool based
- annotation (think C preprocessor) vs. composition (features in their own unit)
- preplanning effort (can you add features without designing for it?)
- feature traceability (mapping between feature model to solution space)
- separation of concerns
- information hiding
- uniformity

Software product lines mechanisms include:
- global parameters
- design patterns (observer, strategy, decorator)
- frameworks
- components

Using version control branches to manage variability is also discussed.  Each
branch correspond to a product, and code sharing is provided by the version
control tool.  However, version control manages /products/ rather than
/features/.  Features are not apparent independently of the base code, except
when looking at diffs.

Feature-oriented programming allows the decomposition of a program into features
first.  Jak is a Java extension that supports FOP [[cite:BSR-04][BSR-04]].  A feature corresponds
to a layer, and each layer can contain multiple classes that implement the
feature.  Further layers can /refine/ the classes of previous layer, and refer
to their implementation via the =original= keyword.

FeatureHouse [[cite:AKL-13][AKL-13]] is akin to [[Semantic patches]], in that it uses a reduced
syntax tree in order to transform code.  One writes a base program, then another
program can be superimposed on it by matching their reduced syntax trees.  The
base program code can be called using the =original= keyword.  Three-way merges
are also possible, and resolved like in version control systems.  The model of
reduced syntax trees of FeatureHouse is language independent, as are the
composition mechanisms.  Language plugins can be written to tell
FeatureHouse how to generate, compose, and pretty-print reduced syntax trees.

#+BEGIN_EXAMPLE
public class A {
  private int foo() { return 0; }
}

public class A {
  private int foo() { original(); return 1; }
}
#+END_EXAMPLE

FeatureHouse also supports quantification.  Mixins and traits mechanisms are
essentially instances of superimposition.

FOP is well-suited to implementing /heterogeneous concerns/ (one variation per
join point), while AOP is better for /homogeneous concerns/ (one variation,
multiple join points). [[cite:MO-04][MO-04]] illustrates the compromises of each approaches (and
presents [[Caesar]] as the superior solution).

If you cannot maintain a separation of concerns in the code itself, you can
emulate it through views.  /Virtual separation of concerns/ is using tools to
provide coherent views of features that are scattered in the code [[cite:AK-09][AK-09]].

Virtual separation of concerns has few downsides and many benefits: simplicity
and flexibility being the chief advantages.

Handling feature interactions is an open problem.  Detecting them also.

***** Mechanisms for instrumentation
FOP implementations presented here are static organization of code into
features.  Much like design patterns or frameworks, they require the programmer
to design for extensibility beforehand.  AspectJ allows extending an existing
code base (unlike the original AOP vision, which emphasized the design decision
of separating components from aspects).

The notion of superimposition is nice.  Recognizing that inheritance, mixins and
traits are all instances of superimposition is a powerful insight.

Virtual separation of concerns makes some good points.  If the primary
decomposition is tyranny, then we have no hope of organizing the physical code
into features.  However, we can leverage editing tools to re-arrange and view
the code in any way we like.  One physical representation, many views.  Each
view can provide different information about the system.

The motivation behind all such mechanisms is a desire to organize snippets of
code, to structure modules, and avoid repetitions.  The ultimate conclusion of
that trend is a language-agnostic manipulation syntax based on hypertext.  Each
snippet has a name, and tags (for marking membership of a feature, but mostly
for non-hierarchical organization).  Any snippets can be referenced by another
(for documentation), and can be included for execution.  Snippets can be
referenced to by name, or by tags.  Tags and wildcards allow quantification.

Tags also allow to view the program through different lenses.  Snippets can have
parameters, hence are a form a macros.

Links are two way, and kept in sync by the programming system (editor): this
prevents obliviousness.

**** TRANSLATE Semantic patches
[[cite:PLM-07][PLM-07]].  A solution to /collateral evolution/.  When a library function changes
name, or gains an argument, client code must makes the necessary changes.  The
changes in client code are collateral.

In a semantic patch, one describes the pattern of collateral changes needed to
adapt client code.

#+BEGIN_EXAMPLE
@ rule2 @
identifier proc_info_func;
identifier hostptr;
@@
proc_info_func (
+ struct Scsi_Host *hostptr,
- int hostno
) {
  ...
- struct Scsi_Host *hostptr;
  ...
- hostptr = scri_host_hn_get(hostno);
  ...
- if (!hostptr) { ... return ...; }
  ...
- scsi_host_put(hostptr);
  ...
}
#+END_EXAMPLE

Identifiers are declared in the header with a syntactic class.  They are matched
in the target code according to the context where they appear in the body of the
semantic path.

The dots =...= are an operator to match any sequence of code between two lines.
There is a mention of the dots matching the /control-flow/ of the code, though
nothing indicates that =spatch= interprets the target code in any way.

[[cite:JH-07][JH-07]] demystifies the tool by giving a denotational semantics.  Indeed, the dots
only match the syntax.

The related work section of [[cite:JH-07][JH-07]] has a few surveys on software evolution, and
in particular the Journal of Software Maintenance and Evolution.

All around a nice idea, though you still have to write the semantic patches from
scratch for every change.

The (unintended) idea of source transformation based on dynamic control flow is
interesting.  See [[file:notes.org::*Shapes%20of%20computation][Shapes of computation]].

***** Mechanisms for extension
It's another approach, transforming code to alleviate the maintenance cost.

However, it's a crutch.  We would prefer not having to have to make those
changes in the first place, even if the kernel libraries are updated.

The concept of /collateral evolution/ is certainly related.  When interpreters
evolve, collateral changes are needed on the analyses.  Previous work [[cite:PLM-06][PLM-06]] was
more focused on introducing the collateral evolution problem, with plenty of
examples from the Linux kernel.

** UNSTABLE Travaux connexes concernant l'instrumentation
*** BARE Domain-specific languages
Greater control for language designer.  Gives a constrained playground for
programmers.

Downsides include tooling, development time, unfamiliarity and competition with
general-purposes languages.

Monads can be seen as DSLs (but this is an insight better saved for later).

*** BARE Scripting languages
Tcl 1988, Python 1991, Lua 1993, VBA 1993, JS 1995.  Scripting languages are an
early ‘90s phenomenon.  Dealing with low-level languages was deemed too heavy,
but writing your whole system in a high-level language was too costly.  The
compromise was to write the kernel in C, and the rest in a scripting language.

With sufficiently efficient high-level languages, the kernel+configuration
approach might be unneeded.

JavaScript being a scripting language for the browser, as well as an object used
in the Core, it might be adequate to have a dedicated background section to it.

*** TRANSLATE Emacs
See Emacs Manual, [[cite:Sta-81][Sta-81]], [[cite:Hal-88][Hal-88]].  Emacs is an example of an extensible system.
The mechanisms: global namespace, dynamic scoping, and a simple aspect system.

In [[cite:Sta-81][Sta-81]], it is said that the TECO language was instrumental for the
extensibility of the EMACS system.  An interpreter should be available all the
time, and compiled languages often lack this functionality.

#+BEGIN_QUOTE
A system written in PL/I or PASCAL can be modified and recompiled, but such an
extension becomes a separate version of the entire program. The user must
choose, before invoking the program, which version he wants. Combining two
independent extensions requires comparing and merging the source files.  These
obstacles usually suffice to discourage all extension.
#+END_QUOTE

Especially they list "Language features for extensibility":
1. Global variables.  They can be queried, referred to, and redefined.
2. [[Dynamic binding]].  Useful for redefining binding on the fly.
3. File-local variables.  Good for customization, but really they give a
   file-local value for a global variable.
4. Hooks.  They give points in the control flow to insert extension code.
   Especially when redefining assembly or C functions, which cannot be
   reinterpreted.
5. Error handling.  Throwing the debugger helps discover and recover from
   unexpected situations.
6. Non-local transfers.  Gives an example to exit an infinite loop.


In the related work, Multics EMACS [[cite:Gre-80][Gre-80]] is mentioned as being more flexible,
as it is written in MacLisp directly.  Smalltalk [[cite:Ing-78][Ing-78]] is also said to be
"oriented toward writing extensible programs".

(The Augment editor demoed by Engelbart [[cite:EE-68][EE-68]] is also mentioned, though nothing
is said of its extensibility.)

[[cite:NS-01][NS-01]] proposes a dynamic scope analysis, to translate Emacs Lisp code using
dynamic binding to lexical binding.

****  Mechanisms for extension
Global variables, dynamic binding, hooks.

Though hooks are more a convention than a first-class mechanism.

*** TRANSLATE Eclipse and other IDEs
As noted by [[cite:Ler-11][Ler-11]], the Eclipse platform is extensible, and built using
plugins.  Each plugin states its dependencies (the hooks needed to function),
and its extension points (for other plugins).

Eclipse plugins are compiled, though they can be loaded dynamically (if they are
written properly).  Symptomatically of Java, writing plugins needs lot of
boilerplate code and XML (which Eclipse can generate for you, I understand).

**** Mechanisms for extension
The mechanisms for extension seems to revolve around the observer pattern: a
host plugin raises events which can be intercepted by extensions [[cite:Bol-03][Bol-03]].

So, a lot of convention.

*** BARE Web browsers
Many extensions are written for web browsers.  The mechanisms are heavy,
comparable to the effort of writing an Eclipse plug-in.

In fact, ZaphodFacets was an extension to change the JavaScript interpreter used
by the browser.

**** Mechanisms for extension
Convention.  Write manifest, and define the agreed-upon functions (install,
startup).

*** TRANSLATE Lua
An extensible extension language [[cite:IFF-96][IFF-96]].

Extensible systems comprise of a kernel and a configuration.  The kernel is the
core of the system, the parts that cannot change, and is usually compiled for
speed and efficiency.  The configuration part is written in an interpreted,
flexible language, which can interact with the kernel.

Another take, in the conclusion, is that the kernel is a virtual machine for
programs written in the configuration language.

Note that if performance can suffer, writing the whole system as a configuration
gives even greater flexibility.

Configuration languages can be simple: .ini files, X11 resource files, but they
can have more features (scripting languages).  Also called /extension
languages/.

Five requirements for extension languages:
1. good data structures (key-value maps for configuration)
2. simple syntax for amateur programmers
3. lightweight
4. not static type checking or exception handling, as only small programs are
   written in them
5. should be extensible

Requirement 4 is actually an absence of requirement.  Unfortunately, people
/will/ write large systems in it, especially if the language is easy to pick up.
Arguably, the cost of such features may conflict with requirement 3.  Otherwise,
this list looks more like a checklist for Lua.

On a related note, [[cite:Bla-82][Bla-82]] devotes a whole thesis against exceptions.

Extension programs have no =main=.

Associative arrays are a powerful data structure which make plenty of algorithms
trivial (free hashtables), and more efficient to implement than lists.

Amusingly, the associative array syntax was inspired by BibTeX.

Associative arrays + first-class functions = classes.

No error handling, but errors can be raised.  To catch them, we can define
/fallback/ functions.

Setting a fallback on the "index" event allows to define a custom delegation
mechanism between tables.

Compared to Lisp, Lua is portable and has easier syntax.  Tcl is slow and has
strange syntax.  Python is not embeddable, and is already too complex (modules
and exception handling).

At the time of writing, Lua is 20 times slower than C (this factor is said to be
"typical for interpreted languages", and cites "Java, The Language" for this
assertion).

The latest numbers on the [[http://benchmarksgame.alioth.debian.org][benchmarks game]] show Lua being 5 to 79 times slower,
while consuming more memory.

In the conclusion, they allude at extending web browsers with Lua.  A follow-up
seems to be [[cite:HBI-98][HBI-98]], which proposes Lua as a target for CGI on web servers.

[[cite:IFF-07][IFF-07]] goes over the history of Lua, up to version 5.1 released in 2006.

One tenet of Lua is "Mechanisms, not policy": provide language mechanisms and
let programmers code the way they want to with them.  An example is message
dispatch: rather than using a class construct, Lua programmers can use fallbacks.

Though they regret not stating a policy when it comes to modules, since everyone
is doing its thing, without agreeing on a common protocol.

****  Mechanisms for extension
The kernel+configuration, as seen in EMACS.  Mechanisms over policy shares our
philosophy and provides programmers with tools to solve their problems in their
own way.

To extend Lua, bindings from C can be added, and custom data structure as well.
Changing the interpreter does not seem possible, even from C.

* UNSTABLE Variations sur un interpréteur de lambda-calcul extensible
:PROPERTIES:
:CUSTOM_ID: variations
:END:
Essayons d'abord d'écrire un interpréteur pour chaque sémantique de \lambda^{facet}, sans
anticiper l'instrumentation; c'est à dire sans se préoccuper de pouvoir étendre
l'interpréteur facilement.

Nous appellerons \lambda^{standard} le langage décrit par la première sémantique d'Austin
et Flanagan, qui correspond à un lambda-calcul avec références, et \lambda^{facet} le
langage étendu pour l'évaluation multi-facettes.

Puisque Narcissus est en JavaScript, et suit le patron /module/, écrivons un
interpréteur de \lambda^{standard} dans le même style.

# From [[file:~/Archim%C3%A8de/Th%C3%A8se/lab/lamfa/js/lab/lamfa-narcissus-style.org][lamfa/js/lamfa-narcissus-style.org]]
#+BEGIN_SRC js
var interpreter = (function(){
  var bottom = {...}
  function Address() {...}
  function FunctionObject() {...}

  function ExecutionContext(parent) {
    ...
    this.scope = {}
    this.store = new Store()
  }

  function execute(node, context) {
    switch (node.type) {
    case 'CONST':  ...
    case 'VAR':    ...
    case 'FUN':    ...
    case 'APP':    ...
    ...
    }
  }

  return {
    run: run,
  }
}())
#+END_SRC

Ce simple interpréteur reflète la structure de Narcissus, mais est bien plus
succinct.  Comme dans Narcissus, le gros de la logique de l'interpréteur réside
dans la fonction ~execute~.  Les définitions qui précèdent réifient des objets
de la spécification: ~Address~, ~bottom~, ~FunctionObject~.  On réutilise le nom
~ExecutionContext~ pour indiquer l'objet qui contient l'environnement de
variable et qui est passé au fil des appels récursifs à ~execute~.  En bas ce
sont les fonctions exportées par le module.

On peut maintenant modifier cet interpréteur pour implémenter la seconde
sémantique de \lambda^{facet}.  Comme pour l'instrumentation de Narcissus, on part de
l'interpréteur du langage standard, qu'on modifie par endroits suivant les
besoins de la nouvelle sémantique.  L'idée ici n'est pas de construire un
interpréteur extensible, mais d'identifier les points qui vont varier en
définissant cette variation.  Pour cela, on s'intéresse surtout aux différences
entre les deux versions.

#+CAPTION: Diff simplifié qui illustre les modifications nécessaires pour
#+CAPTION: l'interpréteur \lambda^{facet}.
#+BEGIN_SRC diff
+ function Facet() {...}

+ function evaluateEach() {...}

  function ExecutionContext() {
+   this.pc = []
  }

  function execute(...) {
    ...
    case CALL:
-     v = f._call(a, context)
+     v = evaluateEach(f, context, (f, x) => f._call(a, x))

    case REF:
+     v = constructFacet(context.pc, v, bottom)

    case DEREF:
-     v = a.deref(context)
+     v = evaluateEach(a, context, (a, x) => a.deref(x))
  ...
  }

  return {
    ...
+   runWithPC: runWithPC,
  }
#+END_SRC

Il y a moins de différences que pour l'instrumentation de Narcissus, mais on
retrouve trois des mêmes catégories.  D'abord, il y a de nouvelles définitions
pour les valeurs à facettes et leur évaluation (~Facet~ et ~evaluateEach~).  La
fonction ~evaluateEach~ est utilisée, comme dans l'instrumentation de Narcissus,
pour évaluer les deux parties d'une valeur à facettes récursivement.  Le program
counter est présent, et rajouté à l'objet ~ExecutionContext~ pour pouvoir être
utilisé dans ~execute~.  Enfin, une nouvelle fonction est exportée par le
module.  La seule catégorie non représentée est l'extension de l'objet client
global, car il n'y a pas d'équivalent à l'objet global dans \lambda^{facet}.

Puisqu'on retrouve les mêmes catégories de changement sur cet exemple plus
restreint, on peut supposer que si l'on trouve des moyens d'instrumenter
l'interpréteur de \lambda^{standard} pour l'évaluation à facettes sans duplication de
code, ces moyens seront applicables à Narcissus également.

Il est clair que les choix d'implémentation de l'interpréteur standard sont la
cause de la duplication de code dans l'instrumentation.  Si l'interpréteur
possédait une interface pour être étendu, nous n'aurions pas besoin de dupliquer
le code pour changer ces quelques lignes.  Comment aurions nous dû /construire/
l'interpréteur pour que l'extension requise par \lambda^{facet} puisse être écrite en
minimisant le code dupliqué ?  Ou encore, est-il possible de /modifier/
l'interpréteur standard pour le rendre extensible ?  Ce sont les deux questions
que nous explorons par la suite.

* TRANSLATE Construire un interpréteur extensible
Le style de l'interpréteur du chapitre précédent n'est pas extensible.
L'interpréteur suit le patron module en JavaScript, qui a pour intention de
verrouiller les définitions du module contre toute extension future; qu'elles
soient intentionnelles ou accidentelles (le chapitre ?? décrit en détail le
fonctionnement de ce patron).

Mais ce n'est pas la seule façon d'écrire l'interpréteur en JavaScript.  On peut
très bien suivre un style inspiré de la programmation objet.  Le patron
/interpréteur/ [[cite:GHJ+94][GHJ+94]] est d'ailleurs un bon candidat: chaque nœud de l'arbre de
syntaxe est un objet, et contient une méthode ~interpret~.  Le ~switch~ de la
fonction ~execute~ de Narcissus est donc séparé en morceaux indépendants: chaque
nœud possède le code qui permet d'évaluer sa valeurs et ses effets.

Écrivons un interpréteur pour \lambda^{standard} dans ce style.  D'abord, les objets qui
réifient les nœuds de l'AST:

#+BEGIN_SRC js
var stdInterp = {
  CONST: {
    new(e) {...},
    execute(context) {...}},

  VAR: {
    new(e) {...},
    execute(context) {...}},

  FUN: {
    new(argName, body) {...},
    execute(context) {...}},

  APP: {
    new(fun, arg) {...},
    execute(context) {...}},
#+END_SRC

Chaque objet a deux méthodes: une pour l'instancier, et une pour évaluer le nœud
de l'AST.  Les méthodes ~execute~ contiennent la même logique que les ~case~ du
~switch~ de l'interpréteur dans le style de Narcissus.

Les objets de l'exécution subissent la même transformation:

#+BEGIN_SRC js
  Address: {
    new(a) {...},
    deref(context) {...},
  },

  ExecutionContext: {
    new(scope, store) {...},
  },

  Store: {
    new() {...},
    add(value) {...},
    retrieve(addr) {...},
  },

  FunctionObject: {
    new(node, scope) {...},
    _call(arg, x) {...},
  },
#+END_SRC

Et enfin, la fonction d'entrée de l'interpréteur, ~run~, peut simplement appeler
la méthode ~execute~ sur la racine de l'AST.

#+BEGIN_SRC js
  run(node) {
    return node.execute(this.ExecutionContext.new())
  },
})
#+END_SRC

L'intérêt de cette décomposition c'est qu'on peut maintenant facilement créer
une variante de l'interpréteur grâce à la délégation par prototype de
JavaScript:

#+BEGIN_SRC js
var fctInterp = derive(stdInterp, {
  APP: derive(stdInterp.APP, {
    execute(context) {
      ...
      return evaluateEach(...) }})

  REF: derive(stdInterp.REF, {
    execute(context) {
      ...
      return constructFacet(...) }}),

  DEREF: derive(stdInterp.DEREF, {
    execute(context) {
      return evaluateEach(...) }}),

  ExecutionContext: derive(stdInterp.ExecutionContext, {
    new(scope, store, pc) {...}
  }),

  runWithPC(node, pc) {...},
})
#+END_SRC

Ici, ~fctInterp~ a pour prototype ~stdInterp~, donc toute propriété non présente
sur ~fctInterp~ sera prise de ~stdInterp~.  Inversement, la propriété ~APP~ est
définie sur ~fctInterp~, donc elle "surcharge" la propriété ~APP~ de
~stdInterp~.  La délégation par prototype est un mécanisme bien adapté à notre
problème, car on souhaite que le code de ~fctInterp~ exprime la /différence/ de
l'évaluation de \lambda^{facet}, et seulement cette différence.  Dans un langage à
prototype, un objet n'est pas instancié à partir d'une classe, mais /dérivé/
d'un prototype.  Un objet dérivé n'a besoin que de définir en quoi il diffère de
son prototype.  C'est donc un mécanisme de langage idéal pour notre scénario
avec deux interpréteurs.

#+NAME: lamfa-proto
#+CAPTION: Définition d'un interpréteur en utilisant la délégation par
#+CAPTION: prototype.  L'objet ~fctInterp~ possède une unique propriété propre:
#+CAPTION: ~APP~.  Les autres propriétés sont déléguées à son prototype,
#+CAPTION: ~stdInterp~.
[[file:img/lamfa-proto.svg]]

L'interpréteur de \lambda^{facet} peut donc être défini uniquement en codant les
différences entre ces deux modes d'interprétation.  Les deux objets ~stdInterp~
et ~fctInterp~ coexistent à l'exécution, ce qui permet de pouvoir choisir entre
l'une ou l'autre interprétation pour un même programme.

# Pas si simple, subtilités de références d'objets imbriqués


# Créer de nouveaux interpréteurs à l'exécution.  Pas modifier la même instance
# d'interpréteur.

*** Finding a core example
Looking at the operational semantics for faceted evaluation, we can
see the patterns mentioned previously (=pc= parameter, new cases for
=FacetedValues=).  They are based on a lambda calculus variant, with
references and a "Bottom" value.  Let's try to write an interpreter
for this lambda calculus without anticipating the later
instrumentation.

We'll drop the read/write rules since they only add noise to this
example.  We'll also leave out error handling.

[[file:js/lab/lamfa-es6-standard.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, node) {
    return rules[node.type](σ, θ, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, v1, v2) {
    return application_rules[v1.type](σ, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, e);
    },
  };

  function eval_deref(σ, v) {
    return deref_rules[v.type](σ, v);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}) {
      return σ[a];
    },
  };

  function eval_assign(σ, v1, v2) {
    return assign_rules[v1.type](σ, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = v;
      return σ2;
    },
  };

  let rules = {
    c(σ, θ, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return eval_apply(σ2, v1, v2);
    },

    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = v;
      return [ σ2, address(a) ];
    },

    deref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      return [ σ1, eval_deref(σ1, v) ];
    },

    assign(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return [ eval_assign(σ2, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}) {
    return interpretNode(env, store, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

We used destructuring from ES6 and Unicode identifiers to approximate
the appearance of the big-step semantics.  To effect the operational
rules, we use an ad-hoc pattern matching.  Each AST node is an object
with a =type= field, and the =interpNode= function dispatches to the
function in the =rules= object corresponding to the value of this type
field.  The same pattern matching mechanism is used to distinguish
between an address, a closure or a bottom value.

We can easily instrument this base interpreter by following the
operational semantics from Austin and Flanagan.

[[file:js/lab/lamfa-es6-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, pc, node) {
    return rules[node.type](σ, θ, pc, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, pc, v1, v2) {
    return application_rules[v1.type](σ, pc, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, pc, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, pc, e);
    },

    facet(σ, pc, {k, vh, vl}, v2) {
      if (pc.has(k)) {
        return eval_apply(σ, pc, vh, v2);
      }

      else if (pc.has(-k)) {
        return eval_apply(σ, pc, vl, v2);
      }

      else {
        let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
        let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
        return [ σ2, mk_facet(k, vh1, vl1) ];
      }
    },
  };

  function eval_deref(σ, v, pc) {
    return deref_rules[v.type](σ, v, pc);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}, pc) {
      return σ[a];
    },

    facet(σ, {k, vh, vl}, pc) {
      if (pc.has(k))
        return eval_deref(σ, vh, pc);
      else if (pc.has(-k))
        return eval_deref(σ, vl, pc);
      else
        return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
    },
  };

  function eval_assign(σ, pc, v1, v2) {
    return assign_rules[v1.type](σ, pc, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, pc, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = mk_facet(pc, v, σ[a]);
      return σ2;
    },

    facet(σ, pc, {k, vh, vl}, v) {
      let σ1 = eval_assign(σ, pc.union(k), vh, v);
      return eval_assign(σ1, pc.union(-k), vl, v);
    },
  };

  let rules = {
    c(σ, θ, pc, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, pc, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, pc, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return eval_apply(σ2, pc, v1, v2);
    },

    ref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    },

    deref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      return [ σ1, eval_deref(σ1, v, pc) ];
    },

    assign(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return [ eval_assign(σ2, pc, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return interpretNode(env, store, pc, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );
#+END_SRC

The interesting story is told by looking at the differences between
these two versions.  We see patterns 1 and 2 reappear from our
analysis of the Narcissus instrumentation.  The =pc= parameter must be
passed around in nearly every function, and new cases must be added to
handle facet values.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-es6-standard.js js/lab/lamfa-es6-facets.js; exit 0
#+END_SRC

#+begin_src diff
7,8c7,8
< function interpretNode(σ, θ, node) {
<   return rules[node.type](σ, θ, node);
---
> function interpretNode(σ, θ, pc, node) {
>   return rules[node.type](σ, θ, pc, node);
14a15,28
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
18,19c32,33
< function eval_apply(σ, v1, v2) {
<   return application_rules[v1.type](σ, v1, v2);
---
> function eval_apply(σ, pc, v1, v2) {
>   return application_rules[v1.type](σ, pc, v1, v2);
27c41
<   closure(σ, {x, e, θ}, v) {
---
>   closure(σ, pc, {x, e, θ}, v) {
30c44,60
<     return ↆ(σ, θ1, e);
---
>     return ↆ(σ, θ1, pc, e);
>   },
>
>   facet(σ, pc, {k, vh, vl}, v2) {
>     if (pc.has(k)) {
>       return eval_apply(σ, pc, vh, v2);
>     }
>
>     else if (pc.has(-k)) {
>       return eval_apply(σ, pc, vl, v2);
>     }
>
>     else {
>       let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
>       let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
>       return [ σ2, mk_facet(k, vh1, vl1) ];
>     }
34,35c64,65
< function eval_deref(σ, v) {
<   return deref_rules[v.type](σ, v);
---
> function eval_deref(σ, v, pc) {
>   return deref_rules[v.type](σ, v, pc);
43c73
<   address(σ, {a}) {
---
>   address(σ, {a}, pc) {
45a76,84
>
>   facet(σ, {k, vh, vl}, pc) {
>     if (pc.has(k))
>       return eval_deref(σ, vh, pc);
>     else if (pc.has(-k))
>       return eval_deref(σ, vl, pc);
>     else
>       return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
>   },
48,49c87,88
< function eval_assign(σ, v1, v2) {
<   return assign_rules[v1.type](σ, v1, v2);
---
> function eval_assign(σ, pc, v1, v2) {
>   return assign_rules[v1.type](σ, pc, v1, v2);
57c96
<   address(σ, {a}, v) {
---
>   address(σ, pc, {a}, v) {
59c98
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, σ[a]);
61a101,105
>
>   facet(σ, pc, {k, vh, vl}, v) {
>     let σ1 = eval_assign(σ, pc.union(k), vh, v);
>     return eval_assign(σ1, pc.union(-k), vl, v);
>   },
65c109
<   c(σ, θ, {e}) {
---
>   c(σ, θ, pc, {e}) {
69c113
<   v(σ, θ, {e}) {
---
>   v(σ, θ, pc, {e}) {
73c117
<   fun(σ, θ, {x, e}) {
---
>   fun(σ, θ, pc, {x, e}) {
77,80c121,124
<   app(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return eval_apply(σ2, v1, v2);
---
>   app(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return eval_apply(σ2, pc, v1, v2);
83,84c127,128
<   ref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
---
>   ref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
87c131
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, bottom);
91,93c135,137
<   deref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
<     return [ σ1, eval_deref(σ1, v) ];
---
>   deref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
>     return [ σ1, eval_deref(σ1, v, pc) ];
96,99c140,143
<   assign(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return [ eval_assign(σ2, v1, v2), v2 ];
---
>   assign(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return [ eval_assign(σ2, pc, v1, v2), v2 ];
103,104c147,149
< function interpretProgram(AST, env = {}, store = {}) {
<   return interpretNode(env, store, AST);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return interpretNode(env, store, pc, AST);
117c162,163
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Our ad-hoc pattern matching is perhaps not the most straightforward way to write
such an interpreter in JavaScript.  Another, maybe more familiar way is to use
object-oriented dispatching.

#+BEGIN_aside
And indeed, this form is quite similar to the Interpreter pattern from [[cite:GHJ+94][GHJ+94]].
#+END_aside

[[file:js/lab/lamfa-oo-standard.js]]
#+BEGIN_SRC js
  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return v1.eval_apply(σ2, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        return [ σ1, v.eval_deref(σ1, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return [ v1.eval_assign(σ2, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}) {
    return AST.eval(env, store);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

[[file:js/lab/lamfa-oo-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) {
    return {
      eval_apply(σ, pc, v2) {
        if (pc.has(k)) {
          return vh.eval_apply(σ, pc, v2);
        }

        else if (pc.has(-k)) {
          return vl.eval_apply(σ, pc, v2);
        }

        else {
          let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
          let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
          return [ σ2, mk_facet(k, vh1, vl1) ];
        }
      },

      eval_deref(σ, pc) {
        if (pc.has(k))
          return vh.eval_deref(σ, pc);
        else if (pc.has(-k))
          return vl.eval_deref(σ, pc);
        else
          return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
      },

      eval_assign(σ, pc, v) {
        let σ1 = vh.eval_assign(σ, pc.union(k), v);
        return vl.eval_assign(σ1, pc.union(-k), v);
      }
    };
  }

  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, pc, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1, pc);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return v1.eval_apply(σ2, pc, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom);
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        return [ σ1, v.eval_deref(σ1, pc, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return [ v1.eval_assign(σ2, pc, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return AST.eval(env, store, pc);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );

#+END_SRC

But here again, the pattern appears when looking at the diff.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-oo-standard.js js/lab/lamfa-oo-facets.js; exit 0
#+END_SRC

#+begin_src diff
0a1,53
> Set.prototype.union = function(elem) {
>   let n = new Set(this);
>   n.add(elem);
>   return n;
> }
>
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) {
>   return {
>     eval_apply(σ, pc, v2) {
>       if (pc.has(k)) {
>         return vh.eval_apply(σ, pc, v2);
>       }
>
>       else if (pc.has(-k)) {
>         return vl.eval_apply(σ, pc, v2);
>       }
>
>       else {
>         let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
>         let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
>         return [ σ2, mk_facet(k, vh1, vl1) ];
>       }
>     },
>
>     eval_deref(σ, pc) {
>       if (pc.has(k))
>         return vh.eval_deref(σ, pc);
>       else if (pc.has(-k))
>         return vl.eval_deref(σ, pc);
>       else
>         return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
>     },
>
>     eval_assign(σ, pc, v) {
>       let σ1 = vh.eval_assign(σ, pc.union(k), v);
>       return vl.eval_assign(σ1, pc.union(-k), v);
>     }
>   };
> }
>
31c84
<     eval_apply(σ, v) {
---
>     eval_apply(σ, pc, v) {
34c87
<       return e.eval(σ, θ1);
---
>       return e.eval(σ, θ1, pc);
41c94
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
49c102
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
57c110
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
65,68c118,121
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return v1.eval_apply(σ2, v2);
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return v1.eval_apply(σ2, pc, v2);
75,76c128,129
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
79c132
<       σ2[a] = v;
---
>       σ2[a] = mk_facet(pc, v, bottom);
87,89c140,142
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
<       return [ σ1, v.eval_deref(σ1, v) ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
>       return [ σ1, v.eval_deref(σ1, pc, v) ];
96,99c149,152
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return [ v1.eval_assign(σ2, v2), v2 ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return [ v1.eval_assign(σ2, pc, v2), v2 ];
104,105c157,159
< function interpretProgram(AST, env = {}, store = {}) {
<   return AST.eval(env, store);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return AST.eval(env, store, pc);
111c165,166
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Hence, these patterns appear regardless of the instrumented language
(lambda calculus or JavaScript), and regardless of the language
features used by the implementation (pattern matching or dynamic
dispatch).  Therefore, the problem of finding a way to describe the
instrumentation as a module and minimizing coupling is not specific to
Narcissus or JavaScript interpreters.

We will now exhibit variations in writing the instrumentation as
a separate module with minimal code duplication.

*** The expression problem
First, let's list the additions brought by our extension to the
interpreter:

- we add a new constructor =Couple= to the data type =Term=
- we add a new constructor =Pair= to the data type =Value=
- we add a new case =Couple= to the function =interp=
- we add two new (symmetric) cases =Pair= to the function =plus=

Now, what would a differential description of the extension to our
interpreter look like?

#+BEGIN_SRC haskell
  extend data Term = Couple Int Int
  extend data Value = Pair Int Int

  extend interp (Couple i1 i2) = (Pair i1 i2)
  extend plus (Number a) (Pair i1 i2) = (Pair (a + i1) (a + i2))
  extend plus (Pair i1 i2) (Number a) = (Pair (a + i1) (a + i2))
#+END_SRC

These are all the additions brought by the extension to the original
interpreter.  The new =extend= keyword allows us to:

1. Extend data types with new constructors.
2. Extend function definitions with new cases.

How to extend both data types and functions in a program, without
sacrificing modularity, is a problem known as the /expression problem/
[Wadler].  This (imaginary) =extend= keyword is a solution; there are
real solutions [[[http://www.andres-loeh.de/OpenDatatypes.pdf][Open Data Types and Open Functions]]] (Instead of an
=extend= keyword, they provide an =open= keyword to prefix to initial
declarations of data types and functions.  One could also require both
extended and original codes to include keywords.)

*** The expression problem, with a twist
The expression problem is only concerned with /adding/ data types and
functions, but when we instrument an interpreter, we will often want
to /modify/ its behavior rather than just extend it.

When we take modification into account, what does a differential
description look like?

First, let's go back to the original interpreter, and modify its
behavior.

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Number i)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

Say we need to change the value returned by the interpretation of the
term ‘Constant'.  We want to return a ‘Pair' value instead of a single
number, the second value being a default zero.  We would write the
full version, with replication as:

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int | Pair Int Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Pair i 0)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

What would be the differential description of this change?

#+BEGIN_SRC haskell
  extend data Value = Pair Int Int

  modify interp (Constant i) = (Pair i 0)
#+END_SRC

The keyword =modify= replaces the definition for the targeted case of
=interp=.

[What if we want to combine multiple modifications to the same case
function? A: You'd have to have a clue about the precedence order to
make sense of the result, but these problems are shared by other AOP
applications.]

*** The modular instrumentation problem
We have an interpreter I for a language L, and the source code for I.
We want to instrument the interpreter I, by extending and modifying
its behavior.  Namely, the instrumentation can:
- add terms to the base language
- add values to the base language
- add new operations
- alter the behavior of existing operations, or even suppress them
  entirely.

We constrain the instrumentation by imposing the following
restrictions:

- The instrumented interpreter I' must still be able to execute
  programs written in the language L.  The instrumentation cannot
  remove or modify existing terms of the language.
- The instrumentation must modify only a part of the original
  interpreter operations.  Otherwise, the instrumented interpreter
  may end up with semantics so different from the original interpreter
  that it does not qualify as "instrumentation" anymore; it might as
  well be another interpreter in its own right.

The implementation of this instrumentation will give a new interpreter
I'.  Ideally, this implementation should be as modular as possible; it
should:
- promote isolated reasoning,
- minimize code replication and accidental complexity.  We should be
  able to map the differential description of the instrumentation and
  the code for its implementation.

The /modular instrumentation problem/ is then: how to implement the
instrumentation with the above constraints of modularity?

Note that the changes may bring only additional side effects, and
leave the original behavior unaltered.  How to recognize or enforce
"side-effects only" instrumentation is an interesting
question. ["Recognize" I don't know how.  "Enforce" you can do with
monads, if you have a monadic interpreter.]

** Variations
*** JavaScript
See [[file:js/aoping.org]].

See also [[file:lassy15.org]] for a way to build interpreters incrementally.

**** Split OO-style instrumented interpreter into modules
Let's try to separate the object-oriented instrumented interpreter in
two modules: one for the base interpreter, ‘base.js', and one for the
instrumentation, ‘facets.js'.  The ‘base.js' file should not contain
any instrumentation-specific code, and be as close as possible to the
standard interpreter.

Looking at the diff, we can put =mk_facet=, =facet= and the extension
to =Set.prototype= in a separate file right away.

***** Handling the extra PC parameter
The second, more pervasive change is the addition of the program
counter context in every call.  There are at least two solutions to
this problem:
1. Using a global variable.
2. Using a context object.

****** Using a global variable
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

We can use a global variable for the ‘pc'.  This eliminates the need
to pass the ‘pc' as a formal parameter to most functions, and to pass
it down to tail calls.  However, we then need to save the previous
value of this global pc when temporarily changing the current pc to
evaluate branches in =facet.eval_assign= and =facet.eval_apply=.

With the ‘pc' argument:
#+BEGIN_SRC js
  let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
  let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
#+END_SRC

With a global variable ‘pc':
#+BEGIN_SRC js
  let pc_old = pc;
  pc = pc.union(k);
  let [σ1, vh1] = vh.eval_apply(σ, v2);
  pc = pc.union(-k);
  let [σ2, vl1] = vl.eval_apply(σ1, v2);
  pc = pc_old;
#+END_SRC

This temporary rewriting of a variable is essentially emulating a
dynamic binding of the ‘pc' variable.  There are no built-ins
constructs or syntactic sugar for dynamic binding in JavaScript, but
we can add one for this particular variable.

#+BEGIN_SRC js
  function with_pc(new_pc, thunk) {
    let old_pc = pc;
    pc = new_pc;
    let ret = thunk();
    pc = old_pc;
    return ret;
  }
#+END_SRC

Now the =facet.eval_apply= function is cleaner:

#+BEGIN_SRC js
  let [σ1, vh1] = with_pc(pc.union(k), () => vh.eval_apply(σ, v2));
  let [σ2, vl1] = with_pc(pc.union(-k), () => vl.eval_apply(σ1, v2));
  return [ σ2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The ‘pc' parameter is thus part of the closure of all facet-related
functions.  The =with_pc= construct require mutability in order to
change the current program counter referenced in the closure.
Mutability can be waived as a requirement if the language supports
dynamic scoping [Art of Interp].

****** Using a context object
[[file:js/lab/oo-split-context/base.js]]
[[file:js/lab/oo-split-context/facets.js]]

Using a global variable for just the ‘pc' and not the store or
environment seems heterogeneous.  We can adopt the position that the
‘pc' is an extension to the state of the interpreter, and bundle all
this state in a ‘context' objet passed around in function calls.  Then
we profit from the dynamic nature of JavaScript objects: we can add
any property at runtime.  Rather than hiding the ‘pc' away, it makes
the state passing explicit, and removes the need for =with_pc=.

This solution has the benefit of homogenizing the order of formal
parameters: the context object will always be the first one, and the
=eval= functions will always return a context object rather than just
the store.

Here is what the third branch of =facet.eval_apply= looks like:

#+BEGIN_SRC js
  let [C1, vh1] = vh.eval_apply(with_pc(C, C.pc.union(k)), v2);
  let [C2, vl1] = vl.eval_apply(with_pc(C1, C.pc.union(-k)), v2);
  return [ C2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The evaluation of functions now explicitly deals with contexts:

#+BEGIN_SRC js
  function app(e1, e2) {
    return {
      eval(C) {
        let [C1, v1] = e1.eval(C);
        let [C2, v2] = e2.eval(C1);
        return v1.eval_apply(C2, v2);
      }
    };
  }
#+END_SRC

The signatures are simpler, though we lose a bit in legibility, as =C=
is opaque: we do not see what the context made is of by looking at the
formal parameters.  Only looking at the entry point
(=interpretProgram=) reveals its contents.

The context object bundles all the state needed by the interpreter,
and its extensions.  Adding another piece of state is modular since
the base interpreter already passes down this context object.

This solution does not require mutability, but it benefits from
dynamic typing.  In the case of the base interpreter, the context
object has type (Store * Environment), while in the faceted
interpreter, it has type (Store * Environment * PC).

Note that we could combine both methods, and avoid passing the context
object explicitly in all functions by using a global variable.  In
that case, both mutability and dynamic typing are leveraged.

***** Handling the change in the reference rule
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

With the extra ‘pc' out of the way, the only change remaining is the
=mk_facet= call in the reference rule.  This is how runtime faceted
values are effectively created.

#+BEGIN_SRC diff
79c79
<       σ2[a] = mk_facet(pc, v, bottom);
---
>       σ2[a] = v;
#+END_SRC

We can solve this by creating two different versions of the =ref=
function, one with the standard behavior, and one suited for the
instrumentation.

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom); // this line changes
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

But how do we differentiate which version of =ref= to use when calling
=interpretProgram=?  We need namespaces.  Using objects as namespaces,
both versions of =ref= can coexist.

#+BEGIN_SRC js
  this.base = {
    interpretProgram,
    c, v, fun, app, ref, deref, assign,
    _innards: { bottom, address, closure}
  };
#+END_SRC

The base interpreter exports both public interface and a "restricted"
one intended for extension purposes.

#+BEGIN_SRC js
  this.facets = {
    __proto__: this.base,
    interpretProgram,
    ref
  };
#+END_SRC

The instrumented interpreter exports the same interface as the base,
only overriding definitions for =ref= and =interpretProgram=.

Both interpreters can then be tested independently.

#+BEGIN_SRC js
  with (base) {
    console.log('std', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}
    ));
  }

  with (facets) {
    console.log('facets', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}, [1]
    ));
  }
#+END_SRC

Finally, both base interpreters (with a global ‘pc', and with a
context object) are free of instrumentation-specific concerns.  In
fact, the base interpreter with the global ‘pc' is /identical/ to the
base object-oriented interpreter.

***** Summary
We split the object-oriented interpreter into two modules:
1. The base interpreter which contains no concerns pertaining to
   faceted evaluation.
2. The facet interpreter which extends the context of the base
   interpreter, either by using a (delimited) global, or by extending
   the context object.  The facet interpreter then selects and
   overrides the entry point (=interpretProgram=) and the reference
   rule (=ref=).

We used two different mechanisms to handle the extra ‘pc' parameter:
1. A ‘pc' variable in the closure of all facet-related functions.
   This solution works if the language provides mutable variables or
   dynamic scoping.
2. Modifying the base interpreter to use a context object for the
   store and environment.  We then leveraged the ability to add
   properties to existing objects and included the program counter in
   this context.  Mutability is not required, but dynamic typing
   allows us to use a context object with two incompatibles types.

Overriding the =ref= function required only delegation using the
prototype chain.

**** Split pattern-matching instrumented interpreter into modules
[[file:js/lab/es6-split-global/base.js]]
[[file:js/lab/es6-split-global/facets.js]]

We can handle the extra ‘pc' parameter in the same ways we did with
the object-oriented interpreter.

However, when we try to extend the =ref= rule, two things are
different.

***** The facet-specific rules are scattered
The =eval_apply=, =eval_deref= and =eval_assign= code for facet values
is split across the three =application_rules=, =deref_rules= and
=assign_rules= objects.  In the object-oriented approach, they were
regrouped under the same =facet= object.

#+BEGIN_SRC js
  function facet(k, vh, vl) {
    return {
      eval_apply(σ, v2) {...},
      eval_deref(σ) {...},
      eval_assign(σ, v) {...}
    };
  }
#+END_SRC

#+BEGIN_SRC js
  base.application_rules.facet = (σ, {k, vh, vl}, v2) => {...};
  base.deref_rules.facet = (σ, {k, vh, vl}) => {...};
  base.assign_rules.facet = (σ, {k, vh, vl}, v) => {...};
#+END_SRC

This is a manifestation of the expression problem, or the "tyranny of
the primary decomposition": in the object-oriented approach, adding a
term is just adding an object with all its evaluation functions; while
adding evaluation functions requires modifying all the objects.  Note
that since in JavaScript objects can be extended at runtime, the
primary decomposition has a lower impact.

However, here we have to add the =facet= rules to the base objects
directly.  A more modular approach would be to create new rules
objects that extends (by prototype links) the base rules objects.  But
this is not an option here because the base rules objects are enclosed
by the evaluation functions.  If we create a =facet_application_rules=
object like so:

#+BEGIN_SRC js
  facet_application_rules = {
    __proto__: base.application_rules,
    facet(σ, {k, vh, vl}, v2) => {...}
  };
#+END_SRC

Then we have to redefine the =eval_apply= function to call this new
object instead of the =base.application_rules=.

#+BEGIN_SRC js
  function eval_apply(σ, v1, v2) {
    return facet_application_rules[v1.type](σ, v1, v2);
  }
#+END_SRC

But =eval_apply= is in the closure of the =rules.app= function, so we
have to redefine it as well just to be able to update its closure.

#+BEGIN_SRC js
  app(σ, θ, {e1, e2}) {
    ...
    return eval_apply(σ2, v1, v2);
  },
#+END_SRC

And we would have to do the same for every rules object, and for every
function that refer to these rules object.  In the end, we are back to
copy-pasting the base interpreter just to update the closures of its
functions.

It is clear that the lexical scoping of the rules object is the issue.
Being able to refer to these object from a dynamic scope would resolve
it.

***** Extending the ref function
In the object-oriented approach, we redefined the =ref= function in
the facets module and delegated the other rules to the base
interpreter.  Here we cannot do so.

In the object-oriented interpreter, the =ref= function returns an
object which contains its own evaluation method.  Thus, by overriding
this =ref= function we can change the way =ref= nodes are evaluated.
In the client code, a call to =ref= is dynamically dispatched to
either the base or facet version.

In the pattern matching interpreter, the =ref= function returns an
object /without/ an evaluation function -- only bearing a type used by
=interpretNode= to dispatch to the correct evaluation function.  The
evaluation function is in the =rules= object, separated from the
object created by =ref=.  To change the evaluation of =ref= nodes, we
need to change the =ref= function inside the =rules= object, but we
also need to update the dispatching function.

The =rules= object is in the closure of =base.interpretNode=.
Extending the =rules= object with a new =ref= function would not
change the behavior of =base.interpretNode=.

#+BEGIN_SRC js
  let rules = {
    __proto__: base.rules,
    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    }
  };
#+END_SRC

Trying to redefine a new =interpretNode= function in the facets module
to close over this new =rules= object is not a solution.  Rules from
the base interpreter will still call the =base.interpretNode= function
which refer to the =base.rules= object.  Redefining every function
that calls =base.interpretNode= to call =facets.interpretNode= would
work, but that's basically duplicating the base interpreter.

What does work is to ‘fluid-let' the =rules= object inside
=facets.interpretProgram=.

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    let old_ref = base._innards.rules.ref;
    base._innards.rules.ref = (σ, θ, {e}) => {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    };
    let r = with_pc(new Set(default_pc), () =>
                    base.interpretProgram(AST, env, store));
    base._innards.rules.ref = old_ref;
    return r;
  }
#+END_SRC

Note that, again, this construct is emulating a dynamic scoping of the
=rules.ref= function.  We could define a function similar to =with_pc=
and write the following instead:

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    return with_ref((σ, θ, {e}) => { ... }, () =>
                    with_pc(new Set(default_pc), () =>
                            base.interpretProgram(AST, env, store)));
  }
#+END_SRC

But at this point, having to write several ‘with' functions becomes a
pattern we would like to abstract away once and for all.

Furthermore, this emulation is a brittle way of extending the
functionality.  First we add coupling by using the =base._innards=
interface; secondly we disallow any opportunity for combining
extensions (since we do not /extend/ but /replace/ the base
functionality).  A proper dynamic scoping of the =rules= object has
none of these downsides.

As a compromise, we can recognize that the pattern matching done for
AST nodes is trivial: the objects returned by =app=, =ref=, etc. only
have one method, =eval=.  Instead of returning objects, we can return
closures directly and eliminate the need for the =rules= object.

[[file:js/lab/closures/base.js]]
[[file:js/lab/closures/facets.js]]

#+BEGIN_SRC js
  function ref(e) {
    return (σ, θ) => { ... };
  }
#+END_SRC

This allows us to override the functionality of =ref= using a
namespace, as we did in the object-oriented approach.

***** Summary
The two solutions (global variable and context object) for dealing
with the extra ‘pc' parameter can be applied here as well.  The same
remarks apply.

However, extending the =ref= function requires the ability to modify
values inside the closures of the evaluation function =interpretNode=.
This ability is not provided by the language, but shadowing the =ref=
function (or the rules object, or the =interpretNode= dispatcher) by
using dynamic scoping achieves the same effects.

**** Summary of JavaScript variations
It appears that, the more the interpreter rely on dynamic features,
the easier it is to instrument.  The /dynamic dispatching/ of the
object-oriented interpreter allows the effortless addition of the
=facet= value.  In all the other cases, /dynamic scoping/ was
prescribed.  We also saw that /dynamic typing/ was required at least
in the ‘context object' solution to the extra ‘pc' argument.

Intuitively, it makes sense.  We want different instances of the same
names (=interpretProgram=, =ref=, =rules=) to have different
behaviors, depending on context.  This is exactly what dynamic
dispatching is for: the same method slot can refer to different
implementations, depending on the actual instance of the receiver.
This is also the difference between lexical and dynamic scoping: the
former binds free names to their static surrounding context at
definition time, while the latter binds free names to their caller's
context at runtime.

In a language with dynamic scoping the interpreter should be a breeze
to instrument.  That is the focus of the [[Lisp][Lisp variations]].

** Lisp
[[file:lisp/pm.lisp]]

Common Lisp offers both lexical and dynamic scoping of variables.  We
implement the pattern-matching standard interpreter by defining the
=*rules*= object to be dynamically scoped (by using =defparameter=).
We follow the Common Lisp convention of using stars to surround a
dynamically-scoped name.

#+BEGIN_SRC lisp
  (defparameter *rules*
    `((c . ,(lambda (s env node) ...))

      (ref . ,(lambda (s env node) ...))))

  (defun eval-node (store env node)
    (let ((f (lookup (car node) *rules*)))
      (funcall f store env node)))

  (defun eval-program (AST env store)
    (eval-node store env AST))

  (eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '())
#+END_SRC

Instrumentation is then effortless.  First we define facet-specific
rules by extending the basic rules objects (=append ... *rules*=).

#+BEGIN_SRC lisp
  (defparameter *facets/rules*
    (append
     `((ref . ,(lambda (s env node)
                 ...
                 (mk-facet *pc* v1 bottom)
                 ...)))
     ,*rules*
     ))

  (defparameter *facets/application-rules* ...)
  (defparameter *facets/deref-rules* ...)
  (defparameter *facets/assign-rules* ...)
#+END_SRC

Note the reference to the free dynamic variable =*pc*=.  In the entry
point to facet evaluation, we override the standard rules with the new
ones.  We also declare the =*pc*= argument to be dynamically scoped
inside this call, using =(declare (special *pc*))=.

#+BEGIN_SRC lisp
  (defun facets/eval-program (AST env store *pc*)
    (declare (special *pc*))
    (let ((*rules* *facets/rules*)
          (*application-rules* *facets/application-rules*)
          (*deref-rules* *facets/deref-rules*)
          (*assign-rules* *facets/assign-rules*))
      (eval-program AST env store)))

  (facets/eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '() '(1))
#+END_SRC

So as anticipated, dynamic scoping is an adequate solution to the
issues we ran into in the JavaScript variations.  But dynamic scoping
is not available in all languages.  As a fallback, we saw that a
language with mutable global variables could emulate dynamic scoping.
This begs the question: in an immutable language without dynamic
scoping, how do we deal with the problem of modular instrumentation?

Furthermore, we also saw that dynamic typing was useful in the
‘context' object case.  What if we want to benefit from the guarantees
provided by static types?  Is the modular instrumentation still
feasible?  Is it cumbersome to write?  That is the focus of the
[[Haskell][Haskell variations]].

** Haskell
Building scaffolding with languages features has the following
advantages:
+ No extra syntax or rewriting program required
+ In statically-typed Haskell, the scaffolding is type-checked

Downsides:
- The scaffolding might is seldom straightforward
- Extension + overriding of existing definitions leads to very complex
  code

Extending the syntax is the same, with pros and cons inversed:
- Extra syntax and rewriting program required
- Rewritten program is type-checked, but transformation must be proven
  correct

Advantages:
+ Lightweight syntax is straightforward to use
- Overriding it still awkward to read

*** Building scaffolding with language features
**** Monadic interpreters
The monadic interpreter is mostly taken from [[http://homepages.inf.ed.ac.uk/wadler/papers/essence/essence.ps][Wadler]].

- [[http://web.cecs.pdx.edu/~mpj/pubs/modinterp.html][Monads Transformers and Modular Interpreters]]
  + [[http://www.cas.mcmaster.ca/~kahl/FP/2003/Interpreter.pdf][Haskell implementation]]

**** Either data type
See [[file:hs/extend-types/Extension.fail.1.hs]].  Types are extended like
so:

: data FacetTerm  = Facet Principal FacetTerm FacetTerm | BaseTerm Term

***** What does work
- Maximum reuse from the file ‘Base.hs'
- Able to execute `term0` and `term1`

***** What fails
- `term2` gives a type error:
    "Couldn't match expected type `Term' with actual type `FacetTerm'"

: term2 = (Lam "y" (Facet 0 (BaseTerm (Lam "x" Bot)) (BaseTerm Bot)))

- Also, have to wrap Base.Term values with the BaseTerm constructor

***** What I wanted
- `eval term2` gives the same result as when using Extended.eval.

- The raw term1 and term2 should type without wrapping values.

***** Why it doesn't work
A FacetTerm is either a Facet or a BaseTerm.  A Facet can contain
FacetTerms (and hence BaseTerms), but since BaseTerms are just Terms,
they cannot contain Facets.

***** Conclusion
What we really want is to insert the constructor `Facet` into the
existing data type `Term`.

**** Type classes
Another suggestion by Rémi.

#+BEGIN_SRC haskell
interp :: Dom d => Term -> d
interp (Add l r) = myAdd (interp l) (interp r)

class Dom d where
  myAdd :: d -> d -> d

instance Dom Int where
  myAdd = +

instance Dom OddOrEven where
  myAdd = xor
#+END_SRC

Here you must generalize the interpreter, to accomodate multiple
domains.  But at least the generalization is done using types: the
overhead is minimal.  Though you still need to have indirect calls.

> Ismael: Some disadvantages of this approach are discussed in the
Open Data Types paper, in Section 6.4.

***** Multi-param types classes
#+BEGIN_SRC haskell
class Eval term value where
  eval :: term -> value

#+END_SRC

Becomes quite complicated rapidly.  First you need an extension, then
you quickly run into typing issues that are not worth the flexibility
offered by this strategy.

**** Data types à la carte
From Swierstra, 2008.

[[file:hs/extend-types/Extension.swierstra.hs][A first attempt]]

[[file:hs/InterpreterALC.hs][Another one, with Ismael]]

[[file:hs/ALC-Lambda.hs][Lambda calculus with references and bottom]]

[[file:hs/ALC-Lambda-Facets.hs][Lambda calculus with faceted evaluation]]

[[file:hs/ALC-Facets-Flow.hs][Lambda calculus with faceted evaluation and FlowR tainting]]

Overview of this scaffolding:

Pros:
+ Allows type-checked extension of terms

Cons:
- Quite hairy
- Cannot change the resulting value with using the same approach for
  the value type, which would be even /hairier/.

See also my [[Data types  à la carte (2015)][my second attempt]] at Swierstra, in 2015.

**** Implicit arguments
A simple idea: implicit arguments can replace global variables.

Unfortunately, describing an override of the base eval function is still an
issue.  Maybe use type classes?

See [[file:hs/implicit][implicit]].

**** Facets as a monad
The faceted evaluation strategy requires a new state global: the program
counter.  It makes sense to add this state in a state monad to a monadic
interpreter.  This can be done using monad transformers, or monad views, or
whatever is more convenient.

But maybe we can also view facet values themselves as a specific case of the
list monad.

See [[file:hs/list-monad/][list-monad]].

*** Extending the syntax
See [[file:hs/transform/notes.org]]

[[file:hs/transform/tests/2/LC.hs][Lambda calculus with FlowR instrumentation]]

- Cannot override existing definitions (like one would do with aspects)
- Extending the monadic stack is best done with scaffolding, though
  obliviousness is lost

** Modular monadic interpreters
*** The giants
There's a body of work on monadic interpreters in functional programming
languages built around monads, dating from the ‘90s.

Starting with Eugenio Moggi lectures at Edinburgh in 1990, then followed by
Wadler, Steele, Hudak and Jones.

- Eugenio Moggi.  An abstract view of programming languages. 1990
- Philip Wadler.  The essence of functional programming. 1992
- Guy Steele.  Building interpreters by composing monads. 1994
- Sheng Liang, Paul Hudak, Mark Jones.  Monad Transformers and Modular
  Interpreters. 1995
- Luc Duponcheel.  Using catamorphisms, subtypes and monad transformers for
  writing modular functional interpreters.  1995
- David Spinoza.  Semantic Lego (thesis).  1995

Then there is the, more recent, use of the free monad to build DSLs and
interpreters in functional languages.  Swierstra reports that free monads were
well-known in category theory, but unfamiliar to functional programmers.

- Wouter Swierstra.  Data types à la carte.  2008
- Tom Schrijvers, Bruno C.d.S. Oliveira.  Functional Pearl: The Monad Zipper.
  2010
- Oleg Kiselyov, Amr Sabry, Cameron Swords.  Extensible Effects -- An
  Alternative to Monad Transformers.  2013
- Rúnar Óli.  Compositional application architecture with reasonably-priced
  monads (Scala Days talk).  2014

On the other side of the fence, in OO-land, the go-to solution is a mix of
modules and design patterns.

- Robert Bruce Findler and Matthew Flatt.  Modular Object-Oriented Programming
  with Units and Mixins.  1998
- Bruno C.d.S. Oliveira.  Modular Visitor Components -- A practical Solution to
  the Expression Families Problem.  2009
- Brudo C.d.S. Oliveira, William R. Cook.  Extensibility for the Masses --
  Practical Extensibility with Object Algebras.  2012

**** Data types  à la carte (2015)
Re-reading Swiestra in 2015, my brain parses it much better.

- Coproduct is just disjoint-union.
- Construction of values of the coproduct type are unwieldy, hence the recourse
  to type classes that will automatically call the right injection into the
  coproduct based on the type of the expression.  Expression types are required,
  otherwise the compiler cannot find the right injection.

  The use class constraints and instances to automatically figure out the right
  injection is quite clever.
- Can't we derive functors, for the coproduct, since they seem rather
  straightforward?  We can.  Probably `deriving Functor` was not available in
  Haskell in 2008, when Swierstra wrote it.
- We can give a larger type signature than necessary, since the compiler cannot
  infer it for us.  =Add :+: Val :+: Add :+: Val= is perfectly acceptable.  In
  practice, this is harmless.
- While we can easily extend the language and compose languages using this
  approach, modifying behavior requires defining a new algebra.  Can we re-use
  parts of a previous algebra?  There is no clue as to how to achieve that in
  this paper.
- Having terms appear in the signature to programs gives you guarantees for
  free.  As illustrated in the paper, a program of type =Term Recall Int= (\sect6)
  will never modify the value of the memory cell.  Also the topic of \sect7, and
  related work on type systems for effects.

**** Wadler — The essence of functional programming
How to build a monadic interpreter.  Also serves as a tutorial on monads.

An interpreter just creates computations inside a monad M.  By changing the
definition of M (its type, unit and bind) /and other small changes/, one is able
to change the evaluation order of arguments (from call-by-value to
call-by-name), or give a non-deterministic interpreter.

Plugging the identity monad reduces the monadic interpreter to a straightforward
lambda-calculus interpreter, showing that the monadic interpreter is a
generalization.

Variation one already requires changes in addition to redefining the monad.
This is inevitable to be able to distinguish between calls of =unitM= that
signal errors from calls that produce ground values.

Variation two requires to extend the =interp= function.  The current position is
saved in the =P= monad, which is akin to a "forgetful" state monad.

One important observation on monadic interpreters is that if the interpreter
code needs to interact with the monad (e.g., for resetting the current
position), then the =interp= code needs to make a call to =resetP=.  Monads are
not oblivious at all in this case.  But if the interpreter code does not need to
interact with the monad, then changes to the monad can affect the meaning of the
interpreter in an oblivious fashion.

*** The bigger picture
In the solutions from the functional programming world, the idea seems to
revolve around reifying the computation (the target program of an interpreter),
and write different interpreters.  In other words, to achieve modularity, one
must turn behavior into data: data has no effect unless it is executed by some
interpreter and turned into an actual computation.

So, essentially we are writing programs in a DSL of which we know one
interpreter (hopefully).  This is high-level, programming the specification.
Then we are free to write other interpreters for the same program, to change its
meaning slightly.

The connection to DSLs of course brings works from other communities to the
table.

Viewing the interpreter as data is something that is free in Lisps.  The code
can always be seen as a data structure that is easily parsed.  If turning your
code to data gives you the greatest modularity, then homoiconic languages have
a clear advantage of not requiring a parser.

* TRANSLATE Construire un interpréteur par modules
/Contenu principalement tiré d'un papier présenté à l'atelier FOAL'15 [[cite:KNS-15][KNS-15]]/.

Étendre un interpréteur est une instance du problème de l'expression (voir
[[#problem]]).  Dans le problème de l'expression, on cherche à écrire un
interpréteur que l'on pourra étendre de deux façons: soit en ajoutant des
nouveaux termes au langage, soit en ajoutant de nouvelles opérations qui
s'appliquent aux termes.

On a vu précedemment que résoudre le problème de l'expression dans un langage
dynamique n'est pas difficile ([[#variations]]).  On peut pousser le problème plus
loin en ajoutant le critère de pouvoir /modifier/ des termes ou des opérations
existants.

Ce chapitre présente une façon de construire un interpréteur extensible en
JavaScript, par couches successives.  Chaque couche ajoutée vient enrichir la
fonctionnalité de l'interpréteur dynamiquement, sans avoir à changer les arbres
syntaxiques à exécuter.  Pour ce faire, on utilise principalement la liaison
tardive à travers la construction ~with~ de JavaScript.

** Ajouter des termes
On construit un interpréteur pour un simple langage arithmétique, suivant
l'exemple de Wadler [??] et d'Odersky et Zenger [[cite:OZ-05][OZ-05]].  Pour commencer, on veut
juste représenter des nombres entiers:

: <term> ::= <num>
: <num>  ::= 0 | 1 | 2 | ...

Et pouvoir les évaluer:

: eval : Term -> Integer

L'interpréteur doit pouvoir représenter des constantes et les évaluer.  Pour
implémenter ce terme, on utilise une décomposition objet en JavaScript, qui
rappelle le patron /Interpreter/ [[cite:GHJ+94][GHJ+94]].  Chaque terme du langage sera
représenté par un objet qui compertera une fonction pour chaque opération sur le
langage.

#+BEGIN_SRC js -n -r
var num = {
  new(n) { return {__proto__: this, n} },
  eval() { return this.n }
}

var e1 = num.new(3)                        (ref:foal-ex0)
e1.eval() //: 3
#+END_SRC

#+BEGIN_aside
Dans les programmes de ce chapitre, le trigramme ~//:~ indique la valeur de
l'expression qui précède.  C'est la syntaxe de l'éditeur interactif [[cite:s3c][s3c]].
#+END_aside

#+CAPTION: Représentation de la mémoire du programme après la création de l'objet
#+CAPTION: ~num~, mais avant la création de ~e1~.
[[file:img/lassy-0.svg]]

L'objet ~num~ comporte deux fonctions: ~new~, le constructeur qui capture la
constante; et ~eval~ pour retourner cette constante ultérieurement.  Ligne
[[(foal-ex0)]], on voit comment créer la première expression du langage pour
représenter le nombre 3.  Ligne suivante, on peut évaluer cette expression pour
retourner le nombre passé au constructeur.

Le second terme du langage représente l'addition entre deux constantes.

: <term> ::= <num> | <num> + <num>
: <num>  ::= 0 | 1 | 2 | ...
:
: eval : Term -> Integer

Suivant notre implémentation par objets, on ajoute le terme en créant un objet
~plus~ qui comporte un constructeur et une fonction d'évaluation:

#+BEGIN_SRC js
var plus = {
  new(l, r) { return {__proto__: this, l, r } },
  eval() { return this.l.eval() + this.r.eval() }
}

var e2 = plus.new(num.new(1), num.new(2))
e2.eval() //: 3
#+END_SRC

Pour évaluer ~plus~, on appelle récursivement la fonction ~eval~ sur ses
opérandes ~l~ et ~r~.

On voit donc que pour ajouter un terme à l'interpréteur, il suffit de créer un
nouvel objet qui répond à la fonction ~eval~.

** Ajouter des opérations
Maintenant on souhaite ajouter une seconde opération au langage: afficher une
expression (pretty-printing).

: show : Term -> String

Suivant la décomposition par objets, il faut ajouter l'opération sur chaque
objet qui représente le terme.  Dans des langages objets qui ne permettent pas
l'extension, ajouter une opération ne serait pas aussi simple que d'ajouter un
terme.  C'est la tension qui est au cœur du problème de l'expression.  En
JavaScript, les objets sont extensibles, donc ajouter une opération se fait
simplement:

#+BEGIN_SRC js -n -r
num.show = function() {                      (ref:foal-ex1)
 return this.n.toString()
}

plus.show = function() {                     (ref:foal-ex2)
  return this.l.show() + '+' + this.r.show()
}

plus.new(num.new(1), num.new(2)).show() //: "1+2"  (ref:foal-ex3)
e1.show() //: "3"
e2.show() //: "1+2"
#+END_SRC

Ligne [[(foal-ex1)]], on ajoute l'opération ~show~ au terme ~num~; même chose pour
~plus~ ligne [[(foal-ex2)]].  On voit qu'on peut appliquer la nouvelle opération
~show~ sur un terme de la même façon que ~eval~ (ligne [[(foal-ex3)]]).

Mais les lignes qui suivent montrent que cette nouvelle opération est aussi
disponible sur les termes créés /avant/ l'ajout de l'opération.  Les variables
~e1~ et ~e2~ sont les termes créés dans les exemples précédents.  Au moment de
leur création l'opération ~show~ n'était pas définie, et pourtant elle
s'applique correctement à ces instances.

Affecter les instances existantes de termes est rendu possible par le mécanisme
de délégation.  La délégation est illustrée dans la représentation ci-contre.
On y voit l'objet ~e2~ qui a pour /prototype/ l'objet ~plus~; le prototype
permet d'établir un lien de délégation en JavaScript.  Quand on appelle
l'opération ~show~ sur ~e2~, elle n'est pas présente sur l'objet lui-même, mais
sur son prototype.  L'opération est donc trouvée et appliquée à l'objet.  De
même pour les sous-termes ~e2.l~ et ~e2.r~ qui vont chercher l'opération ~show~
de l'objet ~num~.

Les deux mécanismes qui permettent l'extension des opérations sont donc: la
délégation, et la possibilité d'ajouter des fonctions aux objets après leur
création.  Sans la délégation, l'objet ~e1~ contiendrait une copie de chaque
opération après sa création; il n'y aurait plus de lien entre ~num~ et ses
instances sur toute la durée de vie du programme.  En conséquence, ajouter une
opération à ~num~ n'affecterait pas les instances déjà créées.  Si on souhaitait
ajouter ~show~ à ces instances, il faudrait les cataloguer pour pouvoir toutes
les modifier.  De même, si les objets JavaScript étaient fixes après leur
création, on ne pourrait pas leur ajouter de fonction.  Pour étendre le langage,
il faudrait créer un nouvel objet ~num2~ qui contiendrait l'opération ~show~, et
aurait ~num~ comme prototype.

Pouvoir ajouter une opération rétroactivement peut être pratique dans un système
évolutif.  On peut étendre un interpréteur sans avoir à le relancer, ni même
reconstruire l'arbre syntaxique d'un programme.  On peut enrichir la sémantique
du langage à l'exécution, et évaluer un même programme pour obtenir différents
résultats.

D'un autre côté, on peut aussi vouloir préserver la transparence référentielle
et ne pas affecter les termes existants.  C'est possible par simple délégation:
on crée des dérivés de ~num~ et ~plus~ qui possèdent l'opération ~show~.  On ne
peut pas donner à ces dérivés les même noms que les objets originaux, donc on
utilise un nouvel objet en tant qu'espace de noms.  On ne souhaite pas non
plus coupler prématurément les dérivés de ~num~ et ~plus~ aux objets dont ils
dérivent; on abstrait donc ces objets parents à l'aide d'une fonction.

#+BEGIN_SRC js -n -r
var show = function(base) {                       (ref:foal-ex4)
  var num = {__proto__: base.num,                 (ref:foal-ex5)
    show() { return this.n.toString() }}

  var plus = {__proto__: base.plus,               (ref:foal-ex6)
    show() { return this.l.show() + '+' + this.r.show() }}

  return {num, plus} }                            (ref:foal-ex7)

var s = show({num, plus})                         (ref:foal-ex8)
e2.show //: undefined                             (ref:foal-ex9)
s.plus.new(s.num.new(1), s.num.new(2)).show() //: "1+2"  (ref:foal-ex10)
s.plus.new(s.num.new(1), s.num.new(2)).eval() //: 3      (ref:foal-ex11)
#+END_SRC

Le code émule un système de module: ~show~ ligne [[(foal-ex4)]] est une fonction qui
prend un module de base et retourne un module enrichi avec l'opération ~show~.
Les modules sont ici de simples objets JavaScript.  L'objet ~base~ passé en
argument doit contenir les termes du langage: ~base.num~ et ~base.plus~, qui
sont utilisés comme prototypes des nouveaux objets ~show.num~ et ~show.plus~
(lignes [[(foal-ex4)]] et [[(foal-ex5)]]).  Ces nouveaux objets contiennent juste
l'opération ~show~ définies comme précédemment.  Enfin, la fonction ~show~
retourne un nouveau module (un objet) qui contient les versions enrichies de
~num~ et ~plus~ (ligne [[(foal-ex7)]]).

Une fois cette fonction définie, on peut instancier le module ~show~ dans la
variable ~s~ par l'appel ligne [[(foal-ex8)]].  Si on ne considère pas les
modifications de l'exemple précédent, le terme ~e2~ ne peut pas répondre à
l'opération ~show~ (ligne [[(foal-ex9)]]).  En revanche, une expression construite à
partir des nouveaux termes ~s.plus~ et ~s.num~ supporte l'opération ~show~
(ligne [[(foal-ex10)]]).  Et par délégation, ce nouveau terme supporte aussi
l'opération ~eval~ (ligne [[(foal-ex11)]]).  Les termes existants ne sont donc pas
affectés par l'ajout de l'opération ~show~, et les nouveaux termes supportent
les deux opérations.

Mais il y a maintenant un problème d'interopérabilité entre les deux variantes
du langage.  On peut construire des expressions qui mixent les termes de base
les termes du module 'show':

: s.plus.new(num.new(1), s.num.new(2)).show()

Le premier argument de ~s.plus.new~ est un ~num~ et le second un ~s.num~.  Si on
appelle l'opération ~show~ sur cette expression, le programme lève une erreur
car l'opération ~show~ n'est pas définie sur ~num~.  La définition de
~plus.show~ requiert que ~this.l~ supporte également l'opération ~show~.  Donc,
le constructeur de termes ~s.plus~ ne devrait accepter que des termes du langage
enrichi: des ~s.plus~ ou des ~s.num~.  Or, on ne modifie pas la définition du
constructeur pour restreindre les objets acceptés.

Le constructeur ~s.plus.new~ est le même que ~plus.new~, le constructeur de
base:

: plus.new : Term -> Term -> Term
: s.plus.new : Term -> Term -> Term

mais la définition de ~s.plus.show~ implique en réalité le type:

: s.plus.new : Term with Show -> Term with Show -> Term with Show

où ~with Show~ indique qu'il s'agit d'un terme qui supporte l'opération ~show~.

On peut empêcher la création d'expressions mixtes par construction.  Il suffit
de n'autoriser qu'un seul module de termes à la fois, en masquant les références
vers les ~num~ et ~plus~ originaux.  C'est possible avec la construction ~with~
de JavaScript.

#+BEGIN_SRC js
with (show({num,plus})) {
  plus.new(num.new(1), num.new(2)).show() //: "1+2"
}
#+END_SRC

L'expression construite à l'intérieur du bloc délimité par ~with~ fait référence
simplement à ~num~ et ~plus~.  Mais ici, comme nous somme dans le corps du
~with~, ~num~ et ~plus~ font référence aux objets retournés par l'appel de
~show~, pas aux objets originaux.  Les originaux sont inaccessibles car masqués
par les noms des dérivés dans l'environnement du bloc.  Par construction, les
expressions créées à l'intérieur du ~with~ ne peuvent pas mixer plusieurs
variantes du langage.

Un autre aspect de cette construction est que les dérivés de ~num~ et ~plus~
existent uniquement dans le corps du ~with~; ils ne peuvent pas être référencés
en dehors.  Le ~with(show)~ correspond donc à une activation locale du module
~show~, locale dans la syntaxe et dans le temps.

La construction ~with~ n'est pas strictement nécessaire ici.  On peut très bien
utiliser une fonction à la place:

#+BEGIN_SRC js -n -r
(function({num}) {
  num.new(1).show() //: "1"
}(show({num,plus}))            (ref:foal-ex12)
#+END_SRC

Cette fonction attend un module qui contient ~num~ pour pouvoir créer et
afficher l'expression ~num(1)~.  On boucle ce module en lui passant ~show()~
comme argument à la ligne [[(foal-ex12)]], ce qui a le même effet que ~with~: le
code à l'intérieur de la fonction est exécuté.

#+BEGIN_aside
La syntaxe étant plus lourde avec une fonction qu'avec ~with~ , on utilisera
~with~ dans les exemples qui suivent.
#+END_aside

Cette fonction et ~with~ ont en commun de créer un environnement lexical pour le
code de leur bloc.  Pour empêcher la création d'expressions mixtes, il suffit
donc de contrôler les noms qui sont accessibles au code à l'intérieur du bloc.
Ici, on restreint les expressions au seul terme ~num~ du module ~show~.  Au-dessus,
on a restreint les expressions aux deux termes du module ~show~.

Jusqu'ici, on a vu comment:
- ajouter de nouveaux termes au langage, en ajoutant un module;
- ajouter de nouvelles opérations, soit globalement en modifiant les prototypes
  des termes, soit localement avec ~with~.

Dans les deux cas, les ajouts au code de l'interpréteur se font par
/incréments/, et sans modifier le code écrit au préalable.  Essentiellement, on
a une solution au problème de l'expression en JavaScript.

Mais on peut aller plus loin.  En plus d'ajouter des termes et des opérations,
on peut modifier des opérations existantes, et combiner

# En passant, on a vu comment ajouter des opérations de deux façons différentes:
# soit en modifiant directement le prototype des objets représentants les termes
# pour y ajouter les opérations, soit en dérivant les prototypes.  Dans la
# première solution, tous les objets obéissent aux mêmes opérations.  Dans la
# seconde solution, on peut mixer des termes qui ne supportent pas toutes les
# opérations, ce qui peut causer une erreur à l'exécution.  C'est alors qu'on peut
# restreindre les termes utilisables dans un bloc de code en utilisant ~with~, ou
# une fonction.

** Modifier des opérations
On va maintenant s'intéresser à modifier des opérations.  Par exemple, en
changeant le comportement de l'opération ~eval~ pour renvoyer le double d'une
constante.

Ça reste simple à exprimer en JavaScript, puisque les fonctions contenues dans
des objets sont comme les autres propriétés: remplaçables à tout moment par
simple assignation.  On a utilisé l'assignation pour ajouter ~show~, maintenant
on s'en sert pour écraser la définition de ~eval~ et la remplacer par une
nouvelle:

#+BEGIN_SRC js
num.eval = function() { return this.n * 2 }

num.new(1).eval() //: 2
plus.new(num.new(1), num.new(2)).eval() //: 6
#+END_SRC

Maintenant, une variante.  Plutôt que de retourner le double de ~n~,
l'évaluation d'un ~num~ doit retourner le double du résultat de l'évaluation
d'origine.  Pour implémenter cette variante, il nous faut pouvoir faire
référence à l'évaluation d'origine dans la nouvelle version de ~num.eval~.  On
peut sauvegarder la version d'origine dans une fermeture:

#+BEGIN_SRC js
(function(previous_eval) {
  num.eval = function() { return previous_eval.call(this) * 2 }
}(num.eval))

num.new(1).eval() //: 2
plus.new(num.new(1), num.new(2)).eval() //: 6
#+END_SRC

#+BEGIN_aside
Une fonctionnalité similaire au mot-clé ~proceed~ en AspectJ et autres langages
à aspects.
#+END_aside

Il ne reste plus qu'à appeler cette version antérieure en lui passant l'objet
courant comme receveur à l'aide de l'appel ~call(this)~.

Mais modifier l'objet ~num~ d'origine de cette façon est destructif: la fonction
~num.eval~ d'origine n'est sauvegardée que dans la fermeture, et inaccessible
par la suite.  C'est un mauvais choix si on souhaite rendre l'interpréteur
extensible.  Dans ce cas, on peut plutôt créer un nouveau terme qui fait
référence à la fonction ~num.eval~ pour éviter la duplication de code.  Pour
cela, on crée une fonction qui sert de module, paramétrée par l'objet ~num~
d'origine:

#+BEGIN_SRC js
var double = function(base) {
  var num = {__proto__: base.num,
    eval() { return base.num.eval.call(this) * 2 }}
  return {num} }

with (double({num})) {
  plus.new(num.new(1), num.new(2)).eval() //: 6
}
#+END_SRC

Dans le module ~double~, on redéfinit ~eval~ à partir de l'évaluation
précédente.  Ceci nous permet de /composer/ les modifications d'opérations en
passant une base modifiée comme paramètre au module ~double~.  L'exemple qui
suit illustre comment combiner ces extensions avec des appels de ~with~ en
cascade:

#+BEGIN_SRC js
with (double({num})) {
  with (double({num})) {
    with (double({num})) {
      plus.new(num.new(1), num.new(2)).eval() //: 24
}}}
#+END_SRC

** Passer de l'état aux opérations
Dans l'instrumentation de Narcissus pour l'évaluation à facettes, on a vu que de
l'état était passé récursivement dans l'interpréteur: le /program counter/.
Pour simuler cette situation dans notre interpréteur de langage arithmétique, on
va compter le nombre d'appels à ~eval~.

Pour cela, on écrit une fonction ~state~ qui dérive des termes ~num~ et ~plus~
et incrémente une variable locale lors d'un appel à ~num.eval~ ou ~plus.eval~.
L'évaluation est ensuite déléguée aux prototypes de ces termes.  La valeur du
compteur est accessible via la fonction ~getCount~.

#+BEGIN_SRC js
var state = function(base, count = 0) {
  var num = {__proto__: base.num,
    eval() { count++; return base.num.eval.call(this) }}

  var plus = {__proto__: base.plus,
    eval() { count++; return base.plus.eval.call(this) }}

  var getCount = function() { return count }

  return {num, plus, getCount}}

with (state({num,plus})) {
  getCount() //: 0
  plus.new(num.new(1), num.new(2)).eval() //: 3
  getCount() //: 3
}
#+END_SRC

Lors du premier appel à ~getCount~, le compteur est bien à zéro, puisqu'aucune
instruction n'a été évaluée.  Le second appel retourne 3, après l'évaluation de
l'expression, ce qui correspond bien aux 3 appels de ~eval~: un pour ~plus~, et
un pour chaque ~num~.

Dans la définition de ~state~, les nouvelles versions de ~eval~ sont définies à
partir des termes passés en argument dans l'objet ~base~, ce qui permet encore
une fois la composition d'extensions.  Par exemple, on peut ajouter l'extension
~double~:

#+BEGIN_SRC js
with (double({num})) {
  with (state({num, plus})) {
    getCount() //: 0
    plus.new(num.new(1), num.enw(2)).eval() //: 6
    getCount() //: 3
}}
#+END_SRC

Le résultat de l'évaluation est 6, ce qui indique que la modification ~double~
est bien active.  En même temps, le compteur fonctionne également.  Notons que
l'ordre d'activation des modules n'importe pas ici:

#+BEGIN_SRC js
with (state({num, plus})) {
  with (double({num})) { ... }}
#+END_SRC

donnerait le même résultat.  La commutativité dépend de la définition des
modules, bien entendu.  Dans le cas présent, le compteur peut être vu comme un
effet de bord qui n'interfère pas avec l'évaluation.

Enfin, on peut ajouter le module ~show~ sans avoir à modifier une seule des
définitions précédentes.  Ici encore, l'ordre d'activation des modules n'a pas
d'incidence sur le résultat:

#+BEGIN_SRC js
with (state({num, plus})) {
  with (double({num})) {
    with (show({num, plus})) {
      getCount() //: 0
      var n = plus.new(num.new(1), num.new(2))
      n.eval() //: 6
      getCount() //: 3
      n.show() //: "1+2"
#+END_SRC

* Dynamic scoping to build interpreters
From the LASSY'15 paper.

An arithmetic expression language, the example taken by Wadler and
Odersky as a base to propose solutions to the expression problem.

#+NAME: svg
#+BEGIN_SRC js :exports none
  function toMarkup(obj, open, close, attr) {
    if (typeof obj !== 'object') return obj.toString();

    let tagName = obj.t;
    let attrs = Object.getOwnPropertyNames(obj.attrs).map(n => attr(n, obj.attrs[n]));
    let content = obj.children.map(o => toMarkup(o, open, close, attr)).join('');
    return open(tagName, attrs) + content + close(tagName);
  }

  function toSVG(obj) {
    function openSVG(tagName, attrs) { return '<' + (tagName + ' ' + attrs.join(' ')).trim() + '>' }
    function closeSVG(tagName) { return '</' + tagName + '>' }
    function attrSVG(k, v) { return k  + '="' + v + '"' }
    return toMarkup(obj, openSVG, closeSVG, attrSVG);
  }

  function t(tagName, ...args) {
    let o = {
      t: tagName,
      attrs: Object.create(null),
      a(k, v) { o.attrs[k] = v; return o },
      children: args,
    };
    let shortcuts = ['width', 'height', 'fill', 'stroke', 'cx', 'cy', 'r',
                     'x', 'y', 'x1', 'y1', 'x2', 'y2', 'rx', 'ry',
                     'id', 'viewBox', 'refX', 'refY', 'markerUnits',
                     'markerWidth', 'markerHeight', 'orient',
                     'transform'];
    shortcuts.forEach(s => { o[s] = o.a.bind(o, s) });
    return o;
  }

  function box(names) {
    let marginTop = 0;
    let marginBottom = 10;
    let marginLeft = 7;
    let marginRight = 10
    let fontSize = 15;
    let lineHeight = 18;
    let charWidth = 9;

    let longest = names.map(p => p.length).reduce((a,b) => Math.max(a,b), 0);
    let width = marginLeft + longest * charWidth + marginRight;
    let height = marginTop + marginBottom + lineHeight * names.length;

    let propText = names.map((n, i) => t('text', n).x(marginLeft)
                                                   .y((i+1) * lineHeight)
                                                   .fill('#657b83'));

    let g = t('g',
              t('rect').width(width).height(height).stroke('#657b83').fill('#fdf6e3'),
              ...propText)
      .a('font-size', fontSize)
      .a('font-family', 'Monospace')
      .height(height)
      .width(width);

    return g;
  }

  function link(a, b, options) {
    let {vertical, dashed, reversed, offset} = options || {};
    offset = offset || 15;

    let arrowLength = 50;
    let arrowTip = 15;
    let space = arrowLength + arrowTip;

    if (vertical) {
      var g = above(a, b, space);
      [a, b] = g.children;
      var y1 = a.attrs.height;
      var y2 = y1 + arrowLength;
      var x1 = offset, x2 = offset;
      var center = {x: x1, y: y1 + space / 2};
    } else {
      var g = beside(a, b, space);
      [a, b] = g.children;
      var x1 = a.attrs.width;
      var x2 = x1 + arrowLength;
      var y1 = offset, y2 = offset;
      var center = {x: x1 + space / 2, y: y1};
    }

    let arrow = t('line').x1(x1).x2(x2).y1(y1).y2(y2).stroke('#fdf6e3')
      .a('stroke-width', 5).a('marker-end', 'url(#triangle)');

    if (dashed) arrow.a('stroke-dasharray', '3 1');
    if (reversed) arrow.transform(`rotate(180 ${center.x} ${center.y})`);

    return t('g', a, arrow, b).width(g.attrs.width).height(g.attrs.height);
  }

  function beside(a, b, hspace) {
    hspace = hspace || 20;

    return t('g', a, b.transform(`translate(${a.attrs.width + hspace}, 0)`))
      .width(a.attrs.width + hspace + b.attrs.width)
      .height(Math.max(a.attrs.height, b.attrs.height));
  }

  function above(a, b, vspace) {
    vspace = vspace || 20;
    return t('g', a, b.transform(`translate(0, ${a.attrs.height + vspace})`))
      .width(Math.max(a.attrs.width, b.attrs.width))
      .height(a.attrs.height + vspace + b.attrs.height);
  }

  function name(n) {
    let g = box([n]);
    g.children[0].rx(10).ry(10);
    return g;
  }

  function ref(a, b, options) {
    options = options || {};
    options.dashed = true;
    return link(a, b, options);
  }

  let proto = link;
  let obj = function(o) { return box(Object.getOwnPropertyNames(o)) };

  function svg(...gs) {
    let triangle = t('marker', t('path').a('d', 'M 0 0 L 10 5 L 0 10 z'))
      .id('triangle').viewBox('0 0 10 10').refX(0).refY(5)
      .markerWidth(4).markerHeight(3)
      .orient('auto').fill('#fdf6e3');

    print(toSVG(t('svg', triangle, ...gs)));
  }
#+END_SRC

** The base datatype
#+NAME: num
#+BEGIN_SRC js
  var num = {
    new: function(n) { return {__proto__: this, n} },
    eval: function() { return this.n }};
#+END_SRC

#+BEGIN_SRC js
  <<num>>

  var e1 = num.new(3);
  print(e1.eval());
#+END_SRC

#+RESULTS:
: 3

#+BEGIN_SRC js :results silent
  <<num>>

  <<svg>>
  redirect('img/num.svg')
  svg(ref(name('e1'),
          proto(obj(num.new(3)),
                ref(obj(num), name('num'),
                    {reversed:true}))))
#+END_SRC

# [[file:img/num.svg]]

** Adding a data variant
#+NAME: plus
#+BEGIN_SRC js
  var plus = {
    new: function(l, r) { return {__proto__: this, l, r,} },
    eval: function() { return this.l.eval() + this.r.eval() }};
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>

  var e2 = plus.new(num.new(1), num.new(2));
  print(e2.eval());
#+END_SRC

#+RESULTS:
: 3

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>

  <<svg>>
  redirect('img/num-plus.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(3)),
                      ref(obj(num), name('num'),
                          {reversed:true})))

  var $e2 = ref(name('e2'),
                proto(obj(plus.new(num.new(1), num.new(2))),
                      ref(obj(plus), name('plus'),
                          {reversed:true})))

  svg(above($e1, $e2))
#+END_SRC

# [[file:img/num-plus.svg]]

** Adding an operation
#+NAME: show-invasive
#+BEGIN_SRC js
  num.show = function() { return this.n.toString() }
  plus.show = function() { return this.l.show() + '+' + this.r.show() }
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>

  var e2 = plus.new(num.new(1), num.new(2));

  <<show-invasive>>

  print(e2.show()); // Dynamic extension, without recreating the expression
  print(plus.new(num.new(1), num.new(2)).show());
#+END_SRC

#+RESULTS[dcac8561b9f21e4bb1985c63e9e70ff10ee557dc]:

This extension is invasive: it modifies the prototypes of =num= and
=plus=.  If we want, we can extend safely both objects.

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show-invasive>>

  <<svg>>
  redirect('img/show-invasive.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(3)),
                      ref(obj(num), name('num'),
                          {reversed:true})))

  var $e2 = ref(name('e2'),
                proto(obj(plus.new(num.new(1), num.new(2))),
                      ref(obj(plus), name('plus'),
                          {reversed:true})))

  svg(above($e1, $e2))

#+END_SRC

# [[file:img/show-invasive.svg]]

** Adding an operation as a module
#+NAME: show
#+BEGIN_SRC js
  var show = function(base) {
    var num = {__proto__: base.num,
      show() { return this.n.toString() }};

    var plus = {__proto__: base.plus,
      show() { return this.l.show() + '+' + this.r.show() }};

    return {num, plus};
  };
#+END_SRC

#+NAME: show-ex1
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  var s = show({num, plus});
  print(s.plus.new(s.num.new(1), s.num.new(2)).show());
#+END_SRC

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  let s = show({num, plus});
  let e3 = s.plus.new(s.num.new(1), s.num.new(2));

  <<svg>>
  redirect('img/show-module.svg')
  let $s = ref(name('s'), obj(s));
  let $e2 = ref(name('e2'), proto(obj(plus.new(num.new(1), num.new(2))),
                                  ref(obj(plus), name('plus'), {reversed:true})));

  let $e3 =  ref(name('e3'),
                 proto(obj(e3),
                       ref(obj(e3.__proto__),
                           name('s.plus'), {reversed:true})));

  let g = link($e2, $e3, {vertical:true, reversed:true, offset: 220});
  svg(above($s, g))
#+END_SRC

# [[file:img/show-module.svg]]

Works, but can mix languages in unsafe ways:

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  var s = show({num, plus});

  try { print(s.plus.new(num.new(1), s.num.new(2)).show()); }
  catch (e) { print(e) }
#+END_SRC

#+RESULTS:
: TypeError: this.l.show is not a function

*** A use-case for =with=
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  with(show({num, plus})) {
    print(plus.new(num.new(1), num.new(2)).show())
  }
#+END_SRC

#+RESULTS:
: 1+2

Cannot mix languages anymore because of name shadowing: only one =num=
and one =plus= is known in the body of =with=, and they are both from
the same language.

Inside =with=, here is what we see:

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  let s = show({num, plus});

  <<svg>>
  redirect('img/show-module-with.svg')
  let $num = ref(name('num'), proto(obj(s.num), obj(s.num.__proto__)));
  let $plus = ref(name('plus'), proto(obj(s.plus), obj(s.num.__proto__)));

  svg(above($num, $plus))
#+END_SRC

# [[file:img/show-module-with.svg]]

Outside =with=, the =show= module is out of scope:

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>

  <<svg>>
  redirect('img/show-module-with-outside.svg')
  let $num = ref(name('num'), obj(num));
  let $plus = ref(name('plus'), obj(plus));

  svg(above($num, $plus))
#+END_SRC

# [[file:img/show-module-with-outside.svg]]

*** Selective imports with an IIFE
#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>

  (function({num}) {
    print(num.new(1).show())
  }(show({num, plus})))
#+END_SRC

#+RESULTS:
: 1

Of course, here =plus= is in context, but we would actually put it in
a =base= module as well.

The two forms have a subtle difference: in a =with= we can modify the
values of the scope object by assigning to them, but in the IIFE,
assigning to the arguments has no effect outside the function.  In our
two examples, we always pass a fresh module so there is no issue.

** Modifying an operation
#+BEGIN_SRC js
<<num>>
<<plus>>

num.eval = function() { return this.n * 2 }

print(num.new(1).eval())
print(plus.new(num.new(1), num.new(2)).eval())
#+END_SRC

#+RESULTS:
: 2
: 6

Previous version of =num.eval= is lost: we have no reference to it
anymore.

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>

  <<svg>>
  redirect('img/modify-num.svg')
  var $e1 = ref(name('e1'),
                proto(obj(num.new(1)),
                      ref(box(['new', 'eval: this.n * 2']), name('num'),
                          {reversed:true})))

  var $prev = box(['new', 'eval: this.n'])

  svg(above($e1, $prev))
#+END_SRC

# [[file:img/modify-num.svg]]

*** Non-destructive modification
#+NAME: double
#+BEGIN_SRC js
<<num>>
<<plus>>

var double = function(num_orig) {
  var num = {__proto__: num_orig,
    eval() { return num_orig.eval.call(this) * 2 }}
  return {num}
}
#+END_SRC

#+BEGIN_SRC js
<<double>>

with(double(num)) {
  with(double(num)) {
    print(plus.new(num.new(1), num.new(2)).eval())
  }
}
#+END_SRC

#+RESULTS:
: 12

Inside the inner-most =with=, the objects in scope are the modified
=num=, and the original =plus=.

#+BEGIN_SRC js :results silent :exports none
  <<svg>>
  redirect('img/modify-num-module.svg')
  var $num = ref(name('num'),
                proto(box(['eval: previous() * 2']),
                      proto(box(['eval: previous() * 2']),
                            box(['new', 'eval: this.n']))))

  var $plus = ref(name('plus'), obj(plus))

  svg(above($num, $plus))
#+END_SRC

# [[file:img/modify-num-module.svg]]

While after the =with=, =num= refers to the original, unmodified object.

#+BEGIN_SRC js :results silent :exports none
  <<plus>>
  <<svg>>
  redirect('img/modify-num-module-outside.svg')
  var $num = ref(name('num'), box(['new', 'eval: this.n']))
  var $plus = ref(name('plus'), obj(plus))

  svg(above($num, $plus))
#+END_SRC

# [[file:img/modify-num-module-outside.svg]]

** Passing state
Add a program counter incremented each time a data variant calls
=eval=.

#+NAME: state
#+BEGIN_SRC js
  var state = function(base, pc = 0) {
    var num = {__proto__: base.num,
               eval() { pc++; return base.num.eval.call(this) }}

    var plus = {__proto__: base.plus,
               eval() { pc++; return base.plus.eval.call(this) }}

    var getPC = () => pc

    return {num, plus, getPC}
  }
#+END_SRC

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<state>>

  with (state({num, plus})) {
    print(getPC())
    print(plus.new(num.new(1), num.new(2)).eval())
    print(getPC())
  }
#+END_SRC

#+RESULTS:
: 0
: 3
: 3

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<state>>

  var s = state({num,plus});

  <<svg>>
  redirect('img/state.svg')
  var $getPC = ref(name('getPC'), box(['() => pc']))
  var $num = ref(name('num'), proto(obj(s.num), obj(s.num.__proto__)))
  var $plus = ref(name('plus'), proto(obj(s.plus), obj(s.plus.__proto__)))

  svg(above($getPC, above($num, $plus)))
#+END_SRC

# [[file:img/state.svg]]

** All in one
Combine all the extensions without effort.

#+BEGIN_SRC js
  <<num>>
  <<plus>>
  <<show>>
  <<state>>
  <<double>>

  with (state({num,plus})) {
    with (double(num)) {
      with (show({num,plus})) {
        print(getPC())
        let n = plus.new(num.new(1), num.new(2))
        print(n.eval())
        print(getPC())
        print(n.show())
      }}}
#+END_SRC

#+RESULTS:
: 0
: 6
: 3
: 1+2

#+BEGIN_SRC js :results silent :exports none
  <<num>>
  <<plus>>
  <<show>>
  <<state>>
  <<double>>

  with (state({num,plus})) {
    with (double(num)) {
      with (show({num,plus})) {
        var pc = getPC
        var n = plus.new(num.new(1), num.new(2))
      }}}

    <<svg>>
    redirect('img/all-in-one.svg')

  var $getPC = ref(name('getPC'), box(['() => pc']))
  var $nl = ref(name('n.l'),
                proto(obj(n.l),
                      proto(obj(n.l.__proto__),
                            proto(box(['eval', '(from double)']),
                                  proto(box(['eval', '(from state)']),
                                        ref(obj(n.l.__proto__.__proto__.__proto__.__proto__), name('num'),
                                            {reversed:true}))))))
  var $n = ref(name('n'),
               proto(obj(n),
                     proto(obj(n.__proto__),
                           proto(box(['eval', '(from state)']),
                                 ref(obj(n.l.__proto__.__proto__.__proto__), name('plus'),
                                     {reversed:true})))))

  svg(above($getPC, above($n, $nl)))
#+END_SRC

# [[file:img/all-in-one.svg]]

* STABLE Étendre un interpréteur par manipulation de portée
/Chapitre principalement tiré d'un papier rejeté à DLS'15 et à SAC'16 [[cite:KNS-15b][KNS-15b]]/.

#+BEGIN_epig
The whole purpose of this work is to be able to modify the original interpreter
in the less possible intrusive way, and this appears to be the case, so I find
this work interesting, and since it is self-contained and properly grounded into
existing work, it is certainly worthy of publication. -- Relecteur DLS n°1

Readers not familiar with evaluation contexts and scoping will find that the
paper provides a didactic introduction to the concepts, but familiarised readers
will find the paper rather dull. -- Relecteur DLS n°2

The pattern is either elegant JavaScript hacking or severe abuse of
modularity. I can't tell which! I guess I like it. It illustrates how using
language constructs to implement your own module system, rather than having a
built-in one, enables it to be modified to allow
extensibility. -- Relecteur DLS n°3

I like the simplicity of the approach and the clever use of the "with" statement
semantics to achieve the dynamic extension of the scope from the outside code.
-- Relecteur SAC n°1
#+END_epig

Nous avons vu dans l'étude de cas que Narcissus n'était pas étendu de façon
modulaire.  Dans ce chapitre, on propose une façon simple d'étendre cet
interpréteur, en se basant uniquement sur les fonctionnalités présentes dans le
langage JavaScript.

** Manipuler la portée des variables pour l'instrumentation
L'ingrédient clé de cette section est la notion de /portée de variable/, et
comment manipuler cette portée.  Une variable déclarée dans une fonction n'est
pas accessible depuis une autre fonction; on dit alors que la portée de cette
variable est la fonction dans laquelle elle est définie, ou encore que la
variable est locale à la fonction.  À l'inverse, une variable déclarée globale
sera accessible par tout point d'exécution du programme.

En JavaScript typiquement, on peut considérer que l'ensemble des variables
accessibles à un point d'exécution donné est déterminé par un objet qu'on
appelle /l'environnement/.  Un environnement associe chaque nom de variable à
une valeur (valeur primitive comme un nombre, ou une référence vers un objet),
et possède un lien vers un environnement parent.  Pour nos besoins, un
environnement est donc très similaire à un objet JavaScript.

Les programmes donnés en exemple de cette section sont volontairement triviaux,
puisque ce sont les mécanismes de manipulation de l'environnement qui nous
intéressent, et non le code métier.  À la fin du chapitre, on applique ces
mécanismes à un cas concret, Narcissus.

Les exemples de code sont écrits en suivant un sous-ensemble du standard
ECMAScript 5.1, que l'on va enrichir avec la possibilité de manipuler des
environnements.  On ne donnera pas de sémantique à ce langage étendu, mais ces
manipulations seront expliquées à l'aide de diagrammes qui parsèment ce
chapitre.  Les diagrammes reflètent le modèle qu'on cherche à atteindre; ce
modèle est inspiré du fonctionnement de JavaScript, mais n'est pas forcément lié
à ce langage.  Néanmoins, la seconde section montre comment implémenter ce
modèle en JavaScript.

*** La portée dans le motif module
L'interpréteur Narcissus est construit à partir du /motif module/.  Il n'y a pas
de système de module en JavaScript, et c'est ce motif simple qui est souvent
utilisé à la place.  Le motif utilisé par Narcissus se présente ainsi:

#+BEGIN_aside
Du moins, pas dans le standard ECMAScript 5.1.  Un système de module est proposé
dans la prochaine version.
#+END_aside

#+BEGIN_SRC js -n -r
var Narcissus = (function(){                        (ref:ex0-iife-start)
  var globalBase = { ... }                          (ref:ex0-defs-start)

  function ExecutionContext(type, version) { ... }
  function getValue(v) { ... }
  function putValue(v, w) { ... }
  function evaluate(code) { ... }                   (ref:ex0-defs-end)

  return {                                          (ref:ex0-return-start)
    globalBase: globalBase,
    evaluate: evaluate,
    ...
  }                                                 (ref:ex0-return-end)
}())                                                (ref:ex0-iife-end)
#+END_SRC

Le but de ce motif est de créer un environnement pour toutes les définitions du
module.  En JavaScript, une définition faite à la racine d'un fichier (une
déclaration de variable avec ~var~ ou une définition de ~function~) créée une
entrée dans l'environnement global.  Les entrées de l'environnement global sont
accessibles en lecture et écriture par n'importe quel point du code, et même par
du code d'autres fichiers chargés dynamiquement.  Mettre ses définitions dans
l'environnement global a deux désavantages immédiats: 1) n'importe quel code
peut écraser ces définitions, et 2) vos propres définitions peuvent écraser les
définitions faites précédemment par d'autres fichiers, et même écraser des
parties de l'API standard du langage.  Écraser une définition ne lève pas
d'erreur ou d'avertissement en JavaScript; donc utiliser l'environnement global
peut facilement casser du code chargé précédemment, et rend votre propre code
fragile pour les mêmes raisons.  En outre, les différents moteurs d'exécution de
JavaScript peuvent proposer différentes extensions à l'API standard, ce qui se
traduit par différents noms supplémentaires dans l'environnement global.  Pour
toutes ces raisons, les programmeurs JavaScript apprennent rapidement à enrober
leurs définitions dans un environnement séparé.  Cet environnement est créé en
enrobant les définitions dans une fonction, ouverte à la ligne [[(ex0-iife-start)]]
et fermée à la ligne [[(ex0-iife-end)]].  Toutes les définitions de variables et
fonctions des lignes [[(ex0-defs-start)]] à [[(ex0-defs-end)]] sont ainsi protégées de
tout code extérieur au module, car inaccessibles.

Si le module est là pour fournir une fonctionnalité spécifique, ce qui est le
cas de l'interpréteur qui fournit une fonction d'évaluation, alors il doit
exposer au moins une définition au monde extérieur, l'exporter.  Exporter des
définitions se fait à travers un objet JavaScript (un simple dictionnaire, qui
associe des chaînes de caractères à des valeurs), aux lignes [[(ex0-return-start)]]
à [[(ex0-return-end)]].  Cet objet, appelons-le l'objet exporté, est la valeur de
retour de la fonction qui créée l'environnement du module.

Enfin, l'objet exporté est assigné à la variable ~Narcissus~ (ligne
[[(ex0-iife-start)]]), une définition faite à la racine et qui peut être utilisée
par n'importe quel code chargé dynamiquement, pour accéder à la fonctionnalité
proposée par le module.  La fonction qui crée l'environnement du module est
appelée juste après sa définition (les ~()~ ligne [[(ex0-iife-end)]] appellent la
fonction), on la nomme donc une /fonction immédiatement appelée/, ou FIA.

Essayons de comprendre exactement les mécanismes du langage en jeu dans ce
module.  D'abord, on va construire un exemple minimal de module qui a un
comportement observable:

#+BEGIN_SRC js -n -r
var m = (function(){
  var a = 1                               (ref:ex1-defs-start)
  function f(x) { return x + a }
  function g(x) { return f(x) }           (ref:ex1-defs-end)
  return {g: g}                           (ref:ex1-return)
}())

m.g(0) //: 1                              (ref:ex1-test)
#+END_SRC

On a ici un simple module qui retourne une seule fonction, ~g~.  Cette fonction
applique la fonction ~f~ à son argument et retourne le résultat, et ~f~ à son
tour retourne l'addition de son argument et de ~a~, une variable interne au
module.  Quand on appelle ~m.g(0)~ à l'extérieur du module, on obtient ~1~, qui
est la valeur de ~a~.

#+ATTR_HTML: :style margin-top:-20rem
#+BEGIN_side-figure
[[file:img/dls0.svg]]
#+END_side-figure

Le diagramme illustre les environnements et les objets créés après l'appel de le
la FIA et avant l'appel de la ligne ~m.g~ à la ligne [[(ex1-test)]].  Les objets
JavaScript sont représentés par des boîtes qui contiennent des paires.  Une
paire est une propriété, avec à gauche le nom de la propriété et à droite sa
valeur.  Lorsque la valeur de la paire est une référence, la case contient une
puce d'où part un trait qui finit sur l'objet référencé.  Par exemple, on peut
voir l'objet exporté à gauche, qui a une propriété ~g~ qui pointe vers la
fonction ~g~.

On représente les environnements avec des boîtes également, et l'environnement
parent est relié par le trait qui part de la puce du coin supérieur gauche de
l'environnement.  Lorsqu'un nom est recherché dans un environnement et que ce
nom n'est pas présent dans les propriétés de cet environnement, la recherche
continue dans l'environnement parent, et récursivement jusqu'à ce que soit le
parent n'existe pas, soit la propriété est trouvée.  L'environnement global est
en haut à gauche.

On peut expliquer le motif module en regardant le diagramme qui accompagne le
code.  Avant d'exécuter le code de l'exemple, l'environnement global est vide.
Lorsque la FIA est appelée, elle crée l'environnement en {{{color(c1)}}}.
Puisque la FIA est définie à la racine, l'environnement parent est l'objet
global.  À l'intérieur de la FIA, trois noms sont définis: une variable ~a~ et
deux fonctions ~f~ et ~g~ (lignes [[(ex1-defs-start)]] à [[(ex1-defs-end)]]).
Lorsqu'une fonction est définie, un objet fonction est créé qui contient son
code et son environnement lexical.  Cet environnement sera utilisé pour exécuter
le corps de la fonction lorsqu'elle sera invoquée.  Avant que la FIA ne
retourne, elle crée l'objet exporté (ligne [[(ex1-return)]]) qui contient la
propriété ~g~.  Cette propriété réfère la fonction ~g~ définie à l'intérieur du
module.  Notons qu'il y a /deux/ références à la fonction ~g~ à l'exécution.
Enfin, la FIA retourne l'objet exporté, qui devient la valeur de la variable
globale ~m~.

#+BEGIN_side-figure
[[file:img/dls1.svg]]
#+END_side-figure

On peut aussi suivre le flot de contrôle suscité par l'appel à la ligne
[[(ex1-test)]] sur ce second diagramme.  D'abord, la référence ~m.g~ est résolue en
cherchant la propriété ~m~ dans l'environnement courant, c'est à dire,
l'environnement global.  ~m~ existe et pointe vers un objet, donc l'interpréteur
cherche maintenant la propriété ~g~ dans cet objet.  ~g~ existe, et réfère un
objet fonction, on peut donc procéder à l'appel ~m.g(0)~.  Lorsque la fonction
est appelée dans l'environnement global, l'interpréteur crée un environnement
pour le corps de la fonction qu'on appelle l'objet /d'activation/ de ~g~.  Cet
objet associe les noms des paramètres déclarés par la fonction aux valeurs
passées par l'appel; ici, il y a donc une seule entrée ~x : 0~.  L'objet
d'activation de ~g~ est un environnement, et il a comme environnement parent
l'objet référencé par la propriété /env/ de l'objet fonction de ~g~.  Puis, le
contrôle est transféré au corps de la fonction ~g~, qui contient ~f(x)~.
L'interpréteur résout les noms ~f~ et ~x~ en cherchant à travers la chaîne
d'environnements: d'abord dans l'environnement courant (l'objet d'activation de
~g~).  ~f~ n'existe pas dans cet objet, mais il existe dans son objet parent.
~x~ existe, et sa valeur est utilisée pour continuer l'exécution de l'appel
~f(x)~.  Lorsque ~f~ est appelée, l'interpréteur crée un nouvel objet
d'activation pour les paramètres formels, qui a l'environnement de la FIA comme
environnement parent.  Après ça, l'interpréteur continue l'exécution avec le
corps de ~f~, ~x+a~.  La propriété ~x~ se trouve dans l'objet d'activation de
~f~ (0), et la propriété ~a~ dans l'environnement parent (1).  L'interpréteur
somme les deux valeurs et retourne le résultat.

Le second diagramme sert à illustrer deux points importants sur le motif module:

1. La seul façon que du code extérieur au module a d'accéder aux définitions
   faites à l'intérieur du module est de passer par l'objet exporté.  Dans
   l'exemple que l'on a suivi, ~g~ est la seule référence exportée, mais c'est
   un alias.  Remarquons que si on essayait de changer la valeur de ~m.g~ par
   une autre fonction, ~m.g = function() { ... }~, seule la référence ~m.g~
   serait affectée, mais la propriété ~g~ dans l'environnement de la FIA ferait
   toujours référence à la fonction ~g~ définie dans le module.  Si l'on veut
   modifier les références internes au module, il faut accéder directement à
   l'environnement de la FIA.

2. Toutes les fonctions créées à l'intérieur du module utilisent l'environnement
   de la FIA comme environnement parent.  Autrement dit, les fonctions se
   referment sur leur environnement de définition (leur environnement
   /lexical/), et c'est pourquoi on appelle ces fonctions des /fermetures/.  Si
   l'on a accès à l'environnement de la FIA, on peut changer le comportement des
   fonctions internes au module en changeant les références de cet
   environnement.

On voit alors que l'environnement de la FIA est le point central du module, là
où tous les noms utilisés par le module sont recherchés.  On voit aussi que le
motif module ne permet pas, en l'état, d'accéder à ces noms de l'extérieur du
module.  C'est son rôle, mais ça s'oppose à notre but d'extensibilité.

*** Ouvrir le motif module
On cherche maintenant à accéder à l'environnement de la FIA de l'extérieur du
module, à obtenir une référence vers cet environnement.  En regardant les
diagrammes précédents, on pourrait penser que ~m.g.env~ suffit: il s'agit bien
d'une référence à l'environnement qui nous intéresse depuis l'extérieur du
module.  Malheureusement, cette référence est /interne/ au moteur d'exécution du
langage, donc n'est pas accessible par le code exécuté.  Mais même si on
modifiait le moteur d'exécution pour rendre cette référence publique, elle n'est
pas fiable.  Suivant la construction de ~g~, l'environnement qu'elle retourne
pourrait très bien être différent de celui de la FIA.

#+BEGIN_SRC js -n -r
var m = (function(){
  var mkG = function() {
    return function(x) { return x }
  }
  return {g: mkG()}                                     (ref:ex2-return)
}())
#+END_SRC

#+ATTR_HTML: :style margin-top:-10rem
#+ATTR_LATEX: :options [-10em]
#+BEGIN_side-figure
[[file:img/dls3.svg]]
#+END_side-figure

#+ATTR_HTML: :style margin-top:7rem
#+BEGIN_aside
Dans les diagrammes suivants, on n'illustrera plus les objets d'activation des
fonctions.  Il suffit de se rappeler que l'environnement utilisé pour rechercher
les variables libres est celui de la propriété ~env~.
#+END_aside

Dans cet exemple, la fonction ~g~ exportée par le module ne capture pas
l'environnement de la FIA.  À la ligne [[(ex2-return)]] on exporte la valeur de
retour de la function ~mkG~ sous le nom ~g~.  La fonction ~mkG~ retourne
exactement la même fonction ~g~ que dans l'exemple précédent, avec une
différence notable: cette fonction ~g~ capture l'environnement de l'appel à
~mkG~.  Le diagramme qui l'accompagne illustre cette différence par l'apparition
d'un nouvel objet d'activation pour ~mkG~ qui est référencé comme environnement
de ~g~.  On a ~m.g.env~ qui pointe vers l'environnement créé par ~mkG~
({{{color(c1)}}}), et non l'environnement créé par la FIA ({{{color(c2)}}}).
On voit alors que la propriété ~env~ d'une fonction n'est pas garantie de
pointer vers l'environnement dont nous avons besoin.

On va maintenant s'éloigner du comportement standard de JavaScript que nous
avons suivi jusqu'ici.  Supposons que nous /pouvons/ accéder à l'environnement
créé par la FIA grâce à une nouvelle propriété /E/ qui serait automatiquement
ajoutée à l'objet exporté par le module.  Supposons également que cet
environnement se comporte comme un objet JavaScript régulier; on peut lire et
changer ses propriétés.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_aside
La section suivante montre comment retrouver cette fonctionnalité dans le
langage standard.
#+END_aside

#+BEGIN_SRC js -n -r
var m = (function(){
  var a = 1
  function f(x) { return x + a }
  function g(x) { return f(x) }
  return {g: g}
}())

m.g(0) //: 1                             (ref:ex3-test1)
m.E.a = 2                                (ref:ex3-change)
m.g(0) //: 2                             (ref:ex3-test2)
#+END_SRC

#+ATTR_HTML: :style margin-top:-6rem
#+ATTR_LATEX: :options [-8em]
#+BEGIN_side-figure
[[file:img/dls4.svg]]
#+END_side-figure

Reprenons le module du début, mais avec l'addition de cette nouvelle propriété
~E~.  Si on exécute ~m.g(0)~ à la ligne [[(ex3-test1)]], on obtient toujours ~1~.
Par contre, cette fois on a accès à l'environnement interne au module via la
propriété ~m.E~.  Si on change la valeur de ~a~ dans cet environnement (ligne
[[(ex3-change)]]), alors l'appel ~m.g(0)~ qui suit utilise cette nouvelle valeur, et
c'est pourquoi /le même appel/ donne le résultat différent, ~2~, à la ligne
[[(ex3-test2)]].  Le diagramme qui accompagne l'exemple donne une autre façon de
voir ce qui se passe.  On y voit la nouvelle propriété ~E~ en {{{color(c1)}}}
sur l'objet exporté et référencé par la propriété globale ~m~.  Cette propriété
~m.E~ donne un accès en lecture et écriture sur l'environnement de la FIA.
Ainsi, l'affectation de la ligne [[(ex3-change)]] modifie la valeur de ~a~ dans le
module; dans le diagramme ~a~ a la valeur 2 en conséquence.

En ajoutant cette référence directe à l'environnement interne du module, on est
déjà capable de changer les résultats de l'appel ~m.g~, juste en changeant la
valeur de la variable ~a~.  Mais on peut aussi changer les fonctions.  Si l'on
souhaite changer la fonction ~f~ pour qu'elle retourne ~x + 2 * a~ au lieu de
~x + a~, la ligne [[(ex4-change-f)]] de l'exemple suivant nous indique comment:

#+BEGIN_SRC js -n -r
var m = (function() { ... }())

m.g(0) //: 1
m.E.f = function(x) { return x + 2 * m.E.a }  (ref:ex4-change-f)
m.g(0) //: 2
m.E.a = 2                                     (ref:ex4-change-a)
m.g(0) //: 4
#+END_SRC

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_side-figure
[[file:img/dls5.svg]]
#+END_side-figure

Notons qu'on ne peut pas simplement écrire ~x + 2 * a~.  Si on changeait la
définition de ~f~ à l'intérieur du module, on pourrait référencer ~a~
directement.  Mais ici, on crée une fonction à l'extérieur du module, qui doit
faire référence au ~a~ qui n'est déclaré que dans l'environnement du module.  Si
on écrit juste ~a~, ce sera une variable libre pour cette fonction.  Mais
puisqu'on a une référence à l'environnement du module via ~m.E~, on peut accéder
au ~a~ à travers elle.

Le diagramme illustre clairement le problème: la nouvelle fonction ~f~
({{{color(c1)}}}) n'a pas le même environnement que l'ancienne.  Puisqu'elle est
créée en dehors du module, son environnement est l'objet global (lien
{{{color(c2)}}}).  Le diagramme nous indique aussi une autre solution: modifier
la propriété ~env~ de la fonction qui vient remplacer ~f~.  Si l'on modifie
cette propriété pour pointer vers l'environnement de la FIA, la variable ~a~ ne
sera recherchée dans le même environnement que pour la fonction ~f~ d'origine;
c'est comme si l'on créait une fermeture dans le module, mais défini de
l'extérieur:

#+BEGIN_aside
Malheureusement, on ne peut pas modifier l'environnement capturé par une
fonction en JavaScript.  On ne peut même pas /accéder/ à l'environnement
capturé.  Cette solution ne peut donc pas être implémentée sans modifier le
moteur d'exécution du langage, ce qui n'est pas notre but ici.
#+END_aside

#+BEGIN_SRC js
m.E.f = function(x) { return x + 2 * a }
m.E.f.env = m.E
#+END_SRC

Les deux approches pour référencer ~a~ sont des compromis différents.  Utiliser
~m.E~ est plus explicite, et permet à la nouvelle fonction de capturer
l'environnement dans lequel elle est définie.  Modifier l'environnement de
définition permet d'écrire le code de la fonction comme si on l'écrivait à
l'intérieur du module, ce qui rend les différences de code plus lisibles.

Dans les deux cas, on peut voir que l'appel ~m.g(0)~ qui suit la redéfinition de
~f~ est bien affecté par le changement.  Qui plus est, puisque cette nouvelle
version de ~f~ fait référence au ~a~ du module, on peut changer ~a~ (ligne
[[(ex4-change-a)]]) et constater que le dernier appel ~m.g(0)~ utilise à la fois la
nouvelle fonction ~f~ et la nouvelle valeur de ~a~.  On peut donc modifier le
comportement de la fonction ~g~ en manipulant les variables et les fonctions
définies à /l'intérieur/ du module, mais en écrivant ces changements de
/l'extérieur/ du module.  On n'a pas à toucher le code du module pour pouvoir
changer les résultats des appels à ~m.g~.

Mais les changements que l'on a apporté au module sont /destructifs/.  En
changeant la valeur de ~a~, on détruit sa valeur initiale.  En changeant la
fonction ~f~ aussi.  Dans les deux cas, on modifie la valeur directement dans
l'environnement de la FIA.  Comme ces redéfinitions sont de simples
affectations, on peut sauvegarder la valeur initiale dans une variable pour
pouvoir les réutiliser après:

#+BEGIN_SRC js
var a_orig = m.E.a
var f_orig = m.E.f
#+END_SRC

Ce qui revient à simuler un mécanisme de pile manuellement.  Il y a une solution
plus élégante qui tire partie du mécanisme de résolution des noms de variable
dans une chaîne d'environnements.

*** Disposer les environnements en couches
On a vu que tout environnement a un environnement parent.  Si un nom n'est pas
trouvé dans un environnement, la recherche se poursuit dans l'environnement
parent.  La recherche continue ainsi jusqu'à ce que le nom soit trouvé, ou que
la chaîne d'environnement s'arrête car le lien vers l'environnement parent est
à ~null~.  On peut profiter de ce mécanisme pour construire une façon élégante
de changer les définitions internes au module.

#+ATTR_HTML: :style margin-top:-16rem
#+ATTR_LATEX: :options [-30em]
#+BEGIN_side-figure
[[file:img/dls6.svg]]

Exemple de recherche dans une chaîne d'environnements.  La recherche de la
propriété ~a~ commence par l'environnement le plus récent (en bas), et retourne
la valeur de la première cellule qui contient une propriété "a"; ici c'est le
second objet.  La recherche de ~g~ échoue.  Notons que le ~f~ du second objet
est caché par le ~f~ de l'objet en tête de la chaîne.
#+END_side-figure

Nous allons faire une seconde addition à notre variante de JavaScript.  On
suppose que lorsqu'un module est créé, un nouvel environnement vide, appelé
/l'environnement frontal/ est créé également.  L'environnement frontal a pour
parent l'environnement de la FIA, mais les fonctions définies à la racine du
module capturent l'environnement frontal plutôt que celui de la FIA.  La
propriété ~m.E~ est modifiée également pour pointer vers cet environnement
frontal.

#+ATTR_HTML: :style margin-top:12rem
#+ATTR_LATEX: :options [-12em]
#+BEGIN_side-figure
[[file:img/dls7.svg]]
#+END_side-figure

Si l'on reprend exactement le code du module, sans modifications, mais qu'on
l'exécute avec cette modification sémantique, on obtient la situation de la
figure suivante.  La redéfinition de ~a~ à ~2~ est enregistrée par
l'environnement frontal ({{{color(c1)}}}), plutôt que par l'environnement de la
FIA.  Puisque les deux fonctions ~f~ et ~g~ capturent l'environnement frontal,
elles utilisent bien la valeur redéfinie de ~a~ quand elles sont appelées.  Le
mécanisme de résolution des noms de variable examine les environnements de bas
en haut, en suivant les liens des coins des objets.  On voit donc que n'importe
quelle propriété présente dans l'environnement frontal a priorité sur les
propriétés de l'environnement de la FIA.  C'est pourquoi la redéfinition de ~a~
affecte bien l'appel ~m.g(0)~ de la même manière que dans les exemples
précédents.  Mais on a maintenant la possibilité /d'annuler/ cette redéfinition
en supprimant la propriété ~m.E.a~ de l'environnement frontal.  Puisque
l'environnement frontal ne contient que notre redéfinition, la valeur originale
n'est jamais altérée.  Lorsque l'on supprime ~m.E.a~, on vide l'environnement
frontal, et l'appel final ~m.g(0)~ va trouver la propriété ~a~ dans
l'environnement de la FIA, où la valeur est restée à ~1~.

#+BEGIN_SRC js -n -r
var m = (function() { ... }())

m.g(0) //: 1
m.E.a = 2
m.g(0) //: 2
delete m.E.a
m.g(0) //: 1
#+END_SRC

Avec cet environnement frontal, on peut surcharger et masquer les définitions du
module /sans toucher au code à l'intérieur du module/.  Dans la pratique, on ne
se contenterait pas de changer une seule valeur mais plusieurs, et on voudrait
pouvoir les injecter dans le module et les supprimer d'un coup.  En outre, si on
souhaite expérimenter différents ensembles de changements, on voudrait pouvoir
activer un premier ensemble, puis un second, puis désactiver le premier.
L'environnement frontal n'est pas suffisant pour ce scénario, mais on peut
en utiliser plusieurs.

Pour cela, il faut faire une dernière addition à la sémantique du langage: on
suppose que l'on peut récupérer et modifier le parent de n'importe quel
environnement à travers la propriété ~parent~.  Avec cette propriété, on peut
étendre la chaîne d'environnements utilisée par les fonctions définies dans le
module avec un nombre quelconque d'environnements.  Puisque tout environnement a
un parent, et que l'on a une référence vers l'environnement par lequel débute la
résolution de noms (~m.E~), la chaîne d'environnement a une structure similaire
à une liste chaînée, où l'environnement frontal serait la tête de la liste.  Et
puisqu'on peut modifier le parent d'un environnement, on peut insérer de
nouveaux environnements à n'importe quel point dans cette chaîne.

Le plus utile pour nos besoins est d'insérer un environnement juste après
l'environnement frontal.  Supposons que l'on dispose d'une fonction ~pushEnv(e,
chain)~ qui insère l'environnement ~e~ en tête de la chaîne; et une fonction
~removeEnv(e, chain)~ qui retire l'environnement de la chaîne (quelque soit sa
position).  Alors on peut écrire:

#+BEGIN_SRC js -n -r
var m = (function() { ... }())

m.g(0) //: 1

var e1 = { a:2, f: function(x) { return x + 2 * m.E.a }}
pushEnv(e1, m.E)                                          (ref:ex6-push1)
m.g(0) //: 4

var e2 = { f: function(x) { return -m.E.a }}
pushEnv(e2, m.E)
m.g(0) //: -2                                             (ref:ex6-diag)

removeEnv(e1, m.E)
m.g(0) //: -1
#+END_SRC

Sans changer le module, on ajoute toutes les redéfinitions de ~e1~ en appelant
~pushEnv~ (ligne [[(ex6-push1)]]).  On change à nouveau le comportement de ~f~ dans
l'environnement ~e2~ qu'on insère dans le module grâce au second appel à
~pushEnv~.  On voit que l'appel ~m.g(0)~ qui suit utilise la fonction ~f~ de
~e2~, puisque le résultat est négatif, et la valeur de ~a~ de l'environnement
~e1~, puisque la valeur est ~-2~.  Puis, on retire les changements de ~e1~ grâce
à ~removeEnv~.  L'appel ~m.g(0)~ utilise encore le ~f~ de ~e2~, mais la valeur
de ~a~ est la valeur initiale du module, ~1~.

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_side-figure
[[file:img/dls8.svg]]
#+END_side-figure

Le diagramme illustre la chaîne d'environnements quand l'exécution atteint la
ligne [[(ex6-diag)]].  L'environnement frontal (vide) a pour parent l'environnement
~e2~ ({{{color(c3)}}}), qui lui a comme parent ~e1~ ({{{color(c1)}}}), qui
délègue à l'environnement de la FIA.  Chaque ensemble de changements est isolé
des autres, ce qui permet de les supprimer sans toucher aux autres, et tous sont
ordonnés linéairement (le fil {{{color(c2)}}}), comme les couches d'un gâteau.
Ainsi, la priorité de redéfinition est toujours déterministe: les définitions
qui se trouvent le plus en tête (plus proches de l'environnement frontal) ont
toujours priorité sur celles des environnements du bout de la chaîne.

-----

On a donc une façon de modifier le comportement d'un module sans toucher au code
qui le définit.  Les changements sont écrits à l'extérieur du module, et peuvent
même être déclarés dans des fichiers séparés.  De plus, on peut dynamiquement
activer ou désactiver des ensembles de changements d'un seul coup.  Mais pour
l'instant rien de ce qu'on a définit n'est exécutable.  Il s'agissait
d'illustrer le modèle de fonctionnement dans le cadre d'un langage inspiré de
JavaScript.  Ce faisant, on a dévié de JavaScript en supposant:

1. que les environnements se comportent comme des objets JavaScript standards,
   comme des dictionnaires auxquels on peut ajouter des propriétés, récupérer
   leur valeur;

2. qu'un /environnement frontal/ est créé lors d'un appel à une fonction
   immédiatement appelée (FIA) sur laquelle repose le motif module, et qu'une
   référence vers cet environnement est retournée par la FIA;

3. que l'on pouvait obtenir et modifier le parent de n'importe quel
   environnement.

Aucune de ces suppositions n'est valide en JavaScript.  Cependant, il y a moyen
de réaliser ce modèle en utilisant des constructions standards de JavaScript.
C'est l'objet de la section suivante.

** Ouvrir le motif module en JavaScript
Pour réaliser le modèle illustré dans la section précédente, on peut tirer
partie de deux ingrédients de JavaScript que sont ~with~ et la délégation par
prototype.  On va utiliser ~with~ pour créer un environnement manipulable, puis
répliquer les exemples de composition d'environnement par couches grâce aux
prototypes.

#+ATTR_HTML: :style margin-top:-7rem
#+BEGIN_aside
Tous les exemples de cette section sont exécutables sur un interpréteur
JavaScript qui suit le standard ECMAScript 5.1.
#+END_aside

L'expression ~with~ est grammaticalement similaire à un ~if~.  Elle prend une
expression entre parenthèses, qui est évaluée pour constituer son /objet de
liaison/, et un bloc de code entre accolades.  Voici un exemple d'utilisation de
~with~:

#+BEGIN_SRC js -n -r
var o = { a: 42 }
with (o) {
  function f() { return a }       (ref:ex7-f)
}
f() //: 42                        (ref:ex7-call)
#+END_SRC

#+ATTR_HTML: :style margin-top:-8rem
#+BEGIN_side-figure
[[file:img/dls9.svg]]

L'environnement crée par ~with~ entoure l'objet de liaison.  Notons que
l'environnement est distinct du l'objet de liaison: ~o~ pointe vers l'objet de
liaison, tandis que ~f.env~ pointe vers l'environnement.
#+END_side-figure

On définit une fonction ~f~ dans le corps de ~with~ à la ligne [[(ex7-f)]].  La
fonction va simplement retourner la valeur de la variable ~a~.  Notons que ~a~
est libre dans ce contexte, mais pourtant l'appel de ~f~ à la ligne [[(ex7-call)]]
retourne 42.  Quand ~f~ est définie, elle capture l'environnement lexical, et il
se trouve que ~with~ crée un environnement lexical pour le code qu'il entoure;
cet environnement est basé sur l'objet de liaison, ~o~.  Puisque ~o~ associe le
nom ~a~ à 42, l'appel de ~f~ va trouver cette association et renvoyer 42.

En voyant cet exemple, on peut se demander si l'effet de ~with~ sur le bloc de
code qu'il entoure n'est pas simplement de la liaison dynamique.  Dans un
langage qui permet la liaison dynamique des fonctions, les variables libres sont
résolues en parcourant les environnements de la pile d'appel, plutôt que de
parcourir l'environnement lexical.  Or, ce n'est pas ce qui se passe ici.  Dans
l'exemple ci-dessous, on crée la fonction ~f~ à la racine, puis on l'appelle à
l'intérieur d'un ~with~, de façon à ce que ~with~ crée un environnement sur la
pile lorsque ~f~ est appelée.  L'appel échoue, car la résolution de noms ne
trouve aucune occurrence de ~a~.  La fonction ~f~ n'est donc pas dynamiquement
liée si elle est appelée dans un ~with~.

#+BEGIN_SRC js
function f() { return a }
var o = { a: 42 }
with (o) { f() } //: ReferenceError: a is undefined
#+END_SRC

#+ATTR_HTML: :style margin-top:-6rem
#+BEGIN_side-figure
[[file:img/dls10.svg]]
#+END_side-figure

Ce qui se passe dans le premier exemple, c'est que lorsque la fonction ~f~ est
définie dans ~with~ elle capture l'environnement lexical crée par ~with~; ~f~
suit donc une liaison statique.  Mais l'environnement généré par ~with~ est
particulier car la résolution de noms dans cet environnement est déléguée à
l'objet de liaison.  Et l'objet de liaison peut être dynamique: on peut y
ajouter ou y supprimer des propriétés.  Donc, on dira que la fonction ~f~
/capture statiquement un environnement dynamique/.  L'environnement ne peut être
capturé qu'à la définition de ~f~, mais cet environnement peut être modifié par
la suite.  Les deux diagrammes illustrent clairement la différence de
comportement: dans le premier, ~f.env~ pointe vers l'environnement crée par
~with~, et dans le second cas, ~f.env~ pointe vers l'environnement global; cette
propriété ~f.env~ ne peut pas être changée dynamiquement, donc appeler ~f~ dans
~with~ n'est pas différent de l'appeler en dehors, ~with~ n'a aucun effet ici.
Ce comportement des fonctions définies à l'intérieur d'un ~with~ est exactement
ce qu'il nous faut pour ouvrir le motif module.

Dans le code qui suit, on utilise ~with~ pour créer l'environnement capturé par
~f~ et ~g~, et obtenir une référence vers cet environnement.  On commence par
créer un objet vide ~E~ ligne [[(ex8-create)]].  Puis on passe cet objet à ~with~,
pour qu'il devienne son objet de liaison.  Dans le corps de ~with~, on déclare
la variable ~a~ et les deux fonctions ~g~ exactement comme dans le module qui
nous intéressait dans la section précédente.  Ligne [[(ex8-return)]] en revanche on
retourne une propriété supplémentaire: l'objet ~E~.  Le premier appel ~m.g(0)~
retourne encore 1, indiquant que ~g~ appelle bien ~f~, qui fait bien référence
au ~a~ défini dans le module.

#+BEGIN_SRC js -n -r
var m = (function(){
  var E = Object.create(null)       (ref:ex8-create)
  with (E) {
    var a = 1
    function f(x) { return x + a }
    function g(x) { return f(x) }
    return { g: g, E: E }           (ref:ex8-return)
  }
}())

m.g(0) //: 1

m.E.a = 2                           (ref:ex8-a)
m.g(0) //: 2
#+END_SRC

Comme on a une référence vers l'objet de liaison ~E~, on redéfinit la valeur de
~a~ ligne [[(ex8-a)]].  L'appel ~m.g(0)~ qui suit retourne 2, ce qui montre que la
fonction ~f~ va bien prendre la valeur de ~a~ dans l'objet ~E~.

#+ATTR_HTML: :style margin-top:-20rem
#+BEGIN_side-figure
[[file:img/dls11.svg]]
#+END_side-figure

On peut se demander pourquoi la valeur de ~m.E.a~ est préférée à la déclaration
de ~a~, alors que cette dernière est /plus proche/ lexicalement de la fonction
~f~.  On a vu que ~with~ crée un environnement pour le code qu'il entoure, mais
cet environnement /ne capture pas/ les déclarations.  Les déclarations de
variables et fonctions sont capturées par l'environnement créé par la fonction
qui enrobe le module—la FIA.  Le diagramme illustre la situation.
L'environnement créé par ~with~ est mis en évidence; il contient l'objet de
liaison.  Dans l'objet de liaison il n'y a que la propriété ~a~ associée à 2.
Les déclarations de ~a~, ~f~ et ~g~ faites dans le module n'existent que dans
l'environnement crée par la FIA.  On voit par contre que ~f~ et ~g~ capturent
l'environnement crée par ~with~, puisque leur propriété ~env~ pointe dessus.
C'est pourquoi la valeur de ~m.E.a~ est trouvée en premier par la résolution de
nom lors de l'exécution de ~f~.

#+ATTR_HTML: :style margin-top:-14rem
#+BEGIN_aside
En fait, le standard ECMAScript 5.1 ne précise pas quel environnement doit
capturer la fonction dans ce cas; c'est laissé à la discrétion du moteur
d'exécution.  SpiderMonkey, le moteur utilisé par Firefox, a le comportement
décrit ici.  Mais V8, utilisé par Chrome, suit des règles différentes, et pour
pouvoir répliquer l'exemple, il faut déclarer les fonctions par des expressions
(~var f = function...~).  Le créateur de JavaScript, Brendan Eich, [[https://bugs.chromium.org/p/v8/issues/detail?id=686][indique]] que
le comportement de Firefox dans ce cas est compatible avec la prochaine version
du standard.  N'ayant pas encore accès à une implémentation complète de ce
standard, j'invite donc le lecteur du futur à tester si ce comportement est
effectivement standard.
#+END_aside

D'après le diagramme, il est clair que si l'objet de liaison est vide, le module
se comportera exactement comme si ~with~ n'était pas utilisé, puisque
l'environnement de ~with~ a pour parent l'environnement de la FIA.  La seule
différence observable serait un léger surcoût dû à l'objet supplémentaire sur la
chaîne de résolution de noms.

Une différence cruciale entre le modèle annoncé dans la section précédente et ce
qu'on obtient ici, c'est qu'ici l'environnement créé par ~with~ et l'objet de
liaison sont deux objets distincts, alors que dans le modèle il ne sont qu'un
seul et même objet.  En particulier, la propriété ~m.E~ pointe vers l'objet de
liaison, tandis que les propriétés ~env~ des fonctions ~f~ et ~g~ pointent vers
l'environnement.  La conséquence de cette distinction est qu'on si peut se
servir de ~m.env~ pour surcharger les définitions faites dans le module, on ne
peut pas s'en servir pour /accéder/ aux valeurs de ces définitions de
l'extérieur du module.  Par exemple, ~m.E.f~ ne fait pas référence au ~f~ de
l'environnement de la FIA; ~m.E.f~ n'est pas défini.

Pour remédier à cette déviation du modèle, on peut créer un objet parent à ~E~.
Cet objet contiendra une référence pour chaque déclaration faite dans le module.
On assigne ensuite cet objet parent au /prototype/ de l'objet de liaison, ~E~.
JavaScript est un langage à prototypes: chaque objet a un lien de prototype vers
un autre objet, ou vers ~null~.  Lorsque la résolution de nom cherche la
propriété ~p~ sur un objet qui ne la contient pas, la recherche continue sur le
prototype, et s'arrête si le prototype est absent.  La recherche de nom dans la
chaîne de prototype d'un objet se comporte donc exactement comme la résolution
de noms de variable dans les environnements.  La solution se présente donc
ainsi:

#+BEGIN_SRC js -n -r
var m = (function(){
  var E = Object.create(null)
  with (E) {
    var a = 1
    function f(x) { return x + a }
    function g(x) { return f(x) }

    Object.setPrototypeOf(E,
      { a: a, f: f, g: g }                       (ref:ex9-parent)
    )
    return { g: g, E: E }
  }
}())

m.g(0) //: 1

m.E.f = function(x) { return x + 2 * m.E.a }     (ref:ex9-f)
m.g(0) //: 2

m.E.a = 2
m.g(0) //: 4

delete m.E.f
m.g(0) //: 2

delete m.E.a
m.g(0) //: 1
#+END_SRC

#+BEGIN_side-figure
[[file:img/dls12.svg]]

On ajoute un objet parent pour contourner la distinction entre l'objet de
liaison et l'environnement crée par ~with~.
#+END_side-figure

On crée un objet parent ligne [[(ex9-parent)]] qui contient une référence pour
chaque déclaration du module, et on assigne cet objet au prototype de ~E~.
Grâce à cet objet, on peut faire référence à ~a~ à travers ~m.E.a~ lors de la
redéfinition de ~f~ ligne [[(ex9-f)]].  Puis, lorsqu'on redéfinit ~a~ à travers
~m.E.a~, cette nouvelle valeur a priorité sur la valeur initiale du module, et
est bien capturée par la nouvelle fonction ~f~.  Tout cela est illustré par le
diagramme.  La propriété ~m.E~ pointe sur l'objet de liaison ({{{color(c1)}}}),
qui contient les redéfinitions de ~a~ et ~f~.  Si l'objet de liaison est vide,
alors on recherche les propriétés sur l'objet parent ({{{color(c2)}}}) avant de
les chercher dans l'environnement de la FIA.  Ainsi, lorsque l'on supprime les
propriétés ~a~ et ~f~, le dernier appel ~m.g(0)~ retourne la même valeur que le
premier, avant les modifications.

L'objet parent duplique en partie le code de l'objet exporté, mais il a
également un effet positif.  Sur cet exemple, la redondance est mineure: les
propriétés ajoutées tiennent sur une ligne.  Ce n'est que dans un programme qui
exporte beaucoup de propriétés que cette redondance devient vexante.  Mais
l'objet parent nous apporte une flexibilité supplémentaire par rapport au
modèle: les propriétés de l'objet parent seront les seules propriétés
accessibles de l'extérieur du module.  L'objet parent nous permet de contrôler
la visibilité des propriétés.

Enfin, puisque la chaîne de prototypes d'un objet JavaScript peut être vue comme
une liste chaînée, on peut directement implémenter l'idée des couches
d'environnements du modèle de la section précédente.  Dans le code suivant, on
définit la fonction ~pushEnv(e, head)~ qui insère l'objet ~e~ comme parent de
~head~; la fonction ~removeEnv(e, head)~ retire l'objet ~e~ de la chaîne,
quelle que soit sa position.  Si on reprend le module ~m~ défini dans l'exemple
précédent, on peut maintenant utiliser des objets JavaScript pour grouper les
modifications que l'on souhaite apporter au module (~e1~ et ~e2~), et utiliser
~pushEnv~ et ~removeEnv~ pour activer ou désactiver ces changements
dynamiquement.

#+BEGIN_SRC js
var m = (function(){ ... }())

function pushEnv(e, head) {
  Object.setPrototypeOf(e, Object.getPrototypeOf(head))
  Object.setPrototypeOf(head, e)
}

function removeEnv(e, head) {
  while (Object.getPrototypeOf(head) != null
      && Object.getPrototypeOf(head) !== e)
    head = Object.getPrototypeOf(head)

  if (Object.getPrototypeOf(head) === e) {
    Object.setPrototypeOf(head, Object.getPrototypeOf(e))
    Object.setPrototypeOf(e, null)
  }
}

m.g(0) //: 1

var e1 = { a: 2, f: function(x) { return x + 2 * m.E.a }}
pushEnv(e1, m.E)
m.g(0) //: 4

var e2 = { f: function() { return -m.E.a }}
pushEnv(e2, m.E)
m.g(0) //: -2

removeEnv(e1, m.E)
m.g(0) //: -1
#+END_SRC

Donc ~with~ permet de réaliser l'idée de l'environnement frontal, et la chaîne
de prototypes de l'objet de liaison de ~with~ permet de disposer les
environnements en couches.  Grâce à ces mécanismes, on parvient à ouvrir le
motif module; on peut changer les définitions internes au module par du code
écrit à l'extérieur.

Cependant, les exemples que l'on a utilisés sont restés très simples.  Il serait
plus intéressant de voir comment on peut ouvrir un module plus réaliste, et
quels sont les problèmes qui apparaissent.  Dans la section suivante, on
applique ces mécanismes à Narcissus.

** Étendre Narcissus par manipulation de portée
Dans cette section, on reprend une version non instrumentée de Narcissus, à
laquelle on va ajouter plusieurs analyses en essayant de minimiser les
changements de code à l'intérieur du module de Narcissus.  Pour ouvrir le module
de Narcissus, il suffit d'ajouter ~with~ autour du corps du code ainsi que
l'objet ~parent~:

#+BEGIN_SRC diff
Narcissus.interpreter = (function() {
+  var _env = Object.create(null);
+  with (_env) {
   ...
   // 1500 lines later
   ...
+  var _parent = {
+    globalBase: globalBase,
+    execute: execute,
+    getValue: getValue,
+    ...
+  };
+  Object.setPrototypeOf(_env, _parent);
   return {
     evaluate: evaluate,
     ...
+    _env: _env,
   };
+  }
}());
#+END_SRC

Les changements n'affectent que le début et la fin du fichier.  Puisque le
module exporte déjà des propriétés, on y ajoute juste ~_env~.  Les analyses qui
nous intéressent nécessitent également d'exposer des propriétés internes au
module à travers l'objet ~_parent~ (nous en avons utilisé 14 en tout dans notre
version instrumentée de Narcissus).  Notons au passage que les propriétés ~_env~
et ~_parent~ sont préfixées d'un tiret bas pour éviter une collision de nom avec
les variables déclarées à la racine par Narcissus.  Mais ces lignes sont les
seules modifications nécessaires à apporter au module pour pouvoir commencer
l'instrumentation.

*** Ajouter l'analyse d'évaluation multi-facettes
On a vu dans l'étude de cas que l'implémentation de l'évaluation multi-facettes
sur Narcissus pouvait être répartie en trois catégories de changements.  La
première catégorie était l'ajout du /program counter/ par extension de l'objet
~ExecutionContext~ et d'arguments supplémentaires à la fonction ~getValue~.  On
peut maintenant effectuer ce changement de l'extérieur du module:

#+BEGIN_SRC js -n -r
var N = Narcissus.interpreter._env                        (ref:ex10-N)

var EC = N.ExecutionContext                               (ref:ex10-EC)
function ExecutionContext(type, version) {
  EC.call(this, type, version)

  this.pc = getPC() || new ProgramCounter()
}

ExecutionContext.prototype = Object.create(EC.prototype)  (ref:ex10-ECproto)

function getPC() {
  var x = EC.current
  return x && x.pc
}

var GV = N.getValue
function getValue(v, pc) {                                (ref:ex10-getV)
  pc = pc || getPC()

  if (v instanceof FacetedValue)
    return derefFacetedValue(v, pc)

  return GV(v)
}

N.ExecutionContext = ExecutionContext                     (ref:ex10-instr1)
N.getValue = getValue                                     (ref:ex10-instr2)
#+END_SRC

On commence par définir un nom raccourci ligne [[(ex10-N)]] pour l'environnement
exporté par Narcissus.  Puis on modifie le constructeur ~ExecutionContext~ après
avoir sauvegardé l'original ligne [[(ex10-EC)]], pour pouvoir y faire référence dans
le nouveau constructeur, mais aussi dans ~getPC~.  Le nouveau constructeur
appelle le constructeur original (équivalent à un appel à ~super~ en Java) et
ajoute la valeur courante du /program counter/ comme propriété ~pc~.  Les autres
propriétés de l'objet ~ExecutionContext~ sont héritées par la ligne
[[(ex10-ECproto)]].  Ligne [[(ex10-getV)]], on définit une nouvelle fonction ~getValue~
qui va remplacer et appeler l'originale, mais qui récupère le /program counter/
courant soit donné en argument, soit par un appel à ~getPC~ si l'argument n'est
pas renseigné.  De cette façon, les appels à ~getValue~ dans l'interpréteur ne
nécessitent pas de modification pour l'argument supplémentaire, mais le code de
l'instrumentation peut appeler ~getValue~ en passant une valeur pour ~pc~.
Enfin, on installe ces modifications dans l'environnement frontal de
l'interpréteur (lignes [[(ex10-instr1)]] et [[(ex10-instr2)]]).

La seconde catégorie de changements concernait l'extension de la fonction
~execute~ pour chaque nœud de l'AST, dans le but de scinder l'évaluation des
valeurs à facettes en appelant la nouvelle fonction ~evaluateEach~ sur chacune
des facettes.  La fonction ~execute~ est essentiellement un long ~switch~ (600
lignes) avec un ~case~ pour chaque type de nœud de l'AST.  On redéfinit la
fonction ~execute~ en suivant la même structure.

#+BEGIN_SRC js +n -r
var EX = N.execute
function execute(n.x) {
  switch (n.type) {
    case IF:
      var cond = N.getValue(N.execute(n.condition, x), x.pc)
      evaluateEach(cond, function(v, x) {
        if (v) N.execute(n.thenPart, x)
        else if (n.elsePart) N.execute(n.elsePart, x)
      }, x)
    break

    ... // other instrumented cases

    default: var v = EX(n, x)                                 (ref:ex11-default)
  }
  return v
}

N.execute = execute                                           (ref:ex11-redef)
#+END_SRC

L'évaluation multi-facettes ne redéfinit le comportement que pour quelques types
de nœuds de l'AST, donc on délègue à la fonction initiale dans le ~default~
ligne [[(ex11-default)]].  En déclarant un ~case IF~, on redéfinit le traitement des
nœuds de type ~IF~ dans l'AST.  Notons qu'ici on appelle récursivement
~N.execute~, et que dans le code de la fonction d'origine, l'appel récursif est
noté ~execute~, mais après la ligne [[(ex11-redef)]], les deux noms pointeront vers
la même fonction: celle que l'on vient de définir.

Enfin, la troisième catégorie de changements regroupait les ajouts de propriétés
à l'objet ~globalBase~.  Il s'agit donc de définir un nouvel objet ~globalBase~
qui hérite de l'objet original, mais auquel on rajoute les propriétés
supplémentaires:

#+BEGIN_SRC js +n -r
var globalBase = Object.create(N.globalBase)

globalBase.isFacetedValue = function(v) {
  return (v instanceof FacetedValue)
}

...  // other properties

N.globalBase = globalBase
#+END_SRC

Mais ce n'est pas suffisant.  Dans le module de Narcissus, l'objet ~globalBase~
est utilisé juste après sa création pour peupler l'objet global du code client.
Ce processus prend place dans le module, avant que les propriétés exportées ne
soient retournées.  Or dans notre façon d'étendre le module, on ne peut modifier
les valeurs qu'après l'export.  Il nous faut donc modifier le code du module
pour pouvoir effectuer ce changement.  Il suffit de fournir une fonction
~populateEnvironment~ qui construit l'objet global à partir de ~globalBase~, et
qui devra être appelée après la construction du module.  Ça nous laisse
maintenant la possibilité d'étendre l'objet ~globalBase~.  Cette modification au
module consiste à déplacer les 30 instructions qui peuplaient l'objet global
dans la fonction ~populateEnvironment~, et à rajouter cette nouvelle fonction
dans les propriétés exportées.

Un dernier changement du module a été nécessaire, plus léger cette fois.  Une
fonction cruciale, la fonction utilisée par le code client pour déclencher un
appel de fonction, était /anonyme/ dans le module.  Sans nom, il n'y avait aucun
moyen de la redéfinir de l'extérieur du module; le motif "module ouvert" ne peut
surcharger que des valeurs identifiées.  Pour pouvoir redéfinir cette fonction,
nous lui avons simplement donné un nom.

*** Ajouter l'analyse de trace
En plus de l'évaluation multi-facettes, on a défini trois autres analyses sur
l'interpréteur: une simple analyse de trace, une analyse de flot d'information,
FlowR, décrite par Pasquier, Bacon et Shand [[cite:PBS-14][PBS-14]], et une analyse qui extrait
les objets et les environnements alloués par l'interpréteur, afin de récolter
les informations pour dessiner les diagrammes de cette section.

L'analyse de trace est la plus simple: il suffit d'intercepter les appels à
~execute~ pour écrire sur la sortie standard quel le type du nœud de l'AST passé
en paramètre, ce qui nous donne l'ordre d'évaluation du programme.  Puisque
~execute~ est appelée récursivement, on indente aussi la sortie suivant la
profondeur de la pile d'appel.  Le simple code:

#+BEGIN_aside
Cette analyse nous a été particulièrement utile pour déboguer certains cas
épineux d'interaction entre les analyses et l'interpréteur.
#+END_aside

#+BEGIN_EXAMPLE
$ cat tests/function.js
function f(a) { return a }
f(1+1)
#+END_EXAMPLE

génère la trace:

#+BEGIN_EXAMPLE
$ ./njs -l trace -f tests/function.js
SCRIPT
  FUNCTION f(a)
  SEMICOLON
    CALL f((1 + 1))
      IDENTIFIER f
      LIST
        PLUS
          NUMBER 1
          NUMBER 1
      SCRIPT
        RETURN
          IDENTIFIER a
#+END_EXAMPLE

Ce qui donne suffisamment d'information pour suivre le chemin emprunté par
l'interpréteur lorsqu'il évalue ce programme.

La nouvelle version de la fonction ~execute~ est simple:

#+BEGIN_SRC js
function execute(proceed, n, x) {
  printIndentation(indentation);
  print(nodetypesToNames[n.type]);

  indentation += indentationStep;
  let ret = proceed(n, x);
  indentation -= indentationStep;

  return ret;
}
#+END_SRC

On instrumente aussi la fonction ~print~, qui peut être utilisée par le code
client et qui pourrait venir se mêler à la trace, puisque les deux écrivent sur
la même sortie.  On pourrait redéfinir le ~print~ client pour écrire sur une
autre sortie (ou la trace), mais ici on a choisi de simplement préfixer les
appels clients de ~print~:

#+BEGIN_SRC js
function printStdout(...args) {
  printIndentation(indentation);
  putstr('#output ');
  print(...args);
}
#+END_SRC

Il suffit ensuite d'insérer ces redéfinitions dans l'environnement du module.
Comme l'insertion est systématique pour définir une analyse, on a défini une
interface d'instrumentation légère, qui permet de masquer les détails de la
manipulation des environnements, et de donner du code plus déclaratif:

#+BEGIN_SRC js
i13n.pushLayer(_env, {
  execute: i13n.around(_env.execute, execute),
  globalBase: i13n.delegate(_env.globalBase,
                            {print: printStdout}),
});
#+END_SRC

#+ATTR_HTML: :style margin-top:-7rem
#+BEGIN_aside
‘i13n' est le numéronyme d'"instrumentation", comme ‘i18n' est le numéronyme
d'"internationalisation"; c'est aussi la convention déjà suivie par
AspectScript.
#+END_aside

La fonction ~i13n.pushLayer~ est un alias de la fonction ~pushEnv~ définie dans
la section [[*Disposer les environnements en couches]].

La fonction ~i13n.around(fn, a)~ retourne simplement une nouvelle fonction qui
appelle la fonction ~a~ en lui passant ~fn~, la fonction d'origine, en premier
argument:

#+BEGIN_SRC js
function around(fn, a) {
  return function(...args) {
    return a(fn, ...args);
  }
}
#+END_SRC

#+ATTR_HTML: :style margin-top:-8rem
#+BEGIN_aside
Après un remplacement de la fonction ~f~ par ~i13n.around(f, g)~ dans un module,
tous les appels à ~f~ dans le module feront appel à ~g~.  L'effet est équivalent
à la réécriture: f(a, b, ...) => g(f, a, b, ...)
#+END_aside

Le nom évoque le /pointcut/ d'AspectJ éponyme; l'effet est proche.  Cependant
ici aucun tissage ne prend place: le code de ~fn~ n'est pas modifié.  On
retourne juste une nouvelle fonction.

L'autre motif récurrent est l'extension d'un objet défini par l'interpréteur.
On pourrait directement ajouter notre nouvelle propriété dans l'objet en
question (ici, ~globalBase~), mais le changement serait irréversible.  Pour que
ce changement soit désactivable par ~i13n.removeLayer~, il faut mettre dans
notre couche d'instrumentation un objet qui contient la nouvelle propriété, et
qui délègue les autres propriétés à l'objet initial.  C'est conceptuellement le
même principe que pour ~around~, mais appliqué aux objets.

Et en JavaScript, il suffit d'utiliser la délégation par prototype pour réaliser
ce motif d'extension:

#+BEGIN_SRC js
function delegate(o, r) {
  var n = Object.create(o)
  Object.assign(n, r)
  return n
}
#+END_SRC

#+ATTR_HTML: :style margin-top:-18rem
#+BEGIN_side-figure
[[file:img/dls13.svg]]

~delegate(o, r)~ retourne un nouvel objet ({{{color(c1)}}}), qui contient les
propriétés de ~r~, et délègue les autres à ~o~.  C'est ce nouvel objet qu'on
insère dans l'environnement du module.
#+END_side-figure

*** Ajouter l'analyse FlowR
Le principe de l'analyse FlowR est d'empêcher des flots de données illicites
dans le programme.  Pour cela, on colle des étiquettes sur les fonctions, comme
"privée" ou "interne", une approche similaire aux /principals/ des valeurs à
facettes.  Il y a deux catégories d'étiquettes: les étiquettes d'/envoi/ et les
étiquettes de /réception/.  Une étiquette d'envoi est propagée aux données
retournées par la fonction, et une étiquette de réception indique quelles
données peuvent être passées en argument à la fonction.  L'algorithme de FlowR
intercepte chaque appel de fonction, vérifie les étiquettes de réception pour
s'assurer que les arguments reçus sont compatibles, puis exécute la fonction, et
propage les étiquettes d'envoi sur la valeur retournée.

#+BEGIN_aside
Cette description est incomplète, car il n'est pas nécessaire de détailler le
fonctionnement de l'analyse pour la suite.  Le lecteur intéressé par ces détails
peut se plonger dans [[cite:PBS-14][PBS-14]].
#+END_aside

Un exemple, tiré de [[cite:PBS-14][PBS-14]], où, après affectation des étiquettes aux objets
~Patient~ et ~PublicData~, le premier appel de méthode est exécuté, mais le
second est interrompu pour cause de flot invalide de données:

#+BEGIN_EXAMPLE
$ cat tests/flowr.js
setTag(Patient, 'receive', 'medical', 1);
setTag(Patient, 'receive', 'default', 0);
// [...] set other tags

var p = new Patient();
var d = new PublicData();

d.add(p.generate_anonymized_record());  // should pass
d.add(p.get_record());                  // should fail
#+END_EXAMPLE

#+BEGIN_EXAMPLE
$ ./njs -l flowr -f tests/flowr.js
Illegal flow: undefined to [object Object]
#+END_EXAMPLE

Pour réaliser cette analyse, il suffit donc de permettre au code client
d'ajouter des étiquettes aux fonctions (via une fonction qu'on appellera
~setTag~), et de capturer chaque appel de fonction afin d'appliquer
l'algorithme.  Narcissus traite l'appel de fonction dans la fonction ~__call__~
de l'objet ~FunctionObject~.  On remplace la fonction ~__call__~ par notre
propre version, qui exécute l'algorithme de FlowR.  Voici notre version:

#+BEGIN_SRC js
function flowrCall(__call__, receiver, args, caller) {

  // [...] check send labels

  let ret = __call__.call(this, receiver, args, caller);

  // [...] propagate labels to return value

  return ret;
}
#+END_SRC

Il n'est pas utile de s'attarder sur le code qui applique l'algorithme; ce sont
de simples appels à des fonctions définies dans le même module.  En revanche, on
peut voir que l'on fait ici appel à la fonction ~__call__~ d'origine, reçue
comme premier argument.  Donc notre ~flowrCall~ pourra être installé sur
~FunctionObjet~ en utilisant la fonction ~i13n.around~ définie plus haut.

L'objet ~FunctionObject~ est l'objet à partir duquel toutes les fonctions du
code client sont instanciées—leur objet /prototype/.  Lorsque l'interpréteur
exécute un appel de fonction, il délègue le travail à la propriété ~__call__~ de
la fonction en question.  Si on installe notre version ~flowrCall~ sur cet
objet, on remplace donc la fonction ~__call__~ sur toutes les instances de
fonctions client.  Mais on ne veut pas modifier directement un objet de
l'interpréteur, car ce serait un changement irréversible.

Comme pour l'analyse de trace, on va créer un nouvel objet qui va contenir notre
nouvelle version ~flowrCall~, et déléguer le reste des propriétés à l'objet
~FunctionObject~ d'origine.  On insère ensuite ce nouvel objet dans une couche,
avec ~i13n.pushLayer~, ce qui permettra de pouvoir le retirer dynamiquement.  Et
comme pour l'analyse de trace, on peut utiliser ~i13n.delegate~ pour cela.

Mais il y a une légère différence ici: ~FunctionObjet~, contrairement à
~globalBase~, est utilisé comme /constructeur/ d'objet (pour créer les fonctions
clientes).  Donc il faut aussi créer notre propre constructeur.  Toute cette
machinerie est mise en place par la fonction ~i13n.override~, qui utilise
simplement ~around~ et ~delegate~ après avoir créé le constructeur:

#+BEGIN_SRC js
function override(base, methods) {
  var newConstr = function(...args) {
    return base.apply(this, args)
  }

  for (var m in methods)
    methods[m] = around(base.prototype[m], methods[m])

  newConstr.prototype = delegate(base.prototype, methods)

  return newConstr
}
#+END_SRC

Et celle-ci nous permet d'installer l'analyse FlowR en quatre lignes:

#+BEGIN_SRC js
i13n.pushLayer(_env, {
  FunctionObject: i13n.override(_env.FunctionObject, {
    __call__: flowrCall }),
  globalBase: i13n.delegate(_env.globalBase, {setTag}),
})
#+END_SRC

*** Ajouter l'analyse d'environnements
Les techniques que l'on propose dans ce chapitre sont basées essentiellement sur
la manipulation d'environnements.  Pour mieux comprendre ces manipulations, on a
utilisé des diagrammes qui représentent les objets et les environnements
créés par l'interpréteur, et leurs relations.  Ces diagrammes sont un modèle
simplifié du fonctionnement de l'interpréteur, mais sont basés sur les
informations récoltées par l'analyse d'environnements.

L'analyse d'environnements est à la fois le dernier exemple d'analyse, mais
aussi un outil qui a servi pour construire les diagrammes de ce chapitre.  On va
donc s'intéresser à son fonctionnement en plus de la façon de l'implémenter sur
l'interpréteur.

L'analyse collecte tous les environnements créés par l'intepréteur, et les
inspecte pour récolter tous les objets créés, en suivant les références des
propriétés, les liens de prototype, et les liens d'envrionnement parent.

Mais tous les objets ne nous intéressent pas de la même façon.  De nombreux
objets font partie de l'API standard offerte par le langage, comme ~Array~ ou
~String~.  Ce n'est pas nécessaire de voir toutes leurs fonctions, toutes leurs
propriétés, quand on s'intéresse au fonctionnement de ~with~.  On peut donc
paramétrer le collecte d'objet par:

- une liste noire, qui indique quels objets il n'est pas nécessaire d'inspecter.
  Si l'analyse récolte un objet présent dans la liste noire, ses propriétés ne
  sont pas collectées récursivement.  Typiquement, ~Object.prototype~ est un bon
  candidat pour la liste noire;

- une liste blanche, qui permet d'inspecter seulement les propriétés nommées
  dans un objet, et d'ignorer les autres.  Quand on sait quel objet nous
  intéresse, cette liste permet de réduire considérablement le bruit dans le
  graphe créé.  Dans l'exemple ci dessous, seule la propriété ~m~ de l'objet
  global est inspectée;

- la profondeur de recherche, qui limite le nombre d'appels récursifs quand
  l'analyse suit les références d'un environnement.  Au delà, les objets sont
  "opaques": leurs propriétés ne sont pas affichées.  La boîte dessinée contient
  juste un identifiant unique.

Sur cet exemple d'utilisation de ~with~, que l'on a vu de nombreuses fois dans
ce chapitre:

#+BEGIN_EXAMPLE
$ cat tests/with.js
var m = (function() {
  var E = {};
  with (E) {
    var a = 1;
    return {E:E};
  }
}());

m.E.a = 2;

// [...] set up whitelist and blacklist of objects

printScope(2, whitelist, blacklist);
#+END_EXAMPLE

L'analyse produit un graphe au format [[https://en.wikipedia.org/wiki/DOT_(graph_description_language)][DOT]] utilisé par GraphViz.  On peut donc
produire une image en invoquant ~dot~:

#+BEGIN_EXAMPLE
$ ./njs -l scope -f tests/with.js | dot -Tsvg
#+END_EXAMPLE

#+CAPTION: Les flêches en pointillés indiquent les liens de prototype.  Les
#+CAPTION: flêches solides sont des références.  On peut remarquer en haut à
#+CAPTION: gauche les environnements, qui font référence aux objets créés par
#+CAPTION: le code de l'exemple.
[[file:img/env-with.svg]]

Le rendu est un peu brouillon, mais toute l'information est là.  On peut y
retrouver l'objet ~E~, l'objet d'activation de la FIA, l'environnement créé par
~with~.  Et en faisant quelques simplifications, et en réorganisant les boîtes
manuellement, on obtient les diagrammes du chapitre.

Pour collecter les environnements, il suffit de se greffer dans la fonction
~execute~ de l'interpréteur, et de conserver les environnements dans un
ensemble.

#+BEGIN_SRC js
let scopeObjects = new Set();

function harvestScopeObject(n, x) {
  scopeObjects.add(x.scope);
}

i13n.pushLayer(_env, {
  execute: i13n.before(_env.execute, harvestScopeObject),
  globalBase: i13n.delegate(_env.globalBase, {printScope}),
});
#+END_SRC

On utilise cette fois ~i13n.before~ plutôt que ~around~, qui fait aussi écho au
/pointcut/ d'AspectJ.  La fonction ~harvestScopeObject~ sera invoquée avant
chaque appel d'~execute~, et récoltera l'environnement courant.

La fonction ~printScope~, que l'on rajoute dans l'environnement client, va
d'abord construire le graphe en parcourant tous les objets récoltés, puis
produire le fichier au format DOT.

*** Évaluation
Notre instrumentation de Narcissus pour l'évaluation multi-facettes consiste en
440 lignes de code, dans un seul fichier séparé de l'interpréteur;
l'instrumentation faite par Austin et Flanagan comportait 640 lignes mêlées au
code de l'interpréteur.  Notre version factorise quelques changements, ce qui
explique la différence.  L'intérêt d'avoir tous les changements dans un seul
fichier à part c'est qu'on peut les étudier et avoir une idée de comment
l'évaluation multi-facettes vient modifier l'interpréteur.  On peut par exemple
comparer le code de l'instrumentation avec la sémantique de référence de [[cite:AF-12][AF-12]].

Pour ajouter l'évaluation multi-facettes, on a ajouté 19 lignes à Narcissus pour
ouvrir le module avec ~with~, et changé 32 lignes pour ajouter la fonction
~populateEnvironment~ et nommer la fonction anonyme.  En tout 51 lignes sur 1300
sont touchées, à comparer avec les 640 lignes de l'instrumentation d'Austin et
Flanagan.  Mais même si les changements sont restreints, il est important de
noter qu'il était tout de même nécessaire de modifier le code de Narcissus.  Les
changements, même mineurs, ont nécessité d'inspecter et de comprendre les
différentes étapes de construction de l'interpréteur.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_aside
Bien sûr, on aurait pu écrire un outil pour modifier automatiquement les lignes
de Narcissus qui nous intéressent, et considérer ainsi qu'aucune ligne de
Narcissus n'a été modifiée directement.  Mais non seulement ça ne change pas
fondamentalement la situation (le code doit être modifié), ce serait en plus
une transformation fragile car basée sur la syntaxe du code plutôt que sur sa
sémantique.
#+END_aside

En plus de l'évaluation multi-facettes, on a défini trois autres analyses sur
l'interpréteur: une analyse de flot d'information, une simple analyse de trace,
et une analyse qui extrait les environnements alloués par l'interpréteur.  On a
pu écrire ces trois analyses dans des fichiers séparés, sans avoir besoin de
modifier le code de l'interpréteur.  Qui plus est, on a pu extraire des motifs
communs d'instrumentation de fonctions (~around~, ~before~), d'objets
(~delegate~), ou les deux (~override~).

En utilisant la technique de disposition en couches de la section [[*Disposer
les environnements en couches]], on peut activer plusieurs analyses à
l'exécution avec des options de lancement.  Par exemple:

: ./njs -l flowr -l trace -f tests/flowr.js

lance l'interpréteur, active l'analyse FlowR, et active aussi la trace avant
d'exécuter le code du fichier "tests/flowr.js".  Un extrait de la sortie montre
que les deux analyses fonctionnent en même temps:

#+BEGIN_EXAMPLE
    CALL d.add(p.get_record())
      DOT d.add
        IDENTIFIER d
      LIST
        CALL p.get_record()
          DOT p.get_record
            IDENTIFIER p
          LIST
          SCRIPT
Illegal flow: undefined to [object Object]
#+END_EXAMPLE

Le ~Illegal flow~ est produit par FlowR, et les autres lignes par la trace.
L'union des deux sorties permet de visualiser à quel moment de l'interprétation
les flots non autorisés se produisent, sans avoir à rajouter du code spécifique
à la trace dans l'analyse FlowR.

La trace ne modifie qu'une seule fonction de l'interpréteur, et cette
modification est compatible avec les modifications apportées par FlowR.
Intuitivement, on voit bien que la trace est orthogonale au contrôle d'accès
effectué par FlowR.  C'est pourquoi les deux analyses peuvent être activées en
même temps sans que l'on ait besoin de rajouter du code spécifique à
l'intégration des deux.  Ce n'est pas le cas pour l'évaluation multi-facettes et
FlowR.  L'évaluation multi-facettes modifie les /valeurs/ manipulées par
l'interpréteur, et FlowR rajoute des /labels/ sur ces valeurs.  Du coup, les
deux analyses touchent à la représentation des valeurs dans l'interpréteur.  En
regardant leur spécification, on ne peut pas dire clairement comment les deux
analyses doivent se combiner, ni ce que le résultat d'une évaluation devrait
produire.  Il faut donc retenir que la disposition en couches permet de
facilement activer plusieurs modifications de l'interpréteur en même temps, mais
que le résultat n'a de sens que si ces modifications n'interfèrent pas entre
elles.

#+ATTR_HTML: :style margin-top:-4rem
#+BEGIN_aside
Comment détecter et résoudre ces interférences reste un problème ouvert.
#+END_aside

Pour s'assurer qu'appliquer ~with~ pour ouvrir Narcissus n'a pas changé son
fonctionnement, nous avons comparé le comportement de la version d'origine et la
version avec ~with~ sur la suite de tests ~test262~.  Cette suite contient plus
de 11000 tests de conformité de la spécification ECMAScript 5.1.  Nous avons
utilisé un sous-ensemble de cette suite, utilisé pour tester le moteur
JavaScript SpiderMonkey de Mozilla.  Sur environ 3300 tests validés par
SpiderMonkey, Narcissus en passe 2600.  Nous avons donc lancé la version
originale de Narcissus et la version avec ~with~ sur ces 3300 tests.  Pour la
version avec ~with~, aucune analyse n'était activée: l'objet de liaison restait
donc vide.  Pour chaque test, notre script comparait les sorties des deux
versions; un test réussissait seulement si les deux sorties étaient identiques.
Au final, les deux versions ont un comportement strictement identique sur ce jeu
de tests.

Nous avons aussi pu comparer Narcissus avec et sans l'évaluation multi-facettes
sur ce même jeu de tests.  Lorsque le /program counter/ de l'évaluation
multi-facettes est vide, l'interpréteur se comporte de façon identique sur les
tests considérés.  Un résultat démontré par Austin et Flanagan sur la sémantique
de l'évaluation basée sur un lambda-calcul, mais pas sur l'implémentation basée
sur le langage JavaScript.

En exécutant les tests, nous avons aussi mesuré le temps d'exécution de chaque
version.  Pour chaque version, le jeu de test a été lancé dix fois, sur un seul
thread, toujours sur la même machine.  Le tableau donne la moyenne arithmétique
du temps pour une exécution complète sur ces dix lancers.

#+BEGIN_aside
Les tests ont été effectué sur un PC portable contenant un Intel i5-3320M
2.60GHz et 4Gb de RAM.  Narcissus a été lancé à travers une [[https://ftp.mozilla.org/pub/firefox/nightly/latest-mozilla-central/jsshell-linux-x86_64.zip][version de
développement]] de SpiderMonkey (version JavaScript-C36.0a1).  Les tests
proviennent de la version de développement de [[https://mxr.mozilla.org/mozilla-central/source/js/src/tests/][mozilla-central]] et ont été
exécutés par le script ~jstests.py~ développé par Mozilla.
#+END_aside

| Interpréteur                                     | Temps moyen (secondes) |
|--------------------------------------------------+------------------------|
| SpiderMonkey                                     | 89 (± 1)               |
| Narcissus original                               | 1040 (± 1)             |
| Narcissus avec ~with~                            | 1218 (± 10)            |
| Narcissus et multi-facettes (Austin et Flanagan) | 1215 (± 2)             |
| Narcissus et multi-facettes avec ~with~          | 1301 (± 1)             |
|--------------------------------------------------+------------------------|

#+BEGIN_aside
En moyenne, un test prend donc environ 0.5 seconde à s'exécuter.  Comme
l'interpréteur est relancé pour chaque test, et que, dans le cas de Narcissus, il
faut /évaluer/ tout l'intepréteur avant de pouvoir exécuter le test, ce temps de
lancement se rajoute à chaque test.  Sur la même machine, lancer Narcissus sur
un fichier vide prend 0.07 seconde en moyenne.  Activer les différentes analyses
ne change pas cette mesure.
#+END_aside

L'écart type est à moins de 1% pour toutes les versions de Narcissus, donc les
temps moyens sont représentatifs.  Si on compare l'interpréteur non instrumenté
avec la version où on ajoute juste ~with~ autour du code du module, la seconde
version prend 17% de temps supplémentaire pour exécuter le jeu de tests.  Ce
surcoût est probablement dû à l'utilisation de ~with~, qui rajoute un
environnement dans la chaîne de résolution des noms.  Non seulement ça fait un
objet de plus à parcourir lors de la résolution de /n'importe quel nom/ du
module, mais en plus les fonctions qui ont des variables libres ne peuvent pas
être optimisées par le compilateur.  Là où, sans ~with~, un accès à une variable
libre peut être résolu statiquement en regardant dans l'environnement lexical de
la fonction, avec ~with~ la variable peut changer dynamiquement, donc le moteur
d'exécution doit toujours résoudre le nom dynamiquement; aucun raccourci n'est
possible.

#+BEGIN_aside
Et il faut aussi comprendre que ~with~ est un mot-clé du langage qui est
rarement utilisé, il est donc raisonnable que les développeurs de moteurs
d'exécution ne cherchent pas à l'optimiser davantage.
#+END_aside

Si on compare l'interpréteur instrumenté par Austin et Flanagan pour
l'évaluation multi-facettes, le surcoût de temps n'est plus que 7%.  Notons
cependant que notre implémentation de l'évaluation multi-facettes factorise
quelques changements par rapport à l'implémentation de base, on ne mesure pas
seulement l'utilisation de ~with~.

De toute façon, comme l'indique le tableau, exécuter n'importe quel exemple avec
Narcissus est d'un ordre de grandeur plus lent qu'avec SpiderMonkey directement.
C'est évidemment dû au fait que Narcissus est écrit en JavaScript.  Mais pour
nos besoins, les plus gros exemples qu'on a voulu exécuter avec les différentes
analyses ajoutées à Narcissus n'ont pris que quelques secondes.  À cette
échelle, une différence de 17% est insignifiante.

Le gain de temps en écriture de code et en clarté de l'instrumentation est,
quant à lui, tangible.  Avant de pouvoir exécuter la suite ~test262~ avec
Narcissus, il a fallu corriger quelques erreurs dans son interprétation du code
JavaScript et compléter sa couverture du standard.  En corrigeant ces erreurs
dans l'interpréteur d'origine, il a fallu dupliquer les correctifs dans la
version instrumentée par Austin et Flanagan, mais pas dans notre
instrumentation.  Notre version de l'évaluation multi-facettes ne duplique pas
le code de l'interpréteur, donc une fois le correctif appliqué à l'interpréteur
de base, l'évaluation multi-facettes bénéficiait du correctif automatiquement.
Cette anecdote illustre l'intérêt d'une décomposition qui cherche à minimiser la
duplication de code.

** Discussion
L'idée principale de ce chapitre est que la manipulation de la portée des
variables d'un module est suffisante pour changer totalement le comportement de
ce module.  Et si on peut changer intégralement le comportement du module, on
peut nécessairement en changer une partie; c'est ce qui nous a permis de
modifier Narcissus pour accepter plusieurs analyses dynamiquement.

*** La tension entre flexibilité et sûreté
On peut voir la propriété ~E~ qui expose l'environnement manipulable du module
comme une interface spéciale, une interface d'instrumentation, qu'on peut
rapprocher du concept d'"implémentation ouverte" de [[cite:Kic-96][Kic-96]].  Cette interface
spéciale expose /tous/ les noms de l'interpréteur, et confère donc aux
programmeurs non seulement le pouvoir de modifier le code du module, mais
également de casser ce code en créant des situations non anticipées par le
créateur du module.  Comme on a pu changer la fonction ~execute~ pour étendre sa
fonctionnalité, on aurait pu tout aussi bien fournir une version qui lance une
exception.  Pas très utile dans notre cas, mais ça montre qu'il est très facile
de rompre la fonctionnalité offerte par le module dès lors que cette interface
spéciale est offerte.

#+ATTR_HTML: :class todo
#+BEGIN_aside
Open scope makes it /extensible/, which it wasn't before.  The paradox is that
module decomposition goes against extensibility instead of enabling it.
Hypothesis modularity => extensibility should be made clearer in Context.
#+END_aside

On a alors une situation quelque peu paradoxale où le programmeur soucieux de
l'intégrité de son code le ficelle dans un module, mais ce faisant empêche tout
autre programmeur d'étendre son code dynamiquement.  Le code a beau utiliser un
module, il ne peut être étendu; il n'est donc pas /modulaire/ par rapport aux
changements que l'on souhaite y apporter, car il faut modifier le code source à
l'intérieur du module pour effectuer ces changements.  Et à l'inverse, la
solution proposée ici casse la protection offerte par le module, mais ce faisant
permet d'étendre le code en écrivant les changements et fonctionnalités
supplémentaires dans des fichiers distincts; une situation que l'on peut
qualifier de /modulaire/.

La version originale du module, et notre version modifiable dynamiquement sont
deux points opposés sur un même axe.  D'un côté, un programme prévisible, sur
lequel il est facile de raisonner statiquement, car son comportement ne peut
être modifié dynamiquement; et de l'autre, un programme ouvert, dont le
comportement peut changer à tout moment de l'exécution, mais qui offre une
grande flexibilité.  Les propriétés offertes par l'un et l'autre sont toutes
désirables: a priori, on préférerait avoir un programme prévisible et
reconfigurable à souhait.  Mais il est facile de voir que les deux buts sont
antagonistes.

Prenons une simple fermeture:

#+BEGIN_SRC js
const a = 1
function f() { return a }
#+END_SRC

À voir le code, on peut déclarer que lorsqu'on appelle ~f~, elle renvoie
toujours 1.  C'est une fermeture sur ~a~, et la valeur de ~a~ ne peut pas être
modifiée car ~a~ est constante.  "~f~ renvoie 1" est donc un invariant qui est
utile pour le programmeur, une garantie du code qui permet de mieux comprendre
les comportement des sites d'appels de ~f~.

Mais en même temps, cette garantie rend la fonction inflexible.  On ne peut pas
changer le comportement de ~f~ sans violer cette garantie.  Si on applique la
technique de ce chapitre pour ouvrir la fonction avec ~with~:

#+BEGIN_SRC js
const a = 1
var E = {}
with (E) {
  function f() { return a }
}

f() //: 1

E.a = 2
f() //: 2
#+END_SRC

alors maintenant on peut modifier la valeur de retour de ~f~, car on peut
modifier ~a~.  C'est donc que la garantie ne tient plus.  Tout ce qu'on peut
dire de ~f~ c'est quelle retourne la valeur de ~a~ contenue dans ~E~, si elle
existe, et 1 sinon.  Mais comme on ne sait rien de la valeur de ~a~ contenue
dans ~E~, autant dire que ~f~ renvoie n'importe quoi.  On n'a plus aucune
garantie sur ~f~, mais une flexibilité complète pour la modifier.

On pourrait retrouver certaines garanties.  Par exemple, si on veut que ~a~ soit
toujours un nombre plus petit que 10, on peut protéger l'objet ~E~ et renvoyer
un proxy:

#+BEGIN_SRC js
const a = 1

var m = (function() {
  var E = {}
  with (E) { function f() { return a } }

  return {f,
    E: {
      set a(v) {
        if (typeof v === 'number' && v < 10)
          E.a = v
      }
  }}
}())

m.f() //: 1

m.E.a = 2
m.f() //: 2

m.E.a = "44"
m.f() //: 2
#+END_SRC

De cette façon, on retrouve une garantie: ~m.f~ renverra toujours un nombre plus
petit que 10.  Mais en échange, on perd en flexibilité, puisque ~a~ doit obéir à
cette contrainte.  Dès lors qu'on ajoute une contrainte, comme ~a < 10~, la
négation de cette contrainte n'est plus un cas possible pour le programme.
Chaque contrainte restreint le programme, et assure ainsi des garanties
supplémentaires, mais chaque contrainte exclut également des possibilités.

#+ATTR_HTML: :style margin-top:-10rem
#+BEGIN_side-figure
[[file:img/dls14.svg]]

Imposer une contrainte sur une variable, ce n'est pas seulement définir
l'ensemble de ses valeurs /possibles/, c'est aussi définir l'ensemble des
valeurs /exclues/.
#+END_side-figure

Si on ne peut pas complètement réconcilier la flexibilité dynamique et la
garantie statique, on peut /choisir/ le compromis entre ces deux extrêmes.  Il y
a toute une dimension entre le motif module, qui fixe les définitions, et
l'utilisation de ~with~ dans le module, qui permet de les changer.  C'est au
programmeur de choisir le compromis le mieux adapté au problème.

*** D'autres façons d'étendre l'interpréteur
Il y a d'autres façons, plus évidentes, d'ouvrir le module.  Pour commencer,
puisqu'on cherche à briser la protection offerte par la fonction anonyme,
pourquoi ne pas l'enlever?  Et définir nos fonctions directement à la racine:

#+BEGIN_SRC js
var a = 1
function f(x) { return x + a }
function g(x) { return f(x) }

g(0) //: 1
#+END_SRC

Ainsi, on peut redéfinir les fonctions et les valeurs comme on le souhaite, et
même dans des fichiers séparés, puisque toutes les définitions résident dans le
même environnement.

#+BEGIN_SRC js
var a = 1
function f(x) { return x + a }
function g(x) { return f(x) }

g(0) //: 1

function f(x) { return 2 }
g(0) //: 2
#+END_SRC

Mais utiliser ~with~ pour ouvrir le module n'anéantit pas toutes les protections
offertes par la FIA.  Avec ~with~, on bénéficie encore de l'espace de nommage
fourni par la fonction: toutes les définitions sont exportées dans un objet ~m~,
ou ~Narcissus.interpreter~, ce qui réduit les chances d'une redéfinition
accidentelle.  Si on met toutes les définitions à la racine, alors on perd cet
avantage.

On peut utiliser un objet pour protéger les définitions mais garder la
possibilité de les modifier:

#+BEGIN_SRC js
var m = {
  a: 1,
  f(x) { return x + m.a },
  g(x) { return m.f(x) },
}

m.g(0) //: 1

m.f = function(x) { return 2 }
m.g(0) //: 2
#+END_SRC

Ça demande de toujours préfixer les références aux définitions du module par
l'identifiant de l'objet (ici, ~m~).  Pour cet exemple, ça ne fait que deux
caractères en plus.  Mais pour modifier un module existant, comme Narcissus, il
faudrait préfixer des centaines de références.  Et cette tâche ne peut pas être
automatisée à tout coup, puisque le langage nous permet d'accéder à des
références indirectement (~m["f"]~).  C'est donc une solution moins pratique que
de juste ajouter ~with~ autour du code.

Une autre solution, plus classique, serait de commencer par établir les points
d'extension nécessaires pour les différentes analyses qu'on envisage d'utiliser
(la fonction ~execute~, par exemple), puis de rajouter un mécanismes explicite
d'appel aux analyses quand l'interpréteur entre dans ces points.  On pourrait
utiliser des /hooks/, ou le patron Observateur, ou encore accepter un Visiteur;
les trois méthodes permettent d'appeler du code extérieur au module à un moment
précis du flot d'exécution.

#+BEGIN_side-figure
[[file:img/dls15.svg]]

Un /hook/ est une liste de fonctions associée à un moment précis du flot de
contrôle d'un module.  Le code extérieur au module ajoute des fonctions dans
cette liste.  Lorsque le code du module atteint le moment précis, il appelle les
fonctions du /hook/.  C'est une façon courante d'injecter du code dans la
configuration d'Emacs [ref-emacs-hooks] et de LaTeX [ref-latex-hooks].
#+END_side-figure

Avec des /hooks/, ou un Observateur, il faut rajouter du code à l'intérieur du
module à tous les endroits où le programmeur d'un module externe voudrait
injecter du code, pour appeler /explicitement/ soit une liste de fonctions, soit
une liste d'observateurs.  Le code externe doit à son tour déclarer quelles
fonctions ou quel observateur il souhaite ajouter dans l'interpréteur.  Le
transfert de contrôle est explicite: il suffit de suivre le code dans
l'interpréteur pour voir les appels à des /hooks/, ou une levée d'événements
pour des observateurs.  L'inconvénient est d'avoir à identifier les endroits qui
peuvent intéresser du code externe, et d'ajouter des /hooks/ explicitement dans
le code pour chacun d'eux.

On utilise généralement ces techniques pour appeler du code qui /ne change pas/
le flot de contrôle du module.  Une fonction appelée par un /hook/ n'effectue
que des effets de bord (affichage sur la sortie standard), ou modifie des
valeurs de son propre module; le processus de l'interpréteur n'est pas impacté.
Ce serait donc une technique suffisante pour implémenter l'analyse de trace ou
d'environnement, qui ne font qu'observer la descente récursive de
l'interpréteur, mais l'analyse multi-facettes et l'analyse de flot requièrent de
changer le résultat.

Le patron Visiteur permettrait de changer le résultat de l'interpréteur.  Si on
suit ce patron, il faut réécrire l'interpréteur pour appeler, pour chaque nœud
de l'AST, la méthode correspondante de l'objet visiteur.  Cet objet est passé à
l'interpréteur avant l'évaluation de code, et c'est ce visiteur qui contient la
logique d'évaluation.  L'évaluation du code JavaScript standard devrait donc
être mise dans un tel objet, pour obéir à l'interface du visiteur.  Et les
analyses pourraient alors également obéir à cette interface, et réutiliser le
code de l'évaluation standard par simple délégation.

Ici encore, le flot de contrôle est explicite: toute la logique est déléguée au
visiteur.  L'inconvénient majeur est qu'il faut d'abord établir une interface de
visiteur qui convienne à tous les besoins des différentes analyses, puis
complètement réécrire l'interpréteur pour se conformer à cette interface.

Toutes ces techniques demandent donc de modifier l'interpréteur en profondeur,
alors qu'on a vu que ~with~ suffisait pour redéfinir n'importe quelle fonction
de l'interpréteur, sans avoir à définir d'interface au préalable, et en
n'ajoutant que quelques lignes.

Une différence majeure néanmoins est que si le module utilise un hook, un
observateur, ou un visiteur, alors les extensions de ce module (les analyses
dans le cas de l'interpréteur) doivent nécessairement passer par l'interface
d'extension définie par l'interpréteur.  Avec ~with~, ce sont les extensions
elles-mêmes qui peuvent redéfinir n'importe quelle fonction du module; comme si
l'interface d'extension était totale, mais sans avoir à le déclarer
explicitement.  Avec ~with~, le programmeur des extensions a la responsabilité
de maintenir le lien qui lui permet d'inclure son code dans le processus de
l'interpréteur (les appels à ~around~, ~override~, ~delegate~, ...); le
programmeur de l'interpréteur ne peut pas savoir où les extensions vont se
greffer, il n'a pas à s'en soucier, d'où moins de code à maintenir.

# This makes me think of Éric's talk at AOSD'13, with joinpoint interfaces where
# you can say at the class level which joinpoints are viewed by outside aspects,
# and which joinpoints are hidden.
# export * except a,b  (intension)
# export c,d,f         (extension)

*** Pourquoi ne pas utiliser la programmation par aspects?
Étendre un module par du code extérieur sans que le code du module n'expose
d'interface pour, activer et désactiver les extensions dynamiquement, les
combiner ... ces objectifs rappellent fortement la programmation par aspects.

Comme on l'a vu dans les différentes analyses ajoutées à Narcissus, on utilise le
vocabulaire d'AspectJ: ~around~, ~before~, ~proceed~.  Donc pourquoi ne pas
utiliser directement la programmation par aspects, qui est un paradigme général,
plutôt que de passer par ~with~ et les prototypes, qui sont des mécanismes
propres à JavaScript?

En utilisant une bibliothèque d'aspect pour JavaScript, on pourrait définir
chaque analyse dans son propre fichier, puis l'intégrer dynamiquement à
l'exécution de l'interpréteur avec des méthodes d'aspect qui ciblent les
jonctions adéquates (l'appel de ~execute~).  On pourrait activer et désactiver
les méthodes d'aspect pour activer ou désactiver dynamiquement les analyses.  On
aurait effectivement le même résultat, mais exprimé en terme d'aspects; une
technique qui pourrait plus facilement se transporter à tout langage disposant
d'une bibliothèque de programmation par aspects.

Idéalement.

On a initialement cherché à utiliser une bibliothèque d'aspects pour
instrumenter Narcissus: AspectScript est la plus complète [[cite:TLT-10][TLT-10]].  Il y a deux
raisons majeures de préférer l'approche de ce chapitre à AspectScript pour
l'instrumentation de Narcissus: la compatibilité et la performance.

AspectScript est une bibliothèque d'aspects qui réifie /tout le code/ qui doit
être instrumenté.  Pour qu'un module puisse émettre des jonctions, il faut
l'envoyer à AspectScript, qui va en faire l'analyse syntaxique, et émettre du
code qui réifie les déclarations, les appels de fonction, les affectations de
variable, toutes les constructions du langage en appels explicites à
AspectScript.  C'est comme ça qu'AspectScript peut capturer n'importe quel
évènement du module.

AspectScript transforme ce module:

#+BEGIN_SRC js
var m = (function() {
  var a = 1;
  function f(x) { return x + a; }
  function g(x) { return f(x); }
  return {g: g, f: f};
}());
#+END_SRC

en ce code:

#+BEGIN_SRC js
var m = AS.i13n.propWrite(
  AS.globalObject, "m",
  (AS.i13n.call(
    AS.globalObject,
    AS.i13n.wrap(function(){return (function (){
      {arguments.callee = arguments.callee.wrapper;
       var f = AS.i13n.varWrite(
         "f",
         AS.i13n.wrap(function(){return (function f(x){
           {var f = arguments.callee
                  = arguments.callee.wrapper;
            return (AS.i13n.varRead("x",x))
                   + (AS.i13n.varRead("a",a));
           }})}),undefined);
       var g = AS.i13n.varWrite(
         "g",
         AS.i13n.wrap(function(){return (function g(x){
           {var g = arguments.callee
                  = arguments.callee.wrapper;
            return AS.i13n.call(
              AS.globalObject,
              (AS.i13n.varRead("f",f)),
              [(AS.i13n.varRead("x",x))],this);
           }})}),undefined);
       var a = (AS.i13n.varWrite("a", 1, undefined));
       return AS.i13n.creation2(
         function(){
           var $__this__=arguments[0];
           arguments=arguments[1];
           AS.i13n.propWrite($__this__, "g",
                             (AS.i13n.varRead("g",g)));
           AS.i13n.propWrite($__this__, "f",
                             (AS.i13n.varRead("f",f)));
         },this,arguments);
      }})}),[],this)));
#+END_SRC

En somme, AspectScript /interprète/ le code du module, délègue l'évaluation au
moteur d'exécution sous-jacent.  Au passage, il émet des /joinpoint/ pour chaque
construction, teste les /pointcut/ pour tous les /advice/ enregistrés, et
exécute le code si le /pointcut/ correspond.

Ce mode de fonctionnement a deux inconvénients.  D'abord, AspectScript duplique
évidemment le travail du moteur d'exécution de JavaScript, ce qui est
inefficace, mais surtout qui peut changer la sémantique du programme
instrumenté.  Puisqu'AspectScript interprète le code du module ciblé, et délègue
l'évaluation au moteur d'exécution, on n'a aucune garantie qu'un appel de
fonction capturé par AspectScript obéit aux mêmes invariants qu'un appel de
fonction standard.  Quand on instrumente un module aussi large que Narcissus, on
cherche à éviter d'introduire des erreurs subtiles dans du code qu'on ne
maîtrise pas.  Mais comme AspectScript ajoute une étape d'interprétation
supplémentaire, si on cherche à comprendre un bogue de Narcissus il faut aussi
prendre en compte l'implémentation d'AspectScript.

#+ATTR_HTML: :style margin-top:-6rem
#+BEGIN_aside
Exemple de déviation de l'évaluation standard de JavaScript: AspectScript ne
peut pas instrumenter du code qui contient ~with~.
#+END_aside

#+CAPTION: Le code invoqué dans AspectScript pour un appel réifié de fonction.
#+BEGIN_SRC js
call: function (obj, fun, args, context){
  acChecker.checkCall(obj, fun);

  if(!weavingNeeded()){
    return fun.apply(obj, args);
  }

  var call = jpPool.get(JP_CALL, obj, fun, args,
                        context, currentJoinPoint);
  if(systemObjects.indexOf(obj) >= 0){
    try{
      return call._proceed();
    }
    finally{
      jpPool.release(call);
    }
  }

  return weave(call);
},
#+END_SRC

Le second inconvénient est que pour faire l'analyse syntaxique du code,
AspectScript utilise son propre analyseur, ce qui encore une fois peut
introduire des déviations par rapport à l'analyseur du moteur d'exécution.
JavaScript est un langage en évolution, et l'analyseur d'AspectScript ne suit
pas cette évolution.  Du code qui utilise des extensions de syntaxe des
dernières versions de JavaScript s'exécutera correctement sur le navigateur,
mais pas s'il est instrumenté par AspectScript.

Parce qu'AspectScript réinterprète le code, et utilise son propre analyseur, il
pose des problèmes de compatibilité.  Mais ces mécanismes ont également un coût
important en temps d'exécution et espace mémoire.  AspectScript est 5 à 15 fois
plus lent qu'un moteur d'exécution de JavaScript.  Sachant qu'on cherche ici à
instrumenter un interpréteur de JavaScript, et qu'on perd déjà un ordre de
grandeur de performance en utilisant un interpréteur métacirculaire, rajouter
une couche d'interprétation d'AspectScript n'est pas idéal, surtout s'il y a
une alternative qui nous permet d'étendre l'interpréteur avec la même
flexibilité à un coût de performance moindre.

La programmation par aspects est donc un bon candidat pour le scénario
d'instrumentation de Narcissus, mais l'implémentation d'AspectScript n'est pas
une solution adéquate pour des raisons techniques.  Une autre implémentation
d'aspects pour JavaScript existe dans la littérature [[cite:LVG-10][LVG-10]], mais elle ne
cible qu'une version interne modifiée du navigateur Internet Explorer.

*** Appliquer la manipulation de portée à d'autres langages
On n'a pas utilisé une bibliothèque d'aspects, mais l'approche de ce chapitre est
certainement ancrée dans le paradigme de la programmation par aspects.  Et c'est
justement intéressant de voir que la simple manipulation de la portée des
variables /suffit/ pour obtenir la flexibilité souhaitée pour ajouter des
analyses à l'interpréteur.

AspectJ fournit des coupes bien plus riches que celles qu'on a utilisées ici.
On n'a pas eu besoin d'intercepter des appels de fonction en testant les
paramètres (le pointcut ~args~), ni de connaître l'état de la pile d'appels
(~cflow~).  Les fonctions que l'on a emprunté à AspectJ, ~around~, ~before~,
sont triviales, définies en 3 lignes.

Le seul mécanisme dont on a eu besoin existait déjà en JavaScript: la
possibilité d'intercepter la résolution d'un nom (de variable ou de fonction)
via ~with~.  Autrement dit, la manipulation de la portée des variables suffit
pour instrumenter Narcissus de façon modulaire.

Et la portée est un concept qui se retrouve dans de nombreux langages de
programmation, pas seulement en JavaScript.  Ce qui veut dire que la
manipulation de portée effectuée dans ce chapitre, et illustrée par les
diagrammes, est applicable à d'autres langages, même s'ils ne disposent pas
d'une construction analogue à ~with~.   Il suffit de permettre au code client de
manipuler directement la résolution de portée, par un mécanisme réflexif par
exemple.

*** Inconvénients de la manipulation de portée avec ~with~
L'utilisation de ~with~ comporte néanmoins quelques inconvénients.  On utilise
~with~ ici juste parce que c'est le seul moyen qu'on a de manipuler la portée
des variables du code du module en JavaScript standard.  Ça nous évite d'avoir à
modifier le moteur d'exécution pour exposer le mécanisme de résolution de
portée, ce qui permet à notre solution de fonctionner sur n'importe quel
interpréteur JavaScript.

Néanmoins, certaines constructions ne peuvent pas être instrumentées.
L'environnement créé par ~with~ ne capture pas les déclarations de fonctions qui
ne sont pas écrites à la racine du module.  Une fonction interne ne peut donc
pas être instrumentée.  Les alias de fonctions ne sont pas instrumentés
automatiquement non plus: si ~m.a~ et ~m.b~ sont deux références au même objet
dans le module, alors redéfinir ~m.a~ ne change pas ~m.b~, puisqu'on ne souhaite
pas modifier les objets du module, seulement changer les références pour pointer
vers de nouvelles variantes.  Enfin, les fonctions anonymes ne peuvent pas être
instrumentées, n'ayant de nom pour pouvoir les référencer.

#+BEGIN_aside
En ECMAScript 6, le mode strict est activé /par défaut/ dans les modules et dans
les classes.
#+END_aside

Et surtout, ~with~ est une construction dépréciée du langage, et notamment
interdite dans le "mode strict" du standard ECMASCript 5.1.  Concrètement, ça
veut dire qu'un module écrit dans ce mode strict ne peut pas être étendu avec
~with~.  C'est une nouvelle manifestation du compromis entre flexibilité et
sûreté du code; le mode strict offre davantage de garanties pour le programmeur
(en particulier, l'absence de ~with~), et en échange il perd la possibilité
d'appliquer la technique de ce chapitre.

* BARE Synthèse
** BARE Séparation des préoccupations: pourquoi?
- Séparation difficile à obtenir
- Difficile de définir les frontières
  - une préoccupation est rarement isolée du reste du programme
- Cause d'autres problèmes
  - appels implicites, perte de compréhension du flot de contrôle
- Pointcut ~ dynamic scoping ~ COMEFROM
  - mécanismes puissants mais peu connus
  - usage difficile à justifier hors cas vraiment spécifiques
- Mauvais sens des priorités
  - Un programme doit d'abord être correct.
  - Puis il doit être maintenable -> bonne documentation des choix de structure
  - Séparation des préoccupations = cerise sur le gâteau.  Mais où est le gâteau?

** BARE Locality of concerns and locality of execution are irreconciable in the source text
Is AOP useful for the instrumentation problem?  First, the initial use case of
AOP, like open implementation, is tangential concerns: algorithmic complexity,
choice of data representation, optimizations, etc.  Post-hoc extension is not
exactly a tangential concern: changing the behavior of the interpreter is a
primary concern.

Second, we have to consider separately the usefulness of the AOP methodology,
and of the AOP mechanisms.

The methodology of separating components from aspects is applicable if our
analyses are tangential.  They are not.  The problem we deal with is that
extensibility was not considered when designing the interpreter, and solutions
must be built on the implementation.

Preserving locality is a guiding tenet of the AOP methodology (avoiding
tangling).  It is also a motivation for writing modular analyses: we want the
analysis code to be in one place.  However, by regrouping the analysis code, we
are sacrificing locality of code execution: statements executed at runtime are
not next to each other in the source code.  Satisfying both notions of locality
would lead to duplication in the code, which is a worse state of affairs.
Solutions to this duplication must come from the tools used to write and browse
code, since the textual format we use offer none.  An editor can maintain two
views of the same unit of code: changes in one view will affect both places.
That way, both notions of locality can coexist.

The second notion of locality, the one from [[cite:FF-04][FF-04]], is one manifestation of the
more general need of a match between runtime behavior and static program
description.  The program source should tell readers what it does, and
navigating through dynamically-bound method calls and oblivious advices hinders
the reading.

Organization of the code should reflect the design decisions: what is primary is
explained first, then exceptions or tangential concerns are relegated to
appendices.  Literate programming [[cite:Knu-84][Knu-84]] can help organize the code in a such
way.

The mechanisms of AOP may serve to extend the interpreter with analyses, without
necessarily obeying the component/aspect decomposition.  Though without editor
support, using AOP mechanisms will only satisfy one notion of locality.

** BARE Late binding is the common ingredient to all modular mechanisms
#+BEGIN_QUOTE
'When /I/ use a word,' Humpty Dumpty said in rather a scornful tone, 'it means
just what I choose it to mean—neither more nor less.'

'The question is,' said Alice, 'whether you /can/ make words mean so many
different things.'

— [[cite:Car-72][Car-72]]
#+END_QUOTE

* Références
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BIBLIOGRAPHY: refs alpha limit:t
